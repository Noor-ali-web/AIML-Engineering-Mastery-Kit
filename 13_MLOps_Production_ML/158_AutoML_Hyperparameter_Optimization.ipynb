{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AutoML & Hyperparameter Optimization - Setup\n",
    "\n",
    "Production AutoML stack:\n",
    "- HPO Frameworks: Optuna, Ray Tune, Hyperopt, KerasTuner, AutoGluon\n",
    "- Optimization Algorithms: Bayesian Optimization (TPE, GP), Evolutionary (CMA-ES, NSGA-II)\n",
    "- NAS: DARTS, ENAS, NASBench, AutoKeras\n",
    "- Multi-Fidelity: Hyperband, ASHA, BOHB\n",
    "- Experiment Tracking: Weights & Biases, MLflow, TensorBoard\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Tuple, Optional, Callable\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import uuid\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"âœ… Setup complete - Ready for AutoML and hyperparameter optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f7e5e",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Grid Search vs Random Search\n",
    "\n",
    "### ðŸ“ What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Compare basic hyperparameter search strategies: grid search and random search\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**1. Grid Search**\n",
    "- **Concept**: Exhaustively try all combinations of hyperparameter values\n",
    "- **Algorithm**:\n",
    "  1. Define discrete values for each hyperparameter (e.g., learning_rate = [0.001, 0.01, 0.1])\n",
    "  2. Generate all combinations (Cartesian product)\n",
    "  3. Train model with each combination\n",
    "  4. Select combination with best validation performance\n",
    "- **Complexity**: Exponential in number of hyperparameters\n",
    "  - 3 hyperparams Ã— 10 values each = 10Â³ = 1,000 trials\n",
    "  - 5 hyperparams Ã— 10 values each = 10âµ = 100,000 trials (infeasible!)\n",
    "\n",
    "**2. Random Search**\n",
    "- **Concept**: Sample hyperparameter combinations randomly from search space\n",
    "- **Algorithm**:\n",
    "  1. Define distributions for each hyperparameter (e.g., learning_rate ~ LogUniform(1e-5, 1e-1))\n",
    "  2. Sample N random combinations\n",
    "  3. Train model with each sample\n",
    "  4. Select best performing combination\n",
    "- **Advantage**: More efficient than grid search for high-dimensional spaces\n",
    "  - **Bergstra & Bengio (2012)**: Random search finds good configs 3Ã— faster than grid search\n",
    "\n",
    "**3. Why Random > Grid?**\n",
    "- **Coverage**: Random search explores more of the hyperparameter space\n",
    "- **Important hyperparameters**: If only 2 of 10 hyperparameters matter, random search tries more values for those 2\n",
    "- **Continuous spaces**: Grid requires discretization, random samples continuous values\n",
    "- **Diminishing returns**: Grid wastes trials on less important regions\n",
    "\n",
    "**Mathematical Insight:**\n",
    "For hyperparameters with low importance (flat response surface), grid search wastes many trials testing the same effective value. Random search spreads trials more evenly across important dimensions.\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Baseline**: Random search is minimum viable HPO strategy\n",
    "- **Cost**: Grid search can cost $10,000+ in compute (100K trials Ã— $0.10/trial)\n",
    "- **Speed**: Random search finds 90%-optimal solution in 10% of trials\n",
    "- **Practical**: Easy to implement, no complex optimization logic\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "Optimize yield prediction model (5 hyperparameters):\n",
    "- **Grid search**: 10âµ trials Ã— 2 min/trial = 200,000 min = 139 days\n",
    "- **Random search**: 100 trials Ã— 2 min/trial = 200 min = 3.3 hours (find 85% optimal)\n",
    "- **Business value**: $18.7M/year from finding good hyperparameters in hours vs months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0281090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Callable\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class HPOTrial:\n",
    "    \"\"\"Single hyperparameter optimization trial\"\"\"\n",
    "    trial_id: str\n",
    "    hyperparameters: Dict[str, Any]\n",
    "    score: float\n",
    "    duration_seconds: float\n",
    "\n",
    "class GridSearchOptimizer:\n",
    "    \"\"\"Exhaustive grid search over hyperparameter space\"\"\"\n",
    "    \n",
    "    def __init__(self, search_space: Dict[str, List[Any]], metric: Callable):\n",
    "        self.search_space = search_space\n",
    "        self.metric = metric  # Function: hyperparameters -> score (higher = better)\n",
    "        self.trials: List[HPOTrial] = []\n",
    "        \n",
    "    def optimize(self, max_trials: int = None) -> HPOTrial:\n",
    "        \"\"\"Run grid search\"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Generate all combinations\n",
    "        param_names = list(self.search_space.keys())\n",
    "        param_values = [self.search_space[name] for name in param_names]\n",
    "        combinations = list(itertools.product(*param_values))\n",
    "        \n",
    "        print(f\"Grid Search: {len(combinations)} total combinations\")\n",
    "        \n",
    "        # Limit trials if specified\n",
    "        if max_trials and len(combinations) > max_trials:\n",
    "            print(f\"âš ï¸ Limiting to {max_trials} trials (would take too long!)\")\n",
    "            combinations = combinations[:max_trials]\n",
    "        \n",
    "        # Try each combination\n",
    "        for i, values in enumerate(combinations):\n",
    "            hyperparams = dict(zip(param_names, values))\n",
    "            \n",
    "            start = time.time()\n",
    "            score = self.metric(hyperparams)\n",
    "            duration = time.time() - start\n",
    "            \n",
    "            trial = HPOTrial(\n",
    "                trial_id=f\"grid_{i}\",\n",
    "                hyperparameters=hyperparams,\n",
    "                score=score,\n",
    "                duration_seconds=duration\n",
    "            )\n",
    "            self.trials.append(trial)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Trial {i+1}/{len(combinations)}: score={score:.4f}\")\n",
    "        \n",
    "        # Return best trial\n",
    "        best_trial = max(self.trials, key=lambda t: t.score)\n",
    "        print(f\"\\nâœ… Best score: {best_trial.score:.4f}\")\n",
    "        return best_trial\n",
    "\n",
    "class RandomSearchOptimizer:\n",
    "    \"\"\"Random sampling from hyperparameter space\"\"\"\n",
    "    \n",
    "    def __init__(self, search_space: Dict[str, tuple], metric: Callable):\n",
    "        # search_space format: {'param': (min, max)} or {'param': [discrete values]}\n",
    "        self.search_space = search_space\n",
    "        self.metric = metric\n",
    "        self.trials: List[HPOTrial] = []\n",
    "        \n",
    "    def _sample_hyperparameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"Sample random hyperparameter configuration\"\"\"\n",
    "        hyperparams = {}\n",
    "        for name, space in self.search_space.items():\n",
    "            if isinstance(space, list):\n",
    "                # Discrete values\n",
    "                hyperparams[name] = random.choice(space)\n",
    "            elif isinstance(space, tuple) and len(space) == 2:\n",
    "                # Continuous range (min, max)\n",
    "                hyperparams[name] = random.uniform(space[0], space[1])\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid search space for {name}\")\n",
    "        return hyperparams\n",
    "    \n",
    "    def optimize(self, n_trials: int = 100) -> HPOTrial:\n",
    "        \"\"\"Run random search\"\"\"\n",
    "        import time\n",
    "        \n",
    "        print(f\"Random Search: {n_trials} random trials\")\n",
    "        \n",
    "        for i in range(n_trials):\n",
    "            hyperparams = self._sample_hyperparameters()\n",
    "            \n",
    "            start = time.time()\n",
    "            score = self.metric(hyperparams)\n",
    "            duration = time.time() - start\n",
    "            \n",
    "            trial = HPOTrial(\n",
    "                trial_id=f\"random_{i}\",\n",
    "                hyperparameters=hyperparams,\n",
    "                score=score,\n",
    "                duration_seconds=duration\n",
    "            )\n",
    "            self.trials.append(trial)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                best_so_far = max(self.trials, key=lambda t: t.score).score\n",
    "                print(f\"  Trial {i+1}/{n_trials}: current score={score:.4f}, best={best_so_far:.4f}\")\n",
    "        \n",
    "        # Return best trial\n",
    "        best_trial = max(self.trials, key=lambda t: t.score)\n",
    "        print(f\"\\nâœ… Best score: {best_trial.score:.4f}\")\n",
    "        return best_trial\n",
    "\n",
    "# Example: Optimize yield prediction model\n",
    "def yield_prediction_objective(hyperparams: Dict[str, Any]) -> float:\n",
    "    \"\"\"\n",
    "    Simulated yield prediction model performance\n",
    "    \n",
    "    Post-silicon context:\n",
    "    - Predict device yield% from parametric test data\n",
    "    - Hyperparameters: n_estimators, max_depth, learning_rate\n",
    "    - Metric: RÂ² score (higher = better)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Simulate model training (realistic response surface)\n",
    "    n_est = hyperparams['n_estimators']\n",
    "    depth = hyperparams['max_depth']\n",
    "    lr = hyperparams['learning_rate']\n",
    "    \n",
    "    # Optimal around: n_est=200, depth=8, lr=0.05\n",
    "    # RÂ² formula (simulated, peaked at optimal config)\n",
    "    r2 = 0.7 + 0.24 * np.exp(-((n_est - 200)**2 / 10000 + (depth - 8)**2 / 16 + (lr - 0.05)**2 / 0.01))\n",
    "    \n",
    "    # Add noise\n",
    "    r2 += np.random.normal(0, 0.02)\n",
    "    \n",
    "    return max(0, min(1, r2))  # Clamp to [0, 1]\n",
    "\n",
    "# Grid search (limited to 50 trials for speed)\n",
    "print(\"=\" * 60)\n",
    "print(\"GRID SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "grid_space = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_depth': [5, 8, 12, 15],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "grid_opt = GridSearchOptimizer(grid_space, yield_prediction_objective)\n",
    "grid_best = grid_opt.optimize(max_trials=50)\n",
    "print(f\"Best hyperparameters: {grid_best.hyperparameters}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "random_space = {\n",
    "    'n_estimators': [50, 100, 200, 300, 500, 1000],\n",
    "    'max_depth': [3, 5, 8, 10, 12, 15, 20],\n",
    "    'learning_rate': (0.001, 0.3)  # Continuous range\n",
    "}\n",
    "random_opt = RandomSearchOptimizer(random_space, yield_prediction_objective)\n",
    "random_best = random_opt.optimize(n_trials=50)\n",
    "print(f\"Best hyperparameters: {random_best.hyperparameters}\")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Grid Search Best:   RÂ² = {grid_best.score:.4f}\")\n",
    "print(f\"Random Search Best: RÂ² = {random_best.score:.4f}\")\n",
    "print(f\"Winner: {'Random Search' if random_best.score > grid_best.score else 'Grid Search'}\")\n",
    "print(f\"\\nðŸ’¡ Random search often finds better configs with same budget!\")\n",
    "print(f\"   (especially for continuous hyperparameters like learning_rate)\")\n",
    "\n",
    "# Business value\n",
    "baseline_r2 = 0.75\n",
    "improvement = max(grid_best.score, random_best.score) - baseline_r2\n",
    "revenue_per_point = 18.7e6 / 0.19  # $18.7M for 0.19 RÂ² improvement (from 0.75 to 0.94)\n",
    "value = improvement * revenue_per_point\n",
    "print(f\"\\nðŸ’° Business Value:\")\n",
    "print(f\"   RÂ² improvement: {improvement:.4f} (from {baseline_r2:.2f} baseline)\")\n",
    "print(f\"   Annual value: ${value/1e6:.1f}M/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b64da",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Bayesian Optimization with Gaussian Processes\n",
    "\n",
    "### ðŸ“ What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement intelligent hyperparameter optimization using Bayesian optimization with Gaussian Process surrogate model\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**1. Bayesian Optimization**\n",
    "- **Idea**: Build a probabilistic model of the objective function and use it to select the most promising hyperparameters\n",
    "- **Algorithm**:\n",
    "  1. **Surrogate model**: Gaussian Process approximates the unknown objective function f(x)\n",
    "  2. **Acquisition function**: Decides which hyperparameters to try next (balance exploration vs exploitation)\n",
    "  3. **Iterative refinement**: Update surrogate with new observations, repeat\n",
    "  \n",
    "**2. Gaussian Process (GP)**\n",
    "- **Concept**: Distribution over functions (not just parameters)\n",
    "- **Mathematics**:\n",
    "  - Prior: f(x) ~ GP(Î¼(x), k(x, x'))\n",
    "    - Î¼(x) = mean function (often 0)\n",
    "    - k(x, x') = kernel function (MatÃ©rn 5/2 is common)\n",
    "  - Posterior (after observations): f(x|D) ~ N(Î¼_post(x), ÏƒÂ²_post(x))\n",
    "    - Î¼_post(x) = k(x, X)(K + ÏƒÂ²I)â»Â¹y (predictive mean)\n",
    "    - ÏƒÂ²_post(x) = k(x, x) - k(x, X)(K + ÏƒÂ²I)â»Â¹k(X, x) (uncertainty)\n",
    "  - Where:\n",
    "    - X = observed hyperparameters\n",
    "    - y = observed scores\n",
    "    - K = kernel matrix K_ij = k(x_i, x_j)\n",
    "\n",
    "**3. Acquisition Functions**\n",
    "- **Expected Improvement (EI)**: E[max(f(x) - f(x_best), 0)]\n",
    "  - Formula: EI(x) = (Î¼(x) - f_best)Î¦(Z) + Ïƒ(x)Ï†(Z)\n",
    "    - Z = (Î¼(x) - f_best) / Ïƒ(x)\n",
    "    - Î¦(Â·) = cumulative standard normal\n",
    "    - Ï†(Â·) = probability density standard normal\n",
    "  - **Intuition**: Balance between high predicted value (Î¼(x)) and high uncertainty (Ïƒ(x))\n",
    "  - **Trade-off**: Exploitation (high Î¼) vs Exploration (high Ïƒ)\n",
    "\n",
    "**4. Why Bayesian > Random?**\n",
    "- **Sample efficiency**: Finds optimal config in 10-50 trials (vs 100-1000 for random)\n",
    "- **Intelligent exploration**: Uses past trials to inform next trial\n",
    "- **Convergence**: Provably converges to global optimum (under smoothness assumptions)\n",
    "- **Cost reduction**: Each trial saves ~$10-100 in compute (especially for expensive models)\n",
    "\n",
    "**Mathematical Insight:**\n",
    "Gaussian Process posterior variance ÏƒÂ²_post(x) is HIGH in unexplored regions and LOW near observations. Acquisition function balances:\n",
    "- High Î¼_post(x): Likely good performance (exploit)\n",
    "- High Ïƒ_post(x): High uncertainty (explore)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Cost**: Training a large model costs $50-500 per trial\n",
    "  - Random search: 100 trials Ã— $100 = $10,000\n",
    "  - Bayesian optimization: 20 trials Ã— $100 = $2,000 (80% savings)\n",
    "- **Time**: Reduce hyperparameter tuning from weeks to days\n",
    "- **Quality**: Find better hyperparameters (Bayesian explores intelligently)\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "Optimize wafer test time vs defect coverage (multi-objective):\n",
    "- **Random search**: 200 trials, 40 hours compute\n",
    "- **Bayesian optimization**: 30 trials, 6 hours compute (7Ã— faster, same quality)\n",
    "- **Business value**: $21.3M/year from 22% test time reduction (120s â†’ 93s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "class BayesianOptimizer:\n",
    "    \"\"\"Bayesian Optimization with Gaussian Process surrogate model\"\"\"\n",
    "    \n",
    "    def __init__(self, bounds: Dict[str, tuple], metric: Callable, maximize: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bounds: {'param': (min, max)} for continuous hyperparameters\n",
    "            metric: Function to optimize (hyperparameters -> score)\n",
    "            maximize: True to maximize metric, False to minimize\n",
    "        \"\"\"\n",
    "        self.bounds = bounds\n",
    "        self.param_names = list(bounds.keys())\n",
    "        self.metric = metric\n",
    "        self.maximize = maximize\n",
    "        \n",
    "        # Gaussian Process with MatÃ©rn 5/2 kernel (smooth but flexible)\n",
    "        kernel = Matern(nu=2.5)\n",
    "        self.gp = GaussianProcessRegressor(\n",
    "            kernel=kernel,\n",
    "            alpha=1e-6,  # Noise level\n",
    "            normalize_y=True,\n",
    "            n_restarts_optimizer=10  # Fit kernel hyperparameters\n",
    "        )\n",
    "        \n",
    "        self.trials: List[HPOTrial] = []\n",
    "        self.X_observed = []  # Hyperparameter vectors\n",
    "        self.y_observed = []  # Observed scores\n",
    "        \n",
    "    def _hyperparams_to_vector(self, hyperparams: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Convert hyperparameter dict to vector\"\"\"\n",
    "        return np.array([hyperparams[name] for name in self.param_names])\n",
    "    \n",
    "    def _vector_to_hyperparams(self, vector: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Convert vector to hyperparameter dict\"\"\"\n",
    "        return {name: float(val) for name, val in zip(self.param_names, vector)}\n",
    "    \n",
    "    def _expected_improvement(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expected Improvement acquisition function\n",
    "        \n",
    "        EI(x) = E[max(f(x) - f_best, 0)]\n",
    "              = (Î¼(x) - f_best)Î¦(Z) + Ïƒ(x)Ï†(Z)\n",
    "        \n",
    "        Where Z = (Î¼(x) - f_best) / Ïƒ(x)\n",
    "        \"\"\"\n",
    "        if len(self.y_observed) == 0:\n",
    "            # No observations yet, return uniform (explore randomly)\n",
    "            return np.ones(len(X))\n",
    "        \n",
    "        # Predict mean and std from GP\n",
    "        mu, sigma = self.gp.predict(X, return_std=True)\n",
    "        sigma = sigma.reshape(-1, 1).flatten()  # Ensure 1D\n",
    "        \n",
    "        # Best observed value\n",
    "        f_best = max(self.y_observed) if self.maximize else min(self.y_observed)\n",
    "        \n",
    "        # Expected improvement\n",
    "        with np.errstate(divide='warn'):\n",
    "            Z = (mu - f_best) / sigma if self.maximize else (f_best - mu) / sigma\n",
    "            ei = (mu - f_best) * norm.cdf(Z) + sigma * norm.pdf(Z) if self.maximize else \\\n",
    "                 (f_best - mu) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "            ei[sigma == 0.0] = 0.0  # Handle zero variance\n",
    "        \n",
    "        return ei\n",
    "    \n",
    "    def _suggest_next(self) -> Dict[str, Any]:\n",
    "        \"\"\"Suggest next hyperparameter configuration to try\"\"\"\n",
    "        # Generate random candidates\n",
    "        n_candidates = 1000\n",
    "        candidates = np.random.uniform(\n",
    "            low=[self.bounds[name][0] for name in self.param_names],\n",
    "            high=[self.bounds[name][1] for name in self.param_names],\n",
    "            size=(n_candidates, len(self.param_names))\n",
    "        )\n",
    "        \n",
    "        # Compute EI for all candidates\n",
    "        ei_values = self._expected_improvement(candidates)\n",
    "        \n",
    "        # Select candidate with highest EI\n",
    "        best_idx = np.argmax(ei_values)\n",
    "        best_candidate = candidates[best_idx]\n",
    "        \n",
    "        return self._vector_to_hyperparams(best_candidate)\n",
    "    \n",
    "    def optimize(self, n_trials: int = 50, n_random_init: int = 5) -> HPOTrial:\n",
    "        \"\"\"Run Bayesian optimization\"\"\"\n",
    "        import time\n",
    "        \n",
    "        print(f\"Bayesian Optimization: {n_trials} trials ({n_random_init} random init)\")\n",
    "        \n",
    "        for i in range(n_trials):\n",
    "            # Random initialization for first few trials\n",
    "            if i < n_random_init:\n",
    "                hyperparams = {\n",
    "                    name: np.random.uniform(bounds[0], bounds[1])\n",
    "                    for name, bounds in self.bounds.items()\n",
    "                }\n",
    "                method = \"random_init\"\n",
    "            else:\n",
    "                # Fit GP and suggest next trial\n",
    "                self.gp.fit(np.array(self.X_observed), np.array(self.y_observed))\n",
    "                hyperparams = self._suggest_next()\n",
    "                method = \"bayesian\"\n",
    "            \n",
    "            # Evaluate metric\n",
    "            start = time.time()\n",
    "            score = self.metric(hyperparams)\n",
    "            duration = time.time() - start\n",
    "            \n",
    "            # Record trial\n",
    "            trial = HPOTrial(\n",
    "                trial_id=f\"bayes_{i}\",\n",
    "                hyperparameters=hyperparams,\n",
    "                score=score,\n",
    "                duration_seconds=duration\n",
    "            )\n",
    "            self.trials.append(trial)\n",
    "            \n",
    "            # Update observations\n",
    "            self.X_observed.append(self._hyperparams_to_vector(hyperparams))\n",
    "            self.y_observed.append(score)\n",
    "            \n",
    "            # Progress\n",
    "            best_so_far = max(self.y_observed) if self.maximize else min(self.y_observed)\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  Trial {i+1}/{n_trials} ({method}): score={score:.4f}, best={best_so_far:.4f}\")\n",
    "        \n",
    "        # Return best trial\n",
    "        best_idx = np.argmax(self.y_observed) if self.maximize else np.argmin(self.y_observed)\n",
    "        best_trial = self.trials[best_idx]\n",
    "        print(f\"\\nâœ… Best score: {best_trial.score:.4f} (found at trial {best_idx + 1})\")\n",
    "        return best_trial\n",
    "\n",
    "# Compare Bayesian vs Random\n",
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "bayes_space = {\n",
    "    'n_estimators': (50, 1000),\n",
    "    'max_depth': (3, 20),\n",
    "    'learning_rate': (0.001, 0.3)\n",
    "}\n",
    "bayes_opt = BayesianOptimizer(bayes_space, yield_prediction_objective, maximize=True)\n",
    "bayes_best = bayes_opt.optimize(n_trials=30, n_random_init=5)\n",
    "print(f\"Best hyperparameters: {bayes_best.hyperparameters}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON: Bayesian vs Random (Same Budget)\")\n",
    "print(\"=\" * 60)\n",
    "random_opt_30 = RandomSearchOptimizer(random_space, yield_prediction_objective)\n",
    "random_best_30 = random_opt_30.optimize(n_trials=30)\n",
    "\n",
    "print(f\"Bayesian (30 trials): RÂ² = {bayes_best.score:.4f}\")\n",
    "print(f\"Random (30 trials):   RÂ² = {random_best_30.score:.4f}\")\n",
    "print(f\"Winner: {'Bayesian' if bayes_best.score > random_best_30.score else 'Random'}\")\n",
    "\n",
    "improvement = bayes_best.score - random_best_30.score\n",
    "if improvement > 0:\n",
    "    print(f\"\\nðŸ’¡ Bayesian is {improvement:.4f} RÂ² points better!\")\n",
    "    print(f\"   This demonstrates intelligent exploration vs random sampling\")\n",
    "\n",
    "# Visualize convergence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Score over trials\n",
    "axes[0].plot([t.score for t in random_opt_30.trials], 'o-', label='Random Search', alpha=0.7)\n",
    "axes[0].plot([t.score for t in bayes_opt.trials], 's-', label='Bayesian Optimization', alpha=0.7)\n",
    "axes[0].axhline(y=0.94, color='green', linestyle='--', label='Optimal RÂ² (0.94)', alpha=0.5)\n",
    "axes[0].set_xlabel('Trial Number')\n",
    "axes[0].set_ylabel('RÂ² Score')\n",
    "axes[0].set_title('Convergence: Bayesian vs Random Search')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Best score so far\n",
    "random_best_so_far = [max([t.score for t in random_opt_30.trials[:i+1]]) for i in range(len(random_opt_30.trials))]\n",
    "bayes_best_so_far = [max([t.score for t in bayes_opt.trials[:i+1]]) for i in range(len(bayes_opt.trials))]\n",
    "\n",
    "axes[1].plot(random_best_so_far, 'o-', label='Random Search', alpha=0.7)\n",
    "axes[1].plot(bayes_best_so_far, 's-', label='Bayesian Optimization', alpha=0.7)\n",
    "axes[1].axhline(y=0.94, color='green', linestyle='--', label='Optimal RÂ² (0.94)', alpha=0.5)\n",
    "axes[1].set_xlabel('Trial Number')\n",
    "axes[1].set_ylabel('Best RÂ² So Far')\n",
    "axes[1].set_title('Best Score Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’° Business Value:\")\n",
    "print(f\"   Bayesian finds better config faster â†’ reduces tuning time\")\n",
    "print(f\"   30 trials vs 100+ for random search â†’ 70% compute savings\")\n",
    "print(f\"   Annual value: ${(bayes_best.score - baseline_r2) * revenue_per_point / 1e6:.1f}M/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c405c",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Multi-Objective Optimization\n",
    "\n",
    "### ðŸ“ What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Optimize multiple conflicting objectives simultaneously (e.g., maximize accuracy AND minimize latency)\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**1. Pareto Optimality**\n",
    "- **Definition**: Configuration x is Pareto optimal if no other configuration dominates it\n",
    "- **Dominance**: xâ‚ dominates xâ‚‚ if:\n",
    "  - xâ‚ is better or equal on all objectives\n",
    "  - xâ‚ is strictly better on at least one objective\n",
    "- **Pareto front**: Set of all Pareto optimal solutions (trade-off curve)\n",
    "\n",
    "**2. Multi-Objective Problem Formulation**\n",
    "- **Single-objective**: max f(x)\n",
    "- **Multi-objective**: max [fâ‚(x), fâ‚‚(x), ..., fâ‚–(x)]\n",
    "  - Example: max [accuracy, -latency, -memory]\n",
    "  - **Trade-offs**: Improving one objective may hurt another\n",
    "    - Higher accuracy model â†’ slower inference (more parameters)\n",
    "    - Faster inference â†’ lower accuracy (simpler model)\n",
    "\n",
    "**3. NSGA-II Algorithm (Non-dominated Sorting Genetic Algorithm II)**\n",
    "- **Algorithm**:\n",
    "  1. Initialize population (random configurations)\n",
    "  2. Evaluate all objectives for each individual\n",
    "  3. Non-dominated sorting: Rank individuals into fronts\n",
    "     - Front 1: Non-dominated individuals\n",
    "     - Front 2: Non-dominated after removing Front 1\n",
    "     - Front 3: Non-dominated after removing Front 1 & 2, etc.\n",
    "  4. Crowding distance: Preserve diversity within each front\n",
    "  5. Selection: Select best individuals (front rank, then crowding distance)\n",
    "  6. Crossover and mutation: Generate offspring\n",
    "  7. Repeat until convergence\n",
    "  \n",
    "**4. Crowding Distance**\n",
    "- **Purpose**: Maintain diversity in Pareto front\n",
    "- **Formula**: For objective m, distance(i) = |f_m(i+1) - f_m(i-1)| / (f_m_max - f_m_min)\n",
    "- **Intuition**: Prefer solutions with larger gaps to neighbors (spread out Pareto front)\n",
    "\n",
    "**Mathematical Insight:**\n",
    "Multi-objective optimization finds a SET of solutions (Pareto front), not a single solution. User selects preferred trade-off post-hoc.\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Real-world**: Most problems have multiple objectives\n",
    "  - Post-silicon: minimize test_time AND maximize defect_coverage\n",
    "  - General ML: maximize accuracy AND minimize latency/memory\n",
    "- **Trade-off visibility**: Pareto front shows all possible trade-offs\n",
    "- **Decision-making**: Stakeholders choose preferred point on Pareto front\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "Optimize wafer test parameters:\n",
    "- **Objective 1**: Minimize test time (lower cost: $120/hour ATE time)\n",
    "- **Objective 2**: Maximize defect coverage (prevent escapes: $5,000/defect)\n",
    "- **Trade-off**: More thorough testing (95% â†’ 99.9% coverage) increases test time (60s â†’ 120s)\n",
    "- **Pareto front**: Shows all optimal trade-off points\n",
    "- **Business value**: $21.3M/year from finding optimal trade-off (93s test time, 99.9% coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebc244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "@dataclass\n",
    "class MultiObjectiveTrial:\n",
    "    \"\"\"Trial with multiple objectives\"\"\"\n",
    "    trial_id: str\n",
    "    hyperparameters: Dict[str, Any]\n",
    "    objectives: Dict[str, float]  # {'accuracy': 0.95, 'latency_ms': 50}\n",
    "    rank: int = 0  # Pareto front rank (1 = best)\n",
    "    crowding_distance: float = 0.0\n",
    "\n",
    "class MultiObjectiveOptimizer:\n",
    "    \"\"\"Multi-objective optimization with NSGA-II\"\"\"\n",
    "    \n",
    "    def __init__(self, bounds: Dict[str, tuple], objective_funcs: Dict[str, Callable], \n",
    "                 maximize: Dict[str, bool]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bounds: {'param': (min, max)}\n",
    "            objective_funcs: {'obj_name': function}\n",
    "            maximize: {'obj_name': True/False} (True = maximize, False = minimize)\n",
    "        \"\"\"\n",
    "        self.bounds = bounds\n",
    "        self.param_names = list(bounds.keys())\n",
    "        self.objective_funcs = objective_funcs\n",
    "        self.objective_names = list(objective_funcs.keys())\n",
    "        self.maximize = maximize\n",
    "        self.trials: List[MultiObjectiveTrial] = []\n",
    "        \n",
    "    def _random_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate random hyperparameter configuration\"\"\"\n",
    "        return {name: np.random.uniform(bounds[0], bounds[1]) \n",
    "                for name, bounds in self.bounds.items()}\n",
    "    \n",
    "    def _dominates(self, trial1: MultiObjectiveTrial, trial2: MultiObjectiveTrial) -> bool:\n",
    "        \"\"\"Check if trial1 dominates trial2\"\"\"\n",
    "        better_or_equal_all = True\n",
    "        strictly_better_at_least_one = False\n",
    "        \n",
    "        for obj_name in self.objective_names:\n",
    "            val1 = trial1.objectives[obj_name]\n",
    "            val2 = trial2.objectives[obj_name]\n",
    "            \n",
    "            if self.maximize[obj_name]:\n",
    "                # Maximize: val1 should be >= val2\n",
    "                if val1 < val2:\n",
    "                    better_or_equal_all = False\n",
    "                if val1 > val2:\n",
    "                    strictly_better_at_least_one = True\n",
    "            else:\n",
    "                # Minimize: val1 should be <= val2\n",
    "                if val1 > val2:\n",
    "                    better_or_equal_all = False\n",
    "                if val1 < val2:\n",
    "                    strictly_better_at_least_one = True\n",
    "        \n",
    "        return better_or_equal_all and strictly_better_at_least_one\n",
    "    \n",
    "    def _fast_non_dominated_sort(self, trials: List[MultiObjectiveTrial]) -> List[List[MultiObjectiveTrial]]:\n",
    "        \"\"\"Sort trials into Pareto fronts\"\"\"\n",
    "        fronts = [[]]\n",
    "        \n",
    "        domination_count = {i: 0 for i in range(len(trials))}\n",
    "        dominated_solutions = {i: [] for i in range(len(trials))}\n",
    "        \n",
    "        # Find domination relationships\n",
    "        for i, trial_i in enumerate(trials):\n",
    "            for j, trial_j in enumerate(trials):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if self._dominates(trial_i, trial_j):\n",
    "                    dominated_solutions[i].append(j)\n",
    "                elif self._dominates(trial_j, trial_i):\n",
    "                    domination_count[i] += 1\n",
    "            \n",
    "            # If not dominated by anyone, it's in front 1\n",
    "            if domination_count[i] == 0:\n",
    "                trial_i.rank = 1\n",
    "                fronts[0].append(trial_i)\n",
    "        \n",
    "        # Find remaining fronts\n",
    "        front_idx = 0\n",
    "        while len(fronts[front_idx]) > 0:\n",
    "            next_front = []\n",
    "            for trial in fronts[front_idx]:\n",
    "                trial_idx = trials.index(trial)\n",
    "                for dominated_idx in dominated_solutions[trial_idx]:\n",
    "                    domination_count[dominated_idx] -= 1\n",
    "                    if domination_count[dominated_idx] == 0:\n",
    "                        trials[dominated_idx].rank = front_idx + 2\n",
    "                        next_front.append(trials[dominated_idx])\n",
    "            fronts.append(next_front)\n",
    "            front_idx += 1\n",
    "        \n",
    "        return fronts[:-1]  # Remove empty last front\n",
    "    \n",
    "    def _crowding_distance(self, trials: List[MultiObjectiveTrial]):\n",
    "        \"\"\"Compute crowding distance for diversity\"\"\"\n",
    "        if len(trials) == 0:\n",
    "            return\n",
    "        \n",
    "        # Initialize distances\n",
    "        for trial in trials:\n",
    "            trial.crowding_distance = 0.0\n",
    "        \n",
    "        # For each objective\n",
    "        for obj_name in self.objective_names:\n",
    "            # Sort by this objective\n",
    "            trials_sorted = sorted(trials, key=lambda t: t.objectives[obj_name])\n",
    "            \n",
    "            # Boundary points get infinite distance\n",
    "            trials_sorted[0].crowding_distance = float('inf')\n",
    "            trials_sorted[-1].crowding_distance = float('inf')\n",
    "            \n",
    "            # Range of this objective\n",
    "            obj_range = trials_sorted[-1].objectives[obj_name] - trials_sorted[0].objectives[obj_name]\n",
    "            if obj_range == 0:\n",
    "                continue\n",
    "            \n",
    "            # Compute distances for intermediate points\n",
    "            for i in range(1, len(trials_sorted) - 1):\n",
    "                distance = (trials_sorted[i+1].objectives[obj_name] - \n",
    "                           trials_sorted[i-1].objectives[obj_name]) / obj_range\n",
    "                trials_sorted[i].crowding_distance += distance\n",
    "    \n",
    "    def optimize(self, population_size: int = 50, n_generations: int = 20) -> List[MultiObjectiveTrial]:\n",
    "        \"\"\"Run NSGA-II\"\"\"\n",
    "        print(f\"NSGA-II: {n_generations} generations, population={population_size}\")\n",
    "        \n",
    "        # Initialize population\n",
    "        population = []\n",
    "        for i in range(population_size):\n",
    "            hyperparams = self._random_config()\n",
    "            objectives = {name: func(hyperparams) \n",
    "                         for name, func in self.objective_funcs.items()}\n",
    "            trial = MultiObjectiveTrial(\n",
    "                trial_id=f\"nsga_{i}\",\n",
    "                hyperparameters=hyperparams,\n",
    "                objectives=objectives\n",
    "            )\n",
    "            population.append(trial)\n",
    "            self.trials.append(trial)\n",
    "        \n",
    "        # Evolve\n",
    "        for gen in range(n_generations):\n",
    "            # Non-dominated sorting\n",
    "            fronts = self._fast_non_dominated_sort(population)\n",
    "            \n",
    "            # Compute crowding distance\n",
    "            for front in fronts:\n",
    "                self._crowding_distance(front)\n",
    "            \n",
    "            # Create offspring (simplified: mutation only)\n",
    "            offspring = []\n",
    "            for _ in range(population_size):\n",
    "                # Select parent (tournament selection based on rank and crowding)\n",
    "                parent = max(np.random.choice(population, size=2, replace=False),\n",
    "                           key=lambda t: (t.rank, t.crowding_distance))\n",
    "                \n",
    "                # Mutate\n",
    "                hyperparams = parent.hyperparameters.copy()\n",
    "                for name in self.param_names:\n",
    "                    if np.random.rand() < 0.3:  # Mutation probability\n",
    "                        hyperparams[name] = np.random.uniform(self.bounds[name][0], \n",
    "                                                             self.bounds[name][1])\n",
    "                \n",
    "                # Evaluate\n",
    "                objectives = {name: func(hyperparams) \n",
    "                             for name, func in self.objective_funcs.items()}\n",
    "                child = MultiObjectiveTrial(\n",
    "                    trial_id=f\"nsga_gen{gen}_child{_}\",\n",
    "                    hyperparameters=hyperparams,\n",
    "                    objectives=objectives\n",
    "                )\n",
    "                offspring.append(child)\n",
    "                self.trials.append(child)\n",
    "            \n",
    "            # Combine and select\n",
    "            combined = population + offspring\n",
    "            fronts = self._fast_non_dominated_sort(combined)\n",
    "            for front in fronts:\n",
    "                self._crowding_distance(front)\n",
    "            \n",
    "            # Select top population_size\n",
    "            new_population = []\n",
    "            for front in fronts:\n",
    "                if len(new_population) + len(front) <= population_size:\n",
    "                    new_population.extend(front)\n",
    "                else:\n",
    "                    # Sort by crowding distance and take remaining\n",
    "                    remaining = population_size - len(new_population)\n",
    "                    front_sorted = sorted(front, key=lambda t: t.crowding_distance, reverse=True)\n",
    "                    new_population.extend(front_sorted[:remaining])\n",
    "                    break\n",
    "            population = new_population\n",
    "            \n",
    "            if (gen + 1) % 5 == 0:\n",
    "                pareto_size = len([t for t in population if t.rank == 1])\n",
    "                print(f\"  Generation {gen+1}/{n_generations}: Pareto front size = {pareto_size}\")\n",
    "        \n",
    "        # Return Pareto front\n",
    "        pareto_front = [t for t in population if t.rank == 1]\n",
    "        print(f\"\\nâœ… Found {len(pareto_front)} Pareto optimal solutions\")\n",
    "        return pareto_front\n",
    "\n",
    "# Multi-objective: test time vs defect coverage\n",
    "def test_time_objective(hyperparams: Dict[str, Any]) -> float:\n",
    "    \"\"\"Minimize test time (seconds)\"\"\"\n",
    "    voltage_steps = hyperparams['voltage_steps']\n",
    "    freq_steps = hyperparams['frequency_steps']\n",
    "    temp_points = hyperparams['temperature_points']\n",
    "    \n",
    "    # More steps = more thorough but slower\n",
    "    test_time = 30 + voltage_steps * 0.5 + freq_steps * 1.2 + temp_points * 2.5\n",
    "    return test_time\n",
    "\n",
    "def defect_coverage_objective(hyperparams: Dict[str, Any]) -> float:\n",
    "    \"\"\"Maximize defect coverage (%)\"\"\"\n",
    "    voltage_steps = hyperparams['voltage_steps']\n",
    "    freq_steps = hyperparams['frequency_steps']\n",
    "    temp_points = hyperparams['temperature_points']\n",
    "    \n",
    "    # More comprehensive testing = higher coverage\n",
    "    coverage = 85 + 10 * (1 - np.exp(-voltage_steps / 20)) + \\\n",
    "               5 * (1 - np.exp(-freq_steps / 15)) + \\\n",
    "               4 * (1 - np.exp(-temp_points / 8))\n",
    "    coverage = min(100, coverage + np.random.normal(0, 0.5))\n",
    "    return coverage\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTI-OBJECTIVE OPTIMIZATION (Test Time vs Coverage)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mo_bounds = {\n",
    "    'voltage_steps': (5, 50),\n",
    "    'frequency_steps': (5, 40),\n",
    "    'temperature_points': (2, 10)\n",
    "}\n",
    "mo_objectives = {\n",
    "    'test_time_sec': test_time_objective,\n",
    "    'defect_coverage_pct': defect_coverage_objective\n",
    "}\n",
    "mo_maximize = {\n",
    "    'test_time_sec': False,  # Minimize\n",
    "    'defect_coverage_pct': True  # Maximize\n",
    "}\n",
    "\n",
    "mo_opt = MultiObjectiveOptimizer(mo_bounds, mo_objectives, mo_maximize)\n",
    "pareto_front = mo_opt.optimize(population_size=40, n_generations=25)\n",
    "\n",
    "# Visualize Pareto front\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot all trials\n",
    "all_times = [t.objectives['test_time_sec'] for t in mo_opt.trials]\n",
    "all_coverages = [t.objectives['defect_coverage_pct'] for t in mo_opt.trials]\n",
    "ax.scatter(all_times, all_coverages, alpha=0.3, s=30, label='All Trials', color='gray')\n",
    "\n",
    "# Plot Pareto front\n",
    "pareto_times = [t.objectives['test_time_sec'] for t in pareto_front]\n",
    "pareto_coverages = [t.objectives['defect_coverage_pct'] for t in pareto_front]\n",
    "pareto_sorted = sorted(zip(pareto_times, pareto_coverages))\n",
    "ax.plot([p[0] for p in pareto_sorted], [p[1] for p in pareto_sorted], \n",
    "        'ro-', linewidth=2, markersize=8, label='Pareto Front')\n",
    "\n",
    "ax.set_xlabel('Test Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Defect Coverage (%)', fontsize=12)\n",
    "ax.set_title('Multi-Objective Optimization: Test Time vs Coverage Trade-off', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show trade-off options\n",
    "print(\"\\nðŸ“Š Pareto Front Solutions (Top 5):\")\n",
    "pareto_sorted_configs = sorted(pareto_front, key=lambda t: t.objectives['test_time_sec'])\n",
    "for i, trial in enumerate(pareto_sorted_configs[:5]):\n",
    "    print(f\"  Option {i+1}: {trial.objectives['test_time_sec']:.1f}s test time, \"\n",
    "          f\"{trial.objectives['defect_coverage_pct']:.1f}% coverage\")\n",
    "\n",
    "print(f\"\\nðŸ’° Business Value: $21.3M/year from optimal trade-off selection\")\n",
    "print(f\"   (93s test time, 99.9% coverage from Pareto front)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27715e20",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Early Stopping & Multi-Fidelity Optimization\n",
    "\n",
    "### ðŸ“ What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Reduce HPO cost by stopping unpromising trials early and using cheap approximations\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**1. Early Stopping**\n",
    "- **Idea**: Stop training if validation performance isn't improving\n",
    "- **Algorithm**:\n",
    "  1. Train for small number of epochs/iterations\n",
    "  2. Check if performance is improving\n",
    "  3. If plateaued or declining â†’ stop trial early\n",
    "  4. If promising â†’ continue training\n",
    "- **Benefit**: Save compute by abandoning bad hyperparameters early\n",
    "\n",
    "**2. Successive Halving (Hyperband)**\n",
    "- **Concept**: Tournament-style elimination of configurations\n",
    "- **Algorithm**:\n",
    "  1. Start with N configurations (e.g., 81)\n",
    "  2. Train all for 1 epoch, keep top 1/3 (27 configs)\n",
    "  3. Train survivors for 3 epochs, keep top 1/3 (9 configs)\n",
    "  4. Train survivors for 9 epochs, keep top 1/3 (3 configs)\n",
    "  5. Train survivors for 27 epochs, keep best (1 config)\n",
    "- **Budget**: Total = 81Ã—1 + 27Ã—3 + 9Ã—9 + 3Ã—27 = 81 + 81 + 81 + 81 = 324 epochs\n",
    "  - vs Full training: 81 configs Ã— 27 epochs = 2,187 epochs (7Ã— savings!)\n",
    "\n",
    "**3. ASHA (Asynchronous Successive Halving Algorithm)**\n",
    "- **Enhancement**: Asynchronous version of Hyperband for parallel workers\n",
    "- **Algorithm**:\n",
    "  1. Workers continuously pull new configs from queue\n",
    "  2. Train for min_epochs, report performance\n",
    "  3. If performance > threshold (e.g., top 50% of completed trials), promote to next rung\n",
    "  4. Promoted configs train for longer (3Ã—, 9Ã—, 27Ã— min_epochs)\n",
    "  5. Repeat until budget exhausted\n",
    "- **Advantage**: No synchronization barriers, efficient GPU utilization\n",
    "\n",
    "**4. Multi-Fidelity Optimization**\n",
    "- **Concept**: Use cheaper approximations to evaluate hyperparameters\n",
    "- **Fidelity dimensions**:\n",
    "  - **Epochs**: 1 epoch << 100 epochs (time)\n",
    "  - **Data size**: 10% data << 100% data (time)\n",
    "  - **Model size**: 10M params << 100M params (memory)\n",
    "  - **Resolution**: 64Ã—64 images << 256Ã—256 images (compute)\n",
    "- **Strategy**: Evaluate at low fidelity, promote promising configs to high fidelity\n",
    "\n",
    "**Mathematical Insight:**\n",
    "Successive halving balances exploration (try many configs at low fidelity) and exploitation (train best configs fully).\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Cost**: Training 100 configs fully costs $10,000 (100 Ã— $100/model)\n",
    "  - With early stopping: $2,000 (80% savings by stopping 70 configs at 10% progress)\n",
    "- **Time**: HPO completes in 2 days vs 10 days\n",
    "- **Quality**: Same final performance (good configs identified early)\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "Neural architecture search for wafer map classification:\n",
    "- **Full training**: 100 architectures Ã— 50 epochs Ã— 30 min = 2,500 hours\n",
    "- **ASHA**: 100 architectures, promote top 25% through rungs â†’ 625 hours (4Ã— faster)\n",
    "- **Business value**: $15.8M/year from finding optimal architecture (96% accuracy vs 89%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "@dataclass\n",
    "class FidelityTrial:\n",
    "    \"\"\"Trial with multi-fidelity support\"\"\"\n",
    "    trial_id: str\n",
    "    hyperparameters: Dict[str, Any]\n",
    "    current_fidelity: int  # Epochs trained so far\n",
    "    performance_history: List[float]  # Performance at each fidelity level\n",
    "    \n",
    "    @property\n",
    "    def current_performance(self) -> float:\n",
    "        return self.performance_history[-1] if self.performance_history else 0.0\n",
    "\n",
    "class ASHAOptimizer:\n",
    "    \"\"\"Asynchronous Successive Halving Algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, search_space: Dict[str, tuple], \n",
    "                 train_func: Callable,  # (hyperparams, epochs) -> performance\n",
    "                 min_fidelity: int = 1,\n",
    "                 max_fidelity: int = 27,\n",
    "                 reduction_factor: int = 3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            search_space: {'param': (min, max)}\n",
    "            train_func: Function that trains model for given epochs and returns performance\n",
    "            min_fidelity: Minimum epochs to train (first rung)\n",
    "            max_fidelity: Maximum epochs to train (final rung)\n",
    "            reduction_factor: Keep top 1/reduction_factor configs at each rung\n",
    "        \"\"\"\n",
    "        self.search_space = search_space\n",
    "        self.param_names = list(search_space.keys())\n",
    "        self.train_func = train_func\n",
    "        self.min_fidelity = min_fidelity\n",
    "        self.max_fidelity = max_fidelity\n",
    "        self.reduction_factor = reduction_factor\n",
    "        \n",
    "        # Compute rungs (fidelity levels)\n",
    "        self.rungs = []\n",
    "        fidelity = min_fidelity\n",
    "        while fidelity <= max_fidelity:\n",
    "            self.rungs.append(fidelity)\n",
    "            fidelity *= reduction_factor\n",
    "        \n",
    "        self.trials: Dict[str, FidelityTrial] = {}\n",
    "        self.rung_performance: Dict[int, List[Tuple[float, str]]] = {r: [] for r in self.rungs}\n",
    "        \n",
    "    def _sample_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Sample random hyperparameter configuration\"\"\"\n",
    "        return {name: np.random.uniform(bounds[0], bounds[1])\n",
    "                for name, bounds in self.search_space.items()}\n",
    "    \n",
    "    def _should_promote(self, trial: FidelityTrial, rung: int) -> bool:\n",
    "        \"\"\"Check if trial should be promoted to next rung\"\"\"\n",
    "        if rung not in self.rung_performance:\n",
    "            return True  # First trial at this rung\n",
    "        \n",
    "        # Get performance of top 1/reduction_factor trials at this rung\n",
    "        rung_trials = self.rung_performance[rung]\n",
    "        if len(rung_trials) < self.reduction_factor:\n",
    "            return True  # Not enough trials yet to make decision\n",
    "        \n",
    "        # Sort by performance (descending)\n",
    "        rung_trials_sorted = sorted(rung_trials, reverse=True)\n",
    "        threshold = rung_trials_sorted[len(rung_trials_sorted) // self.reduction_factor - 1][0]\n",
    "        \n",
    "        return trial.current_performance >= threshold\n",
    "    \n",
    "    def optimize(self, n_configs: int = 81, max_budget: int = None) -> FidelityTrial:\n",
    "        \"\"\"Run ASHA\"\"\"\n",
    "        import time\n",
    "        \n",
    "        print(f\"ASHA: {n_configs} initial configs, rungs = {self.rungs}\")\n",
    "        \n",
    "        total_epochs = 0\n",
    "        max_budget = max_budget or n_configs * self.max_fidelity\n",
    "        \n",
    "        # Queue of (fidelity, trial_id) to evaluate\n",
    "        queue = []\n",
    "        \n",
    "        # Initialize with n_configs random configs at min fidelity\n",
    "        for i in range(n_configs):\n",
    "            trial_id = f\"asha_{i}\"\n",
    "            hyperparams = self._sample_config()\n",
    "            trial = FidelityTrial(\n",
    "                trial_id=trial_id,\n",
    "                hyperparameters=hyperparams,\n",
    "                current_fidelity=0,\n",
    "                performance_history=[]\n",
    "            )\n",
    "            self.trials[trial_id] = trial\n",
    "            heapq.heappush(queue, (self.min_fidelity, trial_id))\n",
    "        \n",
    "        # Process queue\n",
    "        trial_count = 0\n",
    "        while queue and total_epochs < max_budget:\n",
    "            fidelity, trial_id = heapq.heappop(queue)\n",
    "            trial = self.trials[trial_id]\n",
    "            \n",
    "            # Train for this fidelity\n",
    "            epochs_to_train = fidelity - trial.current_fidelity\n",
    "            performance = self.train_func(trial.hyperparameters, epochs_to_train)\n",
    "            \n",
    "            trial.current_fidelity = fidelity\n",
    "            trial.performance_history.append(performance)\n",
    "            total_epochs += epochs_to_train\n",
    "            \n",
    "            # Record performance at this rung\n",
    "            self.rung_performance[fidelity].append((performance, trial_id))\n",
    "            \n",
    "            trial_count += 1\n",
    "            if trial_count % 10 == 0:\n",
    "                best = max(self.trials.values(), key=lambda t: t.current_performance)\n",
    "                print(f\"  Evaluated {trial_count} trials, {total_epochs}/{max_budget} epochs used, \"\n",
    "                      f\"best so far = {best.current_performance:.4f}\")\n",
    "            \n",
    "            # Check if should promote to next rung\n",
    "            next_rung_idx = self.rungs.index(fidelity) + 1\n",
    "            if next_rung_idx < len(self.rungs):\n",
    "                next_fidelity = self.rungs[next_rung_idx]\n",
    "                if self._should_promote(trial, fidelity):\n",
    "                    heapq.heappush(queue, (next_fidelity, trial_id))\n",
    "        \n",
    "        # Return best trial\n",
    "        best_trial = max(self.trials.values(), key=lambda t: t.current_performance)\n",
    "        print(f\"\\nâœ… Best performance: {best_trial.current_performance:.4f}\")\n",
    "        print(f\"   Total epochs used: {total_epochs} (vs {n_configs * self.max_fidelity} for full training)\")\n",
    "        print(f\"   Savings: {100 * (1 - total_epochs / (n_configs * self.max_fidelity)):.1f}%\")\n",
    "        return best_trial\n",
    "\n",
    "# Simulated training function\n",
    "def wafer_map_train_func(hyperparams: Dict[str, Any], epochs: int) -> float:\n",
    "    \"\"\"\n",
    "    Simulate training wafer map CNN classifier\n",
    "    \n",
    "    Performance improves with epochs (diminishing returns) and depends on architecture\n",
    "    \"\"\"\n",
    "    num_layers = int(hyperparams['num_layers'])\n",
    "    filters = int(hyperparams['filters'])\n",
    "    dropout = hyperparams['dropout']\n",
    "    \n",
    "    # Optimal around: 12 layers, 128 filters, 0.3 dropout\n",
    "    architecture_quality = 0.7 + 0.26 * np.exp(\n",
    "        -((num_layers - 12)**2 / 50 + (filters - 128)**2 / 5000 + (dropout - 0.3)**2 / 0.1)\n",
    "    )\n",
    "    \n",
    "    # Performance improves with epochs (logarithmic)\n",
    "    training_progress = 1 - np.exp(-epochs / 10)\n",
    "    \n",
    "    # Final accuracy\n",
    "    accuracy = architecture_quality * training_progress\n",
    "    accuracy += np.random.normal(0, 0.01)  # Noise\n",
    "    \n",
    "    return max(0, min(1, accuracy))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ASHA (Asynchronous Successive Halving)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "asha_space = {\n",
    "    'num_layers': (6, 20),\n",
    "    'filters': (32, 256),\n",
    "    'dropout': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "asha_opt = ASHAOptimizer(\n",
    "    search_space=asha_space,\n",
    "    train_func=wafer_map_train_func,\n",
    "    min_fidelity=1,\n",
    "    max_fidelity=27,\n",
    "    reduction_factor=3\n",
    ")\n",
    "\n",
    "asha_best = asha_opt.optimize(n_configs=81, max_budget=2000)\n",
    "\n",
    "print(f\"\\nBest architecture:\")\n",
    "for param, value in asha_best.hyperparameters.items():\n",
    "    print(f\"  {param}: {value:.2f}\")\n",
    "\n",
    "# Visualize rung progression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Number of trials per rung\n",
    "rung_counts = [len(asha_opt.rung_performance[r]) for r in asha_opt.rungs]\n",
    "axes[0].bar(range(len(asha_opt.rungs)), rung_counts, alpha=0.7, color='steelblue')\n",
    "axes[0].set_xticks(range(len(asha_opt.rungs)))\n",
    "axes[0].set_xticklabels([f\"{r} epochs\" for r in asha_opt.rungs])\n",
    "axes[0].set_xlabel('Rung (Fidelity Level)')\n",
    "axes[0].set_ylabel('Number of Trials')\n",
    "axes[0].set_title('ASHA: Successive Halving (Trials per Rung)')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Performance distribution per rung\n",
    "rung_perfs = [sorted([p for p, _ in asha_opt.rung_performance[r]], reverse=True) \n",
    "              for r in asha_opt.rungs]\n",
    "positions = []\n",
    "for i, perfs in enumerate(rung_perfs):\n",
    "    positions.extend([i] * len(perfs))\n",
    "    axes[1].scatter([i] * len(perfs), perfs, alpha=0.6, s=50)\n",
    "\n",
    "axes[1].set_xticks(range(len(asha_opt.rungs)))\n",
    "axes[1].set_xticklabels([f\"{r} epochs\" for r in asha_opt.rungs])\n",
    "axes[1].set_xlabel('Rung (Fidelity Level)')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('ASHA: Performance Distribution by Rung')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’° Business Value:\")\n",
    "print(f\"   ASHA reduces HPO cost by 75-85% via early stopping\")\n",
    "print(f\"   Same final model quality with 1/4 compute budget\")\n",
    "print(f\"   Post-silicon: $15.8M/year from optimal wafer map CNN (96% accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81845b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Real-World Projects\n",
    "\n",
    "Build production AutoML systems that automate hyperparameter optimization across diverse domains. Each project includes business value estimation and implementation guidance.\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### Project 1: Multi-Fab Yield Prediction AutoML ðŸ’° **$23.5M/year**\n",
    "\n",
    "**Objective**: Automatically find optimal ML model and hyperparameters for yield prediction across 4 fabrication facilities\n",
    "\n",
    "**Business Value**:\n",
    "- **Baseline**: Manual tuning takes 3 months per fab, RÂ² = 0.82\n",
    "- **AutoML**: Find optimal config in 1 week, RÂ² = 0.93\n",
    "- **Impact**: 0.11 RÂ² improvement Ã— 4 fabs Ã— $5.4M/fab = **$23.5M/year**\n",
    "\n",
    "**Features**:\n",
    "- **Search space**: 5 algorithms (Linear, Ridge, RF, XGBoost, LightGBM) Ã— 30 hyperparams each\n",
    "- **AutoML method**: Bayesian optimization with multi-fidelity (10%, 50%, 100% data)\n",
    "- **Objectives**: Maximize RÂ², minimize training time (<10 min)\n",
    "- **Data**: Parametric test data (Vdd, Idd, frequency, temperature) â†’ yield%\n",
    "\n",
    "**Implementation Hints**:\n",
    "```python\n",
    "# Define search space\n",
    "algorithms = ['linear', 'ridge', 'rf', 'xgb', 'lgbm']\n",
    "hyperparams = {\n",
    "    'rf': {'n_estimators': (50, 500), 'max_depth': (5, 20)},\n",
    "    'xgb': {'n_estimators': (50, 1000), 'learning_rate': (0.001, 0.3), 'max_depth': (3, 15)}\n",
    "}\n",
    "\n",
    "# Bayesian optimization with algorithm selection\n",
    "def objective(config):\n",
    "    algo = config['algorithm']\n",
    "    params = {k: v for k, v in config.items() if k != 'algorithm'}\n",
    "    model = get_model(algo, params)\n",
    "    r2 = cross_val_score(model, X, y, cv=5, scoring='r2').mean()\n",
    "    return r2\n",
    "\n",
    "# Run AutoML\n",
    "best_config = bayesian_search(objective, max_trials=200)\n",
    "```\n",
    "\n",
    "**Success Metrics**:\n",
    "- RÂ² > 0.92 on holdout test set\n",
    "- AutoML finds optimal config in <200 trials (<7 days compute)\n",
    "- Model generalizes across all 4 fabs (transfer learning)\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 2: Adaptive ATE Test Parameter Optimization ðŸ’° **$28.7M/year**\n",
    "\n",
    "**Objective**: Continuously optimize ATE test parameters to minimize test time while maximizing defect coverage\n",
    "\n",
    "**Business Value**:\n",
    "- **Baseline**: Static test program, 135 sec/device, 98.5% coverage\n",
    "- **Adaptive**: AutoML adjusts params weekly, 98 sec/device, 99.2% coverage\n",
    "- **Impact**: 27% faster testing Ã— 50M devices/year Ã— $0.58/device = **$28.7M/year**\n",
    "\n",
    "**Features**:\n",
    "- **Multi-objective**: Minimize test_time_sec, maximize defect_coverage_%\n",
    "- **Constraints**: Coverage â‰¥ 99%, test_time â‰¤ 120 sec\n",
    "- **AutoML method**: NSGA-II for Pareto front, stakeholder selects trade-off\n",
    "- **Continuous**: Re-optimize weekly as device characteristics drift\n",
    "\n",
    "**Implementation Hints**:\n",
    "```python\n",
    "# Multi-objective AutoML\n",
    "objectives = {\n",
    "    'test_time': lambda params: simulate_test_time(params),  # Minimize\n",
    "    'coverage': lambda params: estimate_coverage(params)      # Maximize\n",
    "}\n",
    "constraints = [\n",
    "    lambda params: simulate_test_time(params) <= 120,\n",
    "    lambda params: estimate_coverage(params) >= 99.0\n",
    "]\n",
    "\n",
    "# NSGA-II with constraints\n",
    "pareto_front = nsga_ii_optimize(objectives, constraints, max_generations=50)\n",
    "\n",
    "# Stakeholder selects preferred point\n",
    "selected = select_from_pareto(pareto_front, \n",
    "                              time_weight=0.6, coverage_weight=0.4)\n",
    "```\n",
    "\n",
    "**Success Metrics**:\n",
    "- Pareto front with 20-30 diverse solutions\n",
    "- Selected config: <100 sec test time, >99% coverage\n",
    "- Automated re-optimization pipeline (weekly)\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 3: Wafer Map CNN Architecture Search ðŸ’° **$19.8M/year**\n",
    "\n",
    "**Objective**: Use Neural Architecture Search (NAS) to find optimal CNN for wafer defect pattern classification\n",
    "\n",
    "**Business Value**:\n",
    "- **Baseline**: ResNet-50 (manual choice), 89% accuracy, 50M params\n",
    "- **NAS**: Custom architecture, 96% accuracy, 15M params (3Ã— smaller)\n",
    "- **Impact**: 7% accuracy improvement prevents $283M defect escapes â†’ **$19.8M/year** (7% of savings)\n",
    "\n",
    "**Features**:\n",
    "- **Search space**: Layers (5-20), filters per layer (32-512), kernel sizes (3,5,7), skip connections\n",
    "- **Search method**: ASHA for efficient NAS (early stop bad architectures)\n",
    "- **Constraints**: <50M parameters (deployment to edge ATE hardware)\n",
    "- **Data**: 300Ã—300 wafer map images, 8 defect classes\n",
    "\n",
    "**Implementation Hints**:\n",
    "```python\n",
    "# Define architecture search space\n",
    "search_space = {\n",
    "    'num_blocks': (3, 10),\n",
    "    'filters_block1': (32, 128), 'filters_block2': (64, 256),\n",
    "    'kernel_size': [3, 5, 7],\n",
    "    'use_skip_connections': [True, False],\n",
    "    'dropout': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "# ASHA for efficient NAS\n",
    "def train_architecture(arch_params, epochs):\n",
    "    model = build_cnn(arch_params)\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2)\n",
    "    return history.history['val_accuracy'][-1]\n",
    "\n",
    "# Run NAS with early stopping\n",
    "asha = ASHAOptimizer(search_space, train_architecture, \n",
    "                     min_fidelity=3, max_fidelity=50, reduction_factor=3)\n",
    "best_arch = asha.optimize(n_configs=100)\n",
    "```\n",
    "\n",
    "**Success Metrics**:\n",
    "- Accuracy > 95% on test set (8-class wafer map classification)\n",
    "- Model size < 30M parameters (deploy to ATE edge hardware)\n",
    "- NAS completes in <5 days (vs months of manual experimentation)\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 4: Binning Threshold Revenue Optimization ðŸ’° **$16.3M/year**\n",
    "\n",
    "**Objective**: Optimize binning thresholds across multiple fabs to maximize revenue from premium vs standard device sales\n",
    "\n",
    "**Business Value**:\n",
    "- **Baseline**: Fixed thresholds, 68% Bin 1 (premium), avg revenue $180/device\n",
    "- **Optimized**: Dynamic thresholds, 71% Bin 1, avg revenue $186/device\n",
    "- **Impact**: $6/device Ã— 2.7M devices/year = **$16.3M/year**\n",
    "\n",
    "**Features**:\n",
    "- **Revenue-aware**: Optimize for $ revenue, not just accuracy\n",
    "- **Multi-fab**: 4 fabs, 10 parameters/fab, 3 threshold values = 120 hyperparameters\n",
    "- **Constraints**: Bin 1 yield â‰¥ 65%, Bin 2 yield â‰¥ 25%, Fail rate â‰¤ 10%\n",
    "- **AutoML method**: Bayesian optimization with constraints\n",
    "\n",
    "**Implementation Hints**:\n",
    "```python\n",
    "# Revenue objective\n",
    "def revenue_objective(thresholds):\n",
    "    bins = classify_devices(test_data, thresholds)\n",
    "    revenue = (bins['bin1_count'] * 220 + \n",
    "               bins['bin2_count'] * 180 + \n",
    "               bins['bin3_count'] * 140)\n",
    "    return revenue / len(test_data)  # Per-device revenue\n",
    "\n",
    "# Constraints\n",
    "constraints = [\n",
    "    lambda t: classify_devices(test_data, t)['bin1_yield'] >= 0.65,\n",
    "    lambda t: classify_devices(test_data, t)['bin2_yield'] >= 0.25,\n",
    "    lambda t: classify_devices(test_data, t)['fail_rate'] <= 0.10\n",
    "]\n",
    "\n",
    "# Constrained Bayesian optimization\n",
    "best_thresholds = constrained_bayesian_opt(revenue_objective, constraints, \n",
    "                                           max_trials=500)\n",
    "```\n",
    "\n",
    "**Success Metrics**:\n",
    "- Per-device revenue > $185 (vs $180 baseline)\n",
    "- All constraints satisfied (bin yields, fail rate)\n",
    "- Generalizes across product generations (robust thresholds)\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### Project 5: E-Commerce Recommendation System AutoML ðŸ’° **$42M/year**\n",
    "\n",
    "**Objective**: Automatically optimize recommendation algorithm and hyperparameters for personalized product suggestions\n",
    "\n",
    "**Business Value**:\n",
    "- **Baseline**: Collaborative filtering, 18% click-through rate (CTR)\n",
    "- **AutoML**: Hybrid model, 24% CTR (33% improvement)\n",
    "- **Impact**: 6% CTR increase Ã— $700M revenue Ã— 0.01 revenue lift/CTR% = **$42M/year**\n",
    "\n",
    "**Features**:\n",
    "- **Algorithms**: Collaborative filtering, matrix factorization, deep learning, hybrid\n",
    "- **Hyperparameters**: Embedding size, regularization, learning rate, architecture\n",
    "- **AutoML**: Multi-objective (maximize CTR, minimize latency <50ms)\n",
    "- **Data**: 10M users, 500K products, 1B interactions\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 6: Medical Image Diagnosis NAS ðŸ’° **$55M/year**\n",
    "\n",
    "**Objective**: Find optimal CNN architecture for multi-disease classification from chest X-rays\n",
    "\n",
    "**Business Value**:\n",
    "- **Baseline**: DenseNet-121, 88% accuracy, radiologist reviews all cases\n",
    "- **NAS**: Custom architecture, 94% accuracy, reduce reviews by 40%\n",
    "- **Impact**: 6% accuracy improvement Ã— 2M scans/year Ã— $45/review Ã— 0.4 reduction = **$55M/year**\n",
    "\n",
    "**Features**:\n",
    "- **Search space**: 10^18 possible architectures (layers, filters, connections)\n",
    "- **Multi-disease**: 14 pathology classes, multi-label classification\n",
    "- **Efficiency**: <100M parameters (deploy to hospital edge devices)\n",
    "- **AutoML**: ASHA NAS with medical imaging data augmentation search\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 7: Fraud Detection Real-Time AutoML ðŸ’° **$38M/year**\n",
    "\n",
    "**Objective**: Continuously optimize fraud detection model with concept drift adaptation\n",
    "\n",
    "**Business Value**:\n",
    "- **Baseline**: Static XGBoost, 91% recall, retrain quarterly\n",
    "- **AutoML**: Adaptive model selection, 96% recall, retrain weekly\n",
    "- **Impact**: 5% recall improvement prevents $760M fraud â†’ **$38M/year** (5% of prevented losses)\n",
    "\n",
    "**Features**:\n",
    "- **Concept drift**: Fraud patterns change weekly\n",
    "- **Continuous AutoML**: Re-run HPO weekly on recent data\n",
    "- **Latency**: <10ms inference (real-time transaction approval)\n",
    "- **Explainability**: SHAP values for regulatory compliance\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 8: LLM Fine-Tuning Hyperparameter Search ðŸ’° **$31M/year**\n",
    "\n",
    "**Objective**: Optimize fine-tuning hyperparameters for domain-specific large language model\n",
    "\n",
    "**Business Value**:\n",
    "- **Baseline**: Default hyperparameters, 67% task accuracy\n",
    "- **AutoML**: Optimized fine-tuning, 82% task accuracy (15% improvement)\n",
    "- **Impact**: Reduce human annotation time by 50% Ã— 200K hours/year Ã— $75/hour = **$31M/year**\n",
    "\n",
    "**Features**:\n",
    "- **Hyperparameters**: Learning rate, batch size, warmup steps, LoRA rank, dropout\n",
    "- **Multi-fidelity**: Train on 10% data (cheap) â†’ 100% data (expensive)\n",
    "- **AutoML**: Bayesian optimization with early stopping\n",
    "- **Model**: 7B parameter LLaMA fine-tuned for legal document analysis\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’° Total Business Value: **$254.4M/year** across 8 projects\n",
    "\n",
    "**ROI Breakdown**:\n",
    "- Post-silicon projects: **$88.3M/year** (4 projects)\n",
    "- General AI/ML projects: **$166.1M/year** (4 projects)\n",
    "- AutoML reduces manual tuning time by 80-95%\n",
    "- Finds better hyperparameters than manual search\n",
    "- Enables continuous optimization (adapt to data drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b28c42",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "### When to Use AutoML & HPO\n",
    "\n",
    "**Use AutoML when:**\n",
    "- âœ… Hyperparameter tuning is time-consuming (>1 week manual work)\n",
    "- âœ… Compute budget allows exploration (100-1000 trials)\n",
    "- âœ… You need reproducible optimization (no manual guesswork)\n",
    "- âœ… Model performance is critical (business impact justifies cost)\n",
    "- âœ… Data/concept drift requires continuous re-tuning\n",
    "\n",
    "**Avoid AutoML when:**\n",
    "- âŒ Simple baseline sufficient (linear regression with defaults)\n",
    "- âŒ No compute budget (AutoML requires 10-100Ã— baseline training cost)\n",
    "- âŒ Search space too large (>20 hyperparameters â†’ curse of dimensionality)\n",
    "- âŒ Objective function noisy or expensive (>1 hour per trial)\n",
    "- âŒ Interpretability requirements (AutoML may select complex models)\n",
    "\n",
    "---\n",
    "\n",
    "### HPO Method Comparison\n",
    "\n",
    "| Method | Trials Needed | Sample Efficiency | Best For | Limitations |\n",
    "|--------|--------------|-------------------|----------|-------------|\n",
    "| **Grid Search** | 10^d (exponential) | âŒ Poor | Discrete, low-dim (<3 params) | Exponential cost, wasted trials |\n",
    "| **Random Search** | 100-1000 | âš ï¸ Fair | Baseline, continuous params | No learning from past trials |\n",
    "| **Bayesian Optimization** | 20-100 | âœ… Excellent | Expensive objectives, continuous | Assumes smooth objective |\n",
    "| **Evolutionary (CMA-ES)** | 50-200 | âœ… Good | Non-smooth, mixed discrete/continuous | Requires large population |\n",
    "| **NSGA-II** | 100-500 | âš ï¸ Fair | Multi-objective problems | Slower convergence |\n",
    "| **ASHA/Hyperband** | 100-1000 | âœ… Excellent | Deep learning (multi-fidelity) | Needs fidelity dimension |\n",
    "\n",
    "**Decision Framework**:\n",
    "```\n",
    "if num_hyperparameters <= 3 and discrete:\n",
    "    â†’ Grid Search (exhaustive)\n",
    "elif objective_evaluation_time < 10 seconds:\n",
    "    â†’ Random Search (cheap, good baseline)\n",
    "elif multi_objective:\n",
    "    â†’ NSGA-II or Bayesian multi-objective\n",
    "elif has_fidelity_dimension (epochs, data_size):\n",
    "    â†’ ASHA or Hyperband (early stopping)\n",
    "elif objective_smooth and expensive:\n",
    "    â†’ Bayesian Optimization (sample efficient)\n",
    "else:\n",
    "    â†’ CMA-ES (robust, general-purpose)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Production AutoML Stack\n",
    "\n",
    "**Open-Source Frameworks**:\n",
    "1. **Optuna** (Recommended for most use cases)\n",
    "   - Modern, Pythonic API\n",
    "   - Built-in pruning (early stopping)\n",
    "   - Supports distributed optimization (RDB storage)\n",
    "   - Visualization dashboard\n",
    "   \n",
    "2. **Ray Tune** (Recommended for distributed training)\n",
    "   - Integrates with Ray (distributed compute)\n",
    "   - ASHA, PBT (Population Based Training)\n",
    "   - Scalable to 1000s of GPUs\n",
    "   \n",
    "3. **Hyperopt** (Mature, stable)\n",
    "   - TPE (Tree-structured Parzen Estimator) algorithm\n",
    "   - Large community, battle-tested\n",
    "   - MongoDB backend for distributed trials\n",
    "   \n",
    "4. **AutoGluon** (Recommended for AutoML newcomers)\n",
    "   - End-to-end AutoML (preprocessing + model selection + HPO)\n",
    "   - State-of-the-art ensembles\n",
    "   - Minimal code (single function call)\n",
    "\n",
    "**Commercial Platforms**:\n",
    "- **Google Cloud AutoML**: Fully managed, expensive\n",
    "- **AWS SageMaker Automatic Model Tuning**: Integrated with AWS\n",
    "- **Azure Machine Learning**: Hyperdrive for HPO\n",
    "- **H2O Driverless AI**: Enterprise AutoML\n",
    "\n",
    "**Example: Optuna for Yield Prediction**\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    # Train model\n",
    "    model = XGBRegressor(n_estimators=n_estimators, \n",
    "                        max_depth=max_depth,\n",
    "                        learning_rate=learning_rate)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    return scores.mean()\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, timeout=3600)\n",
    "\n",
    "print(f\"Best RÂ²: {study.best_value:.4f}\")\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Foundations\n",
    "\n",
    "**Gaussian Process Regression**:\n",
    "- **Prior**: f ~ GP(Î¼, k)\n",
    "- **Posterior**: f|D ~ N(Î¼_post, Î£_post)\n",
    "  - Î¼_post(x) = k(x, X)(K + ÏƒÂ²I)^(-1)y\n",
    "  - Î£_post(x, x') = k(x, x') - k(x, X)(K + ÏƒÂ²I)^(-1)k(X, x')\n",
    "- **Acquisition**: EI(x) = E[max(f(x) - f*, 0)]\n",
    "\n",
    "**Multi-Objective Optimization**:\n",
    "- **Pareto dominance**: xâ‚ â‰» xâ‚‚ âŸº âˆ€i: f_i(xâ‚) â‰¥ f_i(xâ‚‚) âˆ§ âˆƒj: f_j(xâ‚) > f_j(xâ‚‚)\n",
    "- **Crowding distance**: d(i) = Î£_m |f_m(i+1) - f_m(i-1)| / (f_m_max - f_m_min)\n",
    "- **NSGA-II**: Non-dominated sorting + crowding distance selection\n",
    "\n",
    "**Hyperband Budget Allocation**:\n",
    "- **Successive halving**: n configs, keep top n/Î· at each rung\n",
    "- **Total budget**: B = (log_Î·(R) + 1) Ã— n Ã— r\n",
    "  - R = max fidelity, r = min fidelity, Î· = reduction factor\n",
    "- **Example**: 81 configs, Î·=3, R=27, r=1 â†’ B = 324 epochs\n",
    "\n",
    "---\n",
    "\n",
    "### Cost-Performance Trade-offs\n",
    "\n",
    "**AutoML Costs** (per optimization run):\n",
    "- **Random Search** (100 trials): $1,000 - $10,000\n",
    "- **Bayesian Optimization** (30 trials): $300 - $3,000\n",
    "- **ASHA** (100 configs, early stopping): $500 - $5,000\n",
    "- **Full Grid Search** (10^5 trials): $100,000+ (infeasible!)\n",
    "\n",
    "**Time Savings**:\n",
    "- **Manual tuning**: 1-4 weeks (expert data scientist)\n",
    "- **Random search**: 1-3 days (automated)\n",
    "- **Bayesian optimization**: 4-12 hours (sample efficient)\n",
    "- **ASHA**: 1-2 days (parallel, early stopping)\n",
    "\n",
    "**ROI Example** (Post-Silicon Yield Prediction):\n",
    "- **Manual tuning**: 3 weeks Ã— $5K/week = $15K, RÂ² = 0.85\n",
    "- **Bayesian AutoML**: 1 day Ã— $2K = $2K, RÂ² = 0.93\n",
    "- **Performance gain**: 0.08 RÂ² â†’ $7.9M/year value\n",
    "- **ROI**: ($7.9M - $0) / ($2K - $15K) = **Infinite** (saves time AND money)\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls & Solutions\n",
    "\n",
    "**Pitfall 1: Overfitting to validation set**\n",
    "- **Problem**: Optimize hyperparameters on validation set â†’ leak information\n",
    "- **Solution**: Use nested cross-validation\n",
    "  - Outer loop: Train/test split\n",
    "  - Inner loop: HPO on training set (with validation)\n",
    "  - Report performance on held-out test set\n",
    "\n",
    "**Pitfall 2: Search space too large**\n",
    "- **Problem**: 20 hyperparameters Ã— 10 values = 10^20 combinations\n",
    "- **Solution**: \n",
    "  - Start with important hyperparameters (learning rate, regularization)\n",
    "  - Fix less important hyperparameters to defaults\n",
    "  - Use literature or prior knowledge to narrow ranges\n",
    "\n",
    "**Pitfall 3: Noisy objective function**\n",
    "- **Problem**: Stochastic training â†’ high variance in performance\n",
    "- **Solution**:\n",
    "  - Average over multiple runs (3-5 runs per config)\n",
    "  - Use Bayesian optimization with noise modeling\n",
    "  - Increase training epochs for more stable estimates\n",
    "\n",
    "**Pitfall 4: Objective function too expensive**\n",
    "- **Problem**: Each trial takes 6 hours â†’ AutoML takes months\n",
    "- **Solution**:\n",
    "  - Use multi-fidelity optimization (ASHA, Hyperband)\n",
    "  - Train on subset of data (10% â†’ 100% progressive)\n",
    "  - Use proxy metrics (validation loss at epoch 5 correlates with final accuracy)\n",
    "\n",
    "**Pitfall 5: Ignoring domain constraints**\n",
    "- **Problem**: AutoML finds 500M parameter model (can't deploy to edge device)\n",
    "- **Solution**:\n",
    "  - Add constraints to search space (max_params â‰¤ 50M)\n",
    "  - Use penalized objective (accuracy - 0.01 Ã— num_params)\n",
    "  - Multi-objective optimization (accuracy vs model size)\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps in Your Learning Path\n",
    "\n",
    "**Prerequisites** (should know):\n",
    "- âœ… Machine learning fundamentals (supervised learning, cross-validation)\n",
    "- âœ… Hyperparameters vs parameters distinction\n",
    "- âœ… Overfitting and regularization concepts\n",
    "\n",
    "**You Now Understand**:\n",
    "- âœ… Grid search vs random search vs Bayesian optimization\n",
    "- âœ… Gaussian Process surrogate models and acquisition functions\n",
    "- âœ… Multi-objective optimization with NSGA-II and Pareto fronts\n",
    "- âœ… Early stopping and multi-fidelity methods (ASHA, Hyperband)\n",
    "- âœ… Production AutoML frameworks (Optuna, Ray Tune)\n",
    "\n",
    "**Continue Learning**:\n",
    "- **Next**: Notebook 159 - ML Model Compression & Quantization\n",
    "- **Related**: Notebook 157 - Distributed Training (parallelize HPO)\n",
    "- **Advanced**: Neural Architecture Search (DARTS, ENAS)\n",
    "- **Production**: Notebook 156 - ML Pipeline Orchestration (automate AutoML)\n",
    "\n",
    "**Hands-On Practice**:\n",
    "1. Implement Bayesian optimization from scratch (Gaussian Process + EI)\n",
    "2. Run Optuna on your dataset (compare to manual tuning)\n",
    "3. Set up ASHA for deep learning model (image classification)\n",
    "4. Build multi-objective HPO for accuracy vs latency trade-off\n",
    "5. Deploy AutoML pipeline with MLflow experiment tracking\n",
    "\n",
    "**Advanced Topics** (explore on your own):\n",
    "- **Transfer learning for HPO**: Use hyperparameters from similar datasets\n",
    "- **Meta-learning**: Learn to learn hyperparameters across tasks\n",
    "- **Automated feature engineering**: AutoML for preprocessing\n",
    "- **Neural Architecture Search**: Differentiable architecture search (DARTS)\n",
    "- **Population-based training**: Evolve hyperparameters during training\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "**AutoML democratizes machine learning** by automating the tedious hyperparameter tuning process. Instead of spending weeks manually experimenting, **Bayesian optimization finds near-optimal configurations in hours**. Multi-objective methods like NSGA-II reveal **trade-offs between conflicting objectives** (accuracy vs latency). Early stopping strategies like ASHA reduce costs by **75-85% through intelligent trial pruning**.\n",
    "\n",
    "**Business impact is substantial**: Post-silicon validation benefits from automated yield prediction model selection ($23.5M/year), adaptive ATE test optimization ($28.7M/year), and wafer map CNN architecture search ($19.8M/year). General AI/ML applications see similar gains in e-commerce recommendations ($42M/year), medical diagnosis ($55M/year), and fraud detection ($38M/year).\n",
    "\n",
    "**Production deployment requires careful consideration**: Choose the right AutoML method for your problem (Bayesian for expensive objectives, ASHA for deep learning, NSGA-II for multi-objective), avoid common pitfalls (overfitting to validation set, noisy objectives), and use mature frameworks (Optuna, Ray Tune) for reliability.\n",
    "\n",
    "**The future is automated**: As models grow larger and more complex, manual hyperparameter tuning becomes infeasible. AutoML is not a luxuryâ€”**it's a necessity for competitive ML systems**. Start with simple Bayesian optimization, graduate to multi-fidelity methods, and eventually build continuous AutoML pipelines that adapt to data drift.\n",
    "\n",
    "**Your next step**: Apply AutoML to your most important model. Measure the time savings and performance gains. You'll never go back to manual tuning.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You now have production-ready AutoML skills that save months of manual work and millions in business value!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
