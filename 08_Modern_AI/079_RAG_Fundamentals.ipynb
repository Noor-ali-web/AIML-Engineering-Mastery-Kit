{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d26c9cb6",
   "metadata": {},
   "source": [
    "# 079: RAG (Retrieval-Augmented Generation) Fundamentals\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** the RAG architecture and why it's crucial for LLM applications\n",
    "- **Implement** document chunking and embedding strategies from scratch\n",
    "- **Build** semantic search systems using vector databases (FAISS)\n",
    "- **Create** production RAG pipelines with context retrieval and generation\n",
    "- **Apply** RAG to semiconductor test documentation and failure analysis\n",
    "- **Evaluate** RAG systems using retrieval and generation metrics\n",
    "\n",
    "## üìö What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines:\n",
    "1. **Information Retrieval** - Finding relevant documents from a knowledge base\n",
    "2. **Language Generation** - Using retrieved context to generate accurate responses\n",
    "\n",
    "**Why RAG?**\n",
    "- ‚úÖ Reduces hallucinations by grounding LLM responses in factual data\n",
    "- ‚úÖ Enables LLMs to access current/private information (not in training data)\n",
    "- ‚úÖ More cost-effective than fine-tuning for domain-specific knowledge\n",
    "- ‚úÖ Transparent - can trace answers back to source documents\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Technical Documentation Search**\n",
    "- Query: \"What are the voltage specifications for LPDDR5?\"\n",
    "- Retrieve: Relevant sections from datasheets, test specs\n",
    "- Generate: Concise answer with specific voltage ranges and conditions\n",
    "\n",
    "**Failure Analysis Assistant**\n",
    "- Query: \"Similar failures to wafer W123 die position (50, 75)?\"\n",
    "- Retrieve: Historical failure reports, wafer maps, test logs\n",
    "- Generate: Root cause analysis with similar case references\n",
    "\n",
    "**Test Parameter Recommendations**\n",
    "- Query: \"Optimal test coverage for power consumption validation?\"\n",
    "- Retrieve: Test plans, yield correlation data, best practices\n",
    "- Generate: Recommended test parameters and sequencing\n",
    "\n",
    "## üîÑ RAG Architecture Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Documents] --> B[Chunking]\n",
    "    B --> C[Embedding Model]\n",
    "    C --> D[Vector Database]\n",
    "    \n",
    "    E[User Query] --> F[Query Embedding]\n",
    "    F --> G[Semantic Search]\n",
    "    D --> G\n",
    "    \n",
    "    G --> H[Top-K Retrieved Docs]\n",
    "    H --> I[Context Assembly]\n",
    "    E --> I\n",
    "    \n",
    "    I --> J[LLM with Context]\n",
    "    J --> K[Generated Response]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style D fill:#fff4e1\n",
    "    style J fill:#f0e1ff\n",
    "    style K fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 072: GPT & Large Language Models (LLM fundamentals)\n",
    "- 078: Multimodal LLMs (embedding concepts)\n",
    "- 058: Transformers & Self-Attention (attention mechanism)\n",
    "\n",
    "**Next Steps:**\n",
    "- 080: Advanced RAG Techniques (hybrid search, re-ranking)\n",
    "- 083: AI Agents (RAG as agent tool)\n",
    "- 085: Vector Databases (scaling RAG systems)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build comprehensive RAG systems from the ground up! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350981fc",
   "metadata": {},
   "source": [
    "## **Why Retrieval-Augmented Generation?**\n",
    "\n",
    "### **The LLM Knowledge Problem**\n",
    "\n",
    "**Before RAG:**\n",
    "- ‚ùå LLMs only know information from training data (static, outdated)\n",
    "- ‚ùå Cannot access private/proprietary documents\n",
    "- ‚ùå Hallucinate when uncertain (generate plausible but incorrect information)\n",
    "- ‚ùå Cannot cite sources (no transparency)\n",
    "\n",
    "**After RAG:**\n",
    "- ‚úÖ Access current and private information dynamically\n",
    "- ‚úÖ Ground responses in retrieved factual documents\n",
    "- ‚úÖ Cite sources for transparency and verification\n",
    "- ‚úÖ More cost-effective than fine-tuning for knowledge updates\n",
    "\n",
    "### **The Hallucination Crisis**\n",
    "\n",
    "**Example hallucination scenarios:**\n",
    "- **General LLM:** \"Tell me about the XYZ-3000 chip specifications\" ‚Üí Generates plausible but entirely fictional specifications\n",
    "- **RAG System:** Retrieves actual XYZ-3000 datasheet ‚Üí Cites exact voltage ranges, frequencies from real document\n",
    "\n",
    "**Research shows:** RAG reduces hallucinations by **60-80%** in knowledge-intensive tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Semiconductor Test Documentation Challenges**\n",
    "\n",
    "**The documentation problem:**\n",
    "- üìö **Thousands of documents:** Test specs, datasheets, failure reports, design docs\n",
    "- üîç **Hard to search:** Technical jargon, buried in PDFs, inconsistent terminology  \n",
    "- ‚è∞ **Time-critical:** Engineers need answers during debug sessions (not hours later)\n",
    "- üîê **Confidential:** Cannot use public LLMs with proprietary data\n",
    "\n",
    "**RAG solution value:**\n",
    "- ‚ö° **Instant answers:** Query \"LPDDR5 timing specs\" ‚Üí retrieve relevant sections ‚Üí generate concise answer\n",
    "- üí∞ **Cost savings:** Reduce engineer search time from 30min to 30sec (40√ó faster)\n",
    "- üéØ **Accuracy:** Ground responses in actual test documents (eliminate guesswork)\n",
    "- üîí **Security:** Deploy RAG system on-premises with internal docs\n",
    "\n",
    "**ROI calculation:**\n",
    "- 100 engineers √ó 2 hours/week searching docs = 200 engineer-hours/week\n",
    "- RAG reduces search time by 80% = 160 hours saved/week\n",
    "- At $100/hour loaded cost = **$16K/week savings = $832K/year**\n",
    "\n",
    "---\n",
    "\n",
    "## **What We'll Build**\n",
    "\n",
    "### **1. Educational: RAG from Scratch (NumPy + Simple Embeddings)**\n",
    "\n",
    "Implement core RAG components to understand the mechanics:\n",
    "- Document chunking (fixed-size, sentence-based, semantic)\n",
    "- Simple embedding model (TF-IDF ‚Üí dense vectors)\n",
    "- Cosine similarity search\n",
    "- Context assembly for LLM prompt\n",
    "\n",
    "### **2. Production: Semantic Search with Sentence-BERT + FAISS**\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Documents ‚Üí Chunking (512 tokens) \n",
    "         ‚Üí Sentence-BERT embeddings (384-dim)\n",
    "         ‚Üí FAISS index (IVF + PQ for scale)\n",
    "         ‚Üí Top-K retrieval (K=3-5)\n",
    "         ‚Üí LLM with context\n",
    "```\n",
    "\n",
    "**Performance targets:**\n",
    "- Index 100K document chunks in <5 minutes\n",
    "- Query latency <100ms for top-5 retrieval\n",
    "- Retrieval accuracy (R@5) ‚â•90%\n",
    "\n",
    "### **3. Post-Silicon Validation: Test Spec RAG System**\n",
    "\n",
    "**Dataset:** 500+ semiconductor test specification documents (PDFs, 50K chunks).\n",
    "\n",
    "**Queries:**\n",
    "- \"What is the voltage range for LPDDR5 DQ pins?\"\n",
    "- \"Maximum current specification for power rail VDD_CORE?\"\n",
    "- \"Required temperature range for automotive qualification?\"\n",
    "\n",
    "**Evaluation metrics:**\n",
    "- **Retrieval:** Precision@K, Recall@K, MRR (Mean Reciprocal Rank)\n",
    "- **Generation:** ROUGE-L, BERTScore, human evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## **Notebook Roadmap**\n",
    "\n",
    "### **Part 1: Mathematical Foundations** (Cell 2)\n",
    "- Embedding mathematics\n",
    "- Similarity metrics (cosine, dot product, L2)\n",
    "- Vector space retrieval theory\n",
    "\n",
    "### **Part 2: Document Chunking Strategies** (Cells 3-5)\n",
    "- Fixed-size chunking\n",
    "- Sentence-aware chunking\n",
    "- Semantic chunking\n",
    "- Overlap strategies\n",
    "\n",
    "### **Part 3: Embeddings from Scratch** (Cells 6-8)\n",
    "- TF-IDF vectorization\n",
    "- Dense embedding projection\n",
    "- Simple semantic search\n",
    "\n",
    "### **Part 4: Production Embeddings** (Cells 9-11)\n",
    "- Sentence-BERT (all-MiniLM-L6-v2)\n",
    "- OpenAI embeddings (text-embedding-3-small)\n",
    "- Embedding comparison\n",
    "\n",
    "### **Part 5: Vector Search with FAISS** (Cells 12-15)\n",
    "- FAISS index types (Flat, IVF, HNSW)\n",
    "- Building vector database\n",
    "- Efficient similarity search\n",
    "- Scaling to millions of vectors\n",
    "\n",
    "### **Part 6: Complete RAG Pipeline** (Cells 16-20)\n",
    "- End-to-end RAG system\n",
    "- Query processing\n",
    "- Context assembly\n",
    "- LLM integration (OpenAI/local)\n",
    "- Response generation\n",
    "\n",
    "### **Part 7: Post-Silicon Use Cases** (Cells 21-24)\n",
    "- Test specification search\n",
    "- Failure report retrieval\n",
    "- Design document Q&A\n",
    "- Parameter recommendation\n",
    "\n",
    "### **Part 8: Evaluation & Metrics** (Cells 25-27)\n",
    "- Retrieval metrics (Precision@K, Recall@K, MRR, NDCG)\n",
    "- Generation metrics (ROUGE, BLEU, BERTScore)\n",
    "- End-to-end evaluation\n",
    "\n",
    "### **Part 9: Real-World Projects** (Cell 28)\n",
    "- 8 production-ready RAG project ideas\n",
    "\n",
    "### **Part 10: Best Practices & Takeaways** (Cell 29)\n",
    "- When to use RAG vs fine-tuning\n",
    "- Chunking strategies guide\n",
    "- Embedding model selection\n",
    "- Production deployment patterns\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Concepts**\n",
    "\n",
    "| Concept | Definition | Why It Matters |\n",
    "|---------|------------|----------------|\n",
    "| **Embedding** | Dense vector representation of text | Captures semantic meaning for similarity search |\n",
    "| **Vector Database** | Specialized DB for embedding storage/search | Enables fast similarity queries (sub-100ms) |\n",
    "| **Chunking** | Splitting documents into smaller pieces | Balances context vs precision in retrieval |\n",
    "| **Semantic Search** | Finding similar meaning (not keywords) | Retrieves \"battery life\" when searching \"power consumption\" |\n",
    "| **Top-K Retrieval** | Return K most similar documents | Provides context without overwhelming LLM |\n",
    "| **Cosine Similarity** | Measure of vector angle (0=orthogonal, 1=identical) | Standard metric for semantic similarity |\n",
    "| **Context Window** | Max tokens LLM can process | Limits retrieved context (4K-128K tokens) |\n",
    "| **Hallucination** | LLM generating false information | RAG reduces by grounding in real documents |\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "**Required notebooks:**\n",
    "- **072: GPT & Large Language Models** - Understanding LLM capabilities and limitations\n",
    "- **078: Multimodal LLMs** - Embedding concepts and representation learning\n",
    "\n",
    "**Helpful but optional:**\n",
    "- **058: Transformers & Self-Attention** - Architecture behind embedding models\n",
    "- **071: Transformers & BERT** - Sentence-BERT foundation\n",
    "\n",
    "**Skills:**\n",
    "- Python programming (classes, decorators, type hints)\n",
    "- NumPy for vector operations\n",
    "- Basic understanding of cosine similarity\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Path Context**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[072: GPT/LLMs] --> B[079: RAG Fundamentals]\n",
    "    C[078: Multimodal LLMs] --> B\n",
    "    B --> D[080: Advanced RAG]\n",
    "    B --> E[083: AI Agents]\n",
    "    B --> F[085: Vector Databases]\n",
    "    \n",
    "    D --> G[084: LangChain]\n",
    "    E --> G\n",
    "    F --> G\n",
    "    \n",
    "    style B fill:#4CAF50,color:#fff\n",
    "    style D fill:#e1f5ff\n",
    "    style E fill:#e1f5ff\n",
    "    style F fill:#e1f5ff\n",
    "```\n",
    "\n",
    "**Current Focus:** 079 - RAG Fundamentals (you are here! üéØ)\n",
    "\n",
    "**Next Steps:**\n",
    "- **080: Advanced RAG Techniques** - Hybrid search, re-ranking, query expansion\n",
    "- **083: AI Agents** - Use RAG as agent tool for complex reasoning\n",
    "- **085: Vector Databases** - Scale RAG to millions/billions of documents\n",
    "\n",
    "---\n",
    "\n",
    "Let's build production-grade RAG systems! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e40d6",
   "metadata": {},
   "source": [
    "## üìê Part 1: Mathematical Foundations\n",
    "\n",
    "### RAG Components Mathematics\n",
    "\n",
    "**1. Document Embedding**\n",
    "\n",
    "For document chunk $d_i$, embedding function $f_{embed}$:\n",
    "\n",
    "$$\\mathbf{v}_i = f_{embed}(d_i) \\in \\mathbb{R}^{d}$$\n",
    "\n",
    "Where $d$ is embedding dimension (typically 384, 768, or 1536).\n",
    "\n",
    "**2. Semantic Similarity**\n",
    "\n",
    "Cosine similarity between query $q$ and document $d_i$:\n",
    "\n",
    "$$\\text{sim}(q, d_i) = \\frac{\\mathbf{v}_q \\cdot \\mathbf{v}_i}{||\\mathbf{v}_q|| \\cdot ||\\mathbf{v}_i||} = \\frac{\\sum_{j=1}^{d} v_{q,j} \\cdot v_{i,j}}{\\sqrt{\\sum_{j=1}^{d} v_{q,j}^2} \\cdot \\sqrt{\\sum_{j=1}^{d} v_{i,j}^2}}$$\n",
    "\n",
    "**3. Top-K Retrieval**\n",
    "\n",
    "Retrieve top $k$ most similar documents:\n",
    "\n",
    "$$D_{top-k} = \\{d_i : \\text{sim}(q, d_i) \\text{ in top } k \\text{ values}\\}$$\n",
    "\n",
    "**4. Context Assembly**\n",
    "\n",
    "Concatenate retrieved documents with query:\n",
    "\n",
    "$$\\text{context} = [d_1, d_2, ..., d_k] \\oplus q$$\n",
    "\n",
    "Where $\\oplus$ denotes concatenation with special tokens.\n",
    "\n",
    "**5. Conditional Generation**\n",
    "\n",
    "LLM generates response conditioned on context:\n",
    "\n",
    "$$P(y | q, D_{top-k}) = \\prod_{t=1}^{T} P(y_t | y_{<t}, q, D_{top-k})$$\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "**Information Bottleneck:** LLMs have limited context windows (4k-128k tokens). RAG efficiently uses this by retrieving only relevant information.\n",
    "\n",
    "**Factual Grounding:** Retrieved documents provide factual basis, reducing hallucinations.\n",
    "\n",
    "**Dynamic Knowledge:** Can update knowledge base without retraining the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19811bda",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import core libraries for RAG implementation\n",
    "\n",
    "**Key Libraries:**\n",
    "- **numpy**: Vector operations for embeddings and similarity calculations\n",
    "- **sentence-transformers**: Pre-trained embedding models (SBERT)\n",
    "- **faiss**: Efficient similarity search and vector database\n",
    "- **typing**: Type hints for code clarity\n",
    "\n",
    "**Why These Libraries:**\n",
    "- **Sentence-BERT**: State-of-the-art semantic text embeddings\n",
    "- **FAISS**: Facebook's vector search library (billions of vectors, millisecond latency)\n",
    "- **NumPy**: Foundation for all numerical computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For production RAG (install if needed: pip install sentence-transformers faiss-cpu)\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "    PRODUCTION_LIBS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PRODUCTION_LIBS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Production libraries not installed. Install with:\")\n",
    "    print(\"   pip install sentence-transformers faiss-cpu\")\n",
    "    print(\"   (Educational from-scratch implementation will still work)\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"   Production RAG libraries available: {PRODUCTION_LIBS_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2205e",
   "metadata": {},
   "source": [
    "## üìÑ Part 2: Document Chunking Strategies\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "**Problem:** LLMs have context limits (4K-128K tokens). Large documents must be split into retrievable chunks.\n",
    "\n",
    "**Tradeoffs:**\n",
    "- **Small chunks** (100-200 tokens): Precise retrieval, but may lose context\n",
    "- **Large chunks** (500-1000 tokens): More context, but less precise retrieval\n",
    "- **Optimal:** 300-500 tokens with 50-100 token overlap\n",
    "\n",
    "### Chunking Strategies\n",
    "\n",
    "**1. Fixed-Size Chunking**\n",
    "- Split every N tokens/characters\n",
    "- Simple, fast, but breaks mid-sentence\n",
    "\n",
    "**2. Sentence-Aware Chunking**\n",
    "- Respect sentence boundaries\n",
    "- Better coherence, variable chunk sizes\n",
    "\n",
    "**3. Semantic Chunking**\n",
    "- Split at topic/section boundaries\n",
    "- Best quality, computationally expensive\n",
    "\n",
    "**4. Overlap Strategy**\n",
    "- Add N-token overlap between chunks\n",
    "- Preserves context across boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920b7e7",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement document chunking strategies from scratch\n",
    "\n",
    "**Key Implementations:**\n",
    "- **FixedSizeChunker**: Simple character-based splitting\n",
    "- **SentenceChunker**: Respect sentence boundaries using regex\n",
    "- **OverlapChunker**: Add configurable overlap between chunks\n",
    "- **ChunkMetadata**: Track chunk source and position for traceability\n",
    "\n",
    "**Why This Matters:** Proper chunking is critical for RAG accuracy - too large loses precision, too small loses context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c23ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Metadata for document chunks\"\"\"\n",
    "    doc_id: str\n",
    "    chunk_index: int\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "    overlap_with_previous: int = 0\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"Base class for document chunking strategies\"\"\"\n",
    "    \n",
    "    def chunk(self, text: str, doc_id: str = \"doc_0\") -> List[Tuple[str, ChunkMetadata]]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class FixedSizeChunker(DocumentChunker):\n",
    "    \"\"\"Fixed-size character chunking\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "    \n",
    "    def chunk(self, text: str, doc_id: str = \"doc_0\") -> List[Tuple[str, ChunkMetadata]]:\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            chunk_text = text[start:end]\n",
    "            \n",
    "            metadata = ChunkMetadata(\n",
    "                doc_id=doc_id,\n",
    "                chunk_index=chunk_index,\n",
    "                start_char=start,\n",
    "                end_char=end,\n",
    "                overlap_with_previous=self.overlap if chunk_index > 0 else 0\n",
    "            )\n",
    "            \n",
    "            chunks.append((chunk_text, metadata))\n",
    "            \n",
    "            # Move forward with overlap\n",
    "            start = end - self.overlap\n",
    "            chunk_index += 1\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class SentenceChunker(DocumentChunker):\n",
    "    \"\"\"Sentence-aware chunking (respects sentence boundaries)\"\"\"\n",
    "    \n",
    "    def __init__(self, target_size: int = 500, max_size: int = 700):\n",
    "        self.target_size = target_size\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def chunk(self, text: str, doc_id: str = \"doc_0\") -> List[Tuple[str, ChunkMetadata]]:\n",
    "        # Split into sentences using simple regex\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        start_char = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_len = len(sentence)\n",
    "            \n",
    "            if current_size + sentence_len > self.max_size and current_chunk:\n",
    "                # Save current chunk\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                metadata = ChunkMetadata(\n",
    "                    doc_id=doc_id,\n",
    "                    chunk_index=chunk_index,\n",
    "                    start_char=start_char,\n",
    "                    end_char=start_char + len(chunk_text)\n",
    "                )\n",
    "                chunks.append((chunk_text, metadata))\n",
    "                \n",
    "                # Start new chunk\n",
    "                start_char += len(chunk_text) + 1\n",
    "                current_chunk = [sentence]\n",
    "                current_size = sentence_len\n",
    "                chunk_index += 1\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_size += sentence_len\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            metadata = ChunkMetadata(\n",
    "                doc_id=doc_id,\n",
    "                chunk_index=chunk_index,\n",
    "                start_char=start_char,\n",
    "                end_char=start_char + len(chunk_text)\n",
    "            )\n",
    "            chunks.append((chunk_text, metadata))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "print(\"‚úÖ Document chunking classes defined\")\n",
    "print(f\"   - FixedSizeChunker: {self.chunk_size} chars with {self.overlap} overlap\" if False else \"\")\n",
    "print(f\"   - SentenceChunker: Target {500} chars, max {700} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1f97a",
   "metadata": {},
   "source": [
    "### üìù Testing Chunking Strategies\n",
    "\n",
    "**Purpose:** Test different chunking approaches on semiconductor documentation\n",
    "\n",
    "**Test Document:** Sample LPDDR5 datasheet excerpt with voltage specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ef656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample semiconductor test specification document\n",
    "sample_doc = \"\"\"\n",
    "LPDDR5 Memory Device Specifications - Voltage Requirements\n",
    "\n",
    "VDD Supply Voltage:\n",
    "The VDD supply voltage shall be maintained between 1.05V and 1.15V during normal operation. \n",
    "Operating outside this range may result in device failure or data corruption.\n",
    "\n",
    "VDDQ I/O Supply Voltage:\n",
    "The VDDQ I/O supply voltage for data signals must be in the range of 0.45V to 0.55V.\n",
    "This voltage powers the output drivers and input receivers for DQ, DQS signals.\n",
    "\n",
    "Temperature Operating Range:\n",
    "Commercial grade: 0¬∞C to 85¬∞C ambient temperature.\n",
    "Automotive grade: -40¬∞C to 125¬∞C junction temperature.\n",
    "\n",
    "Test Requirements:\n",
    "All devices must pass parametric test coverage including DC voltage tests, frequency tests,\n",
    "and power consumption validation. Minimum test coverage: 95% for production release.\n",
    "\n",
    "Failure Analysis Protocol:\n",
    "If yield drops below 90%, initiate root cause analysis. Common failure modes include:\n",
    "voltage regulator issues, timing violations, or spatial defects on wafer.\n",
    "\"\"\"\n",
    "\n",
    "# Test chunking strategies\n",
    "print(\"=\" * 70)\n",
    "print(\"FIXED-SIZE CHUNKING (200 chars, 20 overlap)\")\n",
    "print(\"=\" * 70)\n",
    "fixed_chunker = FixedSizeChunker(chunk_size=200, overlap=20)\n",
    "fixed_chunks = fixed_chunker.chunk(sample_doc, doc_id=\"LPDDR5_spec\")\n",
    "\n",
    "for i, (chunk, metadata) in enumerate(fixed_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Chars: {metadata.start_char}-{metadata.end_char}\")\n",
    "    print(f\"  Text: {chunk[:80]}...\")\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(fixed_chunks)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SENTENCE-AWARE CHUNKING\")\n",
    "print(\"=\" * 70)\n",
    "sentence_chunker = SentenceChunker(target_size=300, max_size=400)\n",
    "sentence_chunks = sentence_chunker.chunk(sample_doc, doc_id=\"LPDDR5_spec\")\n",
    "\n",
    "for i, (chunk, metadata) in enumerate(sentence_chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Size: {len(chunk)} chars\")\n",
    "    print(f\"  Text: {chunk[:100]}...\")\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(sentence_chunks)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
