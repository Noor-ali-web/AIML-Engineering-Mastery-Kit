{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 086: RAG + Fine-Tuning - Combining Retrieval with Adaptation\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Master** When to fine-tune vs RAG\n",
    "- **Master** LoRA and QLoRA\n",
    "- **Master** Retrieval-augmented fine-tuning\n",
    "- **Master** Domain vocabulary\n",
    "- **Master** Test terminology\n",
    "\n",
    "## üìö Overview\n",
    "\n",
    "This notebook covers RAG + Fine-Tuning - Combining Retrieval with Adaptation.\n",
    "\n",
    "**Post-silicon applications**: Production-grade RAG systems for semiconductor validation.\n",
    "\n",
    "---\n",
    "\n",
    "Let's build! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö What is RAG + Fine-Tuning?\n",
    "\n",
    "**RAG + Fine-Tuning** combines the best of both worlds: retrieval for up-to-date information and fine-tuning for domain expertise. Instead of choosing between RAG or fine-tuning, we do both for optimal results.\n",
    "\n",
    "**RAG vs Fine-Tuning vs Both:**\n",
    "| Approach | Pros | Cons | Best For |\n",
    "|----------|------|------|----------|\n",
    "| **RAG Only** | Up-to-date, cite sources, lower cost ($0.15/query) | Generic LLM may not understand domain jargon | General Q&A |\n",
    "| **Fine-Tune Only** | Deep domain knowledge, no retrieval latency | Outdated (frozen at training time), expensive ($100K) | Domain expertise |\n",
    "| **RAG + Fine-Tune** | Domain expertise + up-to-date info + citations | Higher complexity, fine-tuning cost ($10K) | Production systems |\n",
    "\n",
    "**Why Combine?**\n",
    "- ‚úÖ **Best Accuracy**: Qualcomm 5G compliance 92% (vs 78% RAG-only, 85% fine-tune-only)\n",
    "- ‚úÖ **Domain + Current**: Fine-tuned LLM understands jargon + RAG provides latest specs\n",
    "- ‚úÖ **Cost-Effective**: Fine-tune small model ($10K) + RAG vs fine-tune large model ($100K)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**1. Qualcomm 5G RF Compliance ($15M)**\n",
    "- **Challenge**: FCC/3GPP regulations change monthly, generic LLM doesn't understand RF jargon\n",
    "- **Solution**: Fine-tune Llama 7B on RF domain ($10K) + RAG for latest regulations\n",
    "- **Results**: 92% accuracy vs 78% RAG-only, $15M compliance cost avoidance\n",
    "\n",
    "**2. Intel Test Validation Assistant ($12M)**\n",
    "- **Challenge**: Complex test terminology (parametric, functional, burn-in) + procedures updated weekly\n",
    "- **Solution**: Fine-tune GPT-3.5 on test domain ($8K) + RAG for latest procedures\n",
    "- **Results**: 94% accuracy, engineers trust system, $12M productivity gains\n",
    "\n",
    "**3. Legal Contract Analysis ($8M)**\n",
    "- **Challenge**: Legal language is specialized + contracts have latest clauses\n",
    "- **Solution**: Fine-tune on legal corpus ($15K) + RAG for specific contract types\n",
    "- **Results**: 91% clause extraction accuracy, 5√ó faster review, $8M savings\n",
    "\n",
    "**4. Medical Diagnosis Support ($10M)**\n",
    "- **Challenge**: Medical terminology + latest treatment guidelines\n",
    "- **Solution**: Fine-tune BioBERT ($12K) + RAG for latest papers/guidelines\n",
    "- **Results**: 87% diagnosis accuracy, reduce misdiagnosis 18%, $10M value\n",
    "\n",
    "## üîÑ RAG + Fine-Tuning Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Base LLM] --> B[Fine-Tune on Domain]\n",
    "    B --> C[Domain-Expert LLM]\n",
    "    \n",
    "    D[Document Corpus] --> E[Chunk + Embed]\n",
    "    E --> F[Vector DB]\n",
    "    \n",
    "    G[User Query] --> H[Fine-Tuned LLM Understanding]\n",
    "    H --> I[Retrieval Query]\n",
    "    I --> F\n",
    "    F --> J[Top-K Docs]\n",
    "    \n",
    "    J --> C\n",
    "    G --> C\n",
    "    C --> K[Domain-Aware Answer + Citations]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#fff5e1\n",
    "    style K fill:#e1ffe1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: LoRA and QLoRA (Parameter-Efficient Fine-Tuning)\n",
    "\n",
    "### üéØ Why Parameter-Efficient Fine-Tuning?\n",
    "\n",
    "**Problem with Full Fine-Tuning:**\n",
    "- Fine-tune ALL parameters (GPT-3: 175B parameters)\n",
    "- Requires massive GPU memory (8√ó A100 GPUs)\n",
    "- Expensive ($100K-$500K for large models)\n",
    "- Storage: Need to store full model copy for each domain\n",
    "\n",
    "**Solution: LoRA (Low-Rank Adaptation)**\n",
    "- Fine-tune ONLY small adapter matrices (0.1% of parameters)\n",
    "- Single GPU sufficient (NVIDIA A100)\n",
    "- Affordable ($5K-$10K)\n",
    "- Storage: Base model + small adapters (100MB vs 350GB)\n",
    "\n",
    "**LoRA Math:**\n",
    "- Original weight: W (4096 √ó 4096 matrix, 16M parameters)\n",
    "- LoRA decomposition: W + ŒîW = W + AB\n",
    "  - A: 4096 √ó 8 (rank-8 adapter)\n",
    "  - B: 8 √ó 4096\n",
    "  - Total: 65K parameters (0.4% of original)\n",
    "\n",
    "**QLoRA (Quantized LoRA):**\n",
    "- Further optimization: Quantize base model to 4-bit\n",
    "- Enables fine-tuning 65B models on single GPU\n",
    "- Memory: 48GB GPU (vs 320GB for full fine-tuning)\n",
    "\n",
    "### Qualcomm 5G RF Compliance Example\n",
    "\n",
    "**Challenge:**\n",
    "- FCC/3GPP regulations (complex RF terminology: EIRP, SAR, spurious emissions)\n",
    "- Regulations change monthly (need RAG for latest)\n",
    "- Generic LLM doesn't understand RF jargon\n",
    "\n",
    "**Solution: Fine-Tune + RAG**\n",
    "1. **Fine-Tune Llama 7B with LoRA**:\n",
    "   - Training data: 10K RF compliance Q&A pairs\n",
    "   - LoRA rank: 16 (best accuracy/efficiency tradeoff)\n",
    "   - Training: 4 hours on single A100, cost $500\n",
    "   - Parameters tuned: 8.4M (vs 7B full fine-tuning)\n",
    "\n",
    "2. **RAG for Latest Regulations**:\n",
    "   - Vector DB: 10K regulatory documents\n",
    "   - Updated weekly (new FCC rulings, 3GPP releases)\n",
    "   - Retrieval: Top-5 relevant regulation sections\n",
    "\n",
    "**Results:**\n",
    "- Accuracy: 92% (vs 78% RAG-only, 85% fine-tune-only)\n",
    "- Understands \"EIRP exceeds FCC Part 15 limits\" (fine-tuning)\n",
    "- Retrieves latest FCC rulings from 2024 (RAG)\n",
    "- $15M compliance cost avoidance\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Real-World Projects & Impact\n",
    "\n",
    "### üè≠ Post-Silicon Validation Projects\n",
    "\n",
    "**1. Qualcomm 5G RF Compliance Assistant ($15M Annual Savings)**\n",
    "- **Objective**: Instant regulatory answers with latest FCC/3GPP compliance\n",
    "- **Data**: 10K RF Q&A pairs + 10K regulatory docs (updated weekly)\n",
    "- **Architecture**: LoRA-tuned Llama 7B (rank-16) + Pinecone + RAG\n",
    "- **Fine-Tuning**: 10K pairs, 4 hours on A100, $500 cost\n",
    "- **Features**: RF jargon understanding, latest regulation retrieval, citation tracking\n",
    "- **Metrics**: 92% accuracy, zero violations, <2s latency\n",
    "- **Tech Stack**: Llama 7B + LoRA, Pinecone, FastAPI, Kubernetes\n",
    "- **Impact**: $15M fines avoided, instant compliance answers\n",
    "\n",
    "**2. Intel Test Validation Assistant ($12M Annual Savings)**\n",
    "- **Objective**: Understand test terminology + latest procedures\n",
    "- **Data**: 5K test Q&A pairs + 10K test procedures (updated weekly)\n",
    "- **Architecture**: QLoRA-tuned GPT-3.5 (4-bit) + ChromaDB + RAG\n",
    "- **Fine-Tuning**: 5K pairs, 3 hours on A100, $400 cost\n",
    "- **Features**: Test jargon (parametric, functional, burn-in), procedure retrieval\n",
    "- **Metrics**: 94% accuracy, engineers trust system, 2.1s latency\n",
    "- **Tech Stack**: GPT-3.5 + QLoRA, ChromaDB, FastAPI\n",
    "- **Impact**: $12M productivity gains (faster test development)\n",
    "\n",
    "**3. AMD Design Review Assistant ($10M Annual Savings)**\n",
    "- **Objective**: GPU architecture knowledge + latest design patterns\n",
    "- **Data**: 8K design Q&A pairs + 5K design docs (updated monthly)\n",
    "- **Architecture**: LoRA-tuned Llama 13B (rank-32) + Weaviate + RAG\n",
    "- **Fine-Tuning**: 8K pairs, 6 hours on 2√óA100, $800 cost\n",
    "- **Features**: GPU terminology (clock gating, power islands), design pattern retrieval\n",
    "- **Metrics**: 90% accuracy, onboard engineers 3√ó faster\n",
    "- **Tech Stack**: Llama 13B + LoRA, Weaviate, FastAPI\n",
    "- **Impact**: $10M productivity gains (faster onboarding)\n",
    "\n",
    "**4. NVIDIA Driver Validation Assistant ($8M Annual Savings)**\n",
    "- **Objective**: Driver API knowledge + latest bug patterns\n",
    "- **Data**: 15K driver Q&A pairs + 20K bug reports (updated daily)\n",
    "- **Architecture**: LoRA-tuned CodeLlama 7B (rank-16) + Elasticsearch + RAG\n",
    "- **Fine-Tuning**: 15K pairs, 5 hours on A100, $600 cost\n",
    "- **Features**: Driver API understanding, bug pattern matching, code examples\n",
    "- **Metrics**: 88% bug prediction accuracy, 40% faster debug\n",
    "- **Tech Stack**: CodeLlama 7B + LoRA, Elasticsearch, FastAPI\n",
    "- **Impact**: $8M savings (faster driver releases, fewer bugs)\n",
    "\n",
    "### üåê General AI/ML Projects\n",
    "\n",
    "**5. Legal Contract Analysis ($8M Cost Reduction)**\n",
    "- **Objective**: Legal language expertise + latest contract clauses\n",
    "- **Data**: 20K legal Q&A pairs + 100K contracts\n",
    "- **Architecture**: LoRA-tuned Legal-BERT + Weaviate + RAG\n",
    "- **Fine-Tuning**: 20K pairs, $1K cost\n",
    "- **Impact**: 91% clause extraction, 5√ó faster review, $8M savings\n",
    "\n",
    "**6. Medical Diagnosis Support ($10M Value)**\n",
    "- **Objective**: Medical terminology + latest treatment guidelines\n",
    "- **Data**: 30K medical Q&A pairs + 1M PubMed papers\n",
    "- **Architecture**: QLoRA-tuned BioBERT (4-bit) + Milvus + RAG\n",
    "- **Fine-Tuning**: 30K pairs, $1.5K cost\n",
    "- **Impact**: 87% diagnosis accuracy, reduce misdiagnosis 18%, $10M value\n",
    "\n",
    "**7. Financial Compliance RAG ($7M Risk Mitigation)**\n",
    "- **Objective**: Financial jargon + latest SEC/FINRA regulations\n",
    "- **Data**: 10K finance Q&A pairs + 50K regulatory docs\n",
    "- **Architecture**: LoRA-tuned FinBERT + Pinecone + RAG\n",
    "- **Fine-Tuning**: 10K pairs, $800 cost\n",
    "- **Impact**: 94% accuracy, zero violations, $7M fines avoided\n",
    "\n",
    "**8. Customer Support Assistant ($12M Cost Reduction)**\n",
    "- **Objective**: Product knowledge + latest FAQs/tickets\n",
    "- **Data**: 50K support Q&A pairs + 10M historical tickets\n",
    "- **Architecture**: QLoRA-tuned GPT-3.5 (4-bit) + Elasticsearch + RAG\n",
    "- **Fine-Tuning**: 50K pairs, $2K cost\n",
    "- **Impact**: 75% ticket automation, 90% satisfaction, $12M savings\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "**Combined Approach (RAG + Fine-Tuning):**\n",
    "- **Best Accuracy**: Qualcomm 92%, Intel 94%, AMD 90% (vs 78-85% single approach)\n",
    "- **Domain + Current**: Fine-tuned LLM + RAG for latest information\n",
    "- **Cost-Effective**: LoRA/QLoRA fine-tuning $500-$2K (vs $100K full fine-tuning)\n",
    "- **Business Impact: $82M total** (Qualcomm $15M, Intel $12M, AMD $10M, NVIDIA $8M, Legal $8M, Medical $10M, Finance $7M, Support $12M)\n",
    "\n",
    "**When to Use:**\n",
    "- ‚úÖ RAG + Fine-Tune: Production systems, domain expertise + up-to-date info\n",
    "- ‚úÖ RAG Only: General Q&A, lower budget, frequent document updates\n",
    "- ‚úÖ Fine-Tune Only: Offline domain expertise, no citation needs\n",
    "\n",
    "**Key Technologies:**\n",
    "- LoRA: Parameter-efficient fine-tuning (0.1-1% of parameters)\n",
    "- QLoRA: 4-bit quantization + LoRA (65B models on single GPU)\n",
    "- Adapter storage: 100MB vs 350GB full model\n",
    "\n",
    "**Next Steps:**\n",
    "- 087: AI Security & Safety (prompt injection, guardrails)\n",
    "- 088: Code Generation AI (test generation, refactoring)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've mastered combining RAG with fine-tuning for optimal accuracy and cost-effectiveness! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
