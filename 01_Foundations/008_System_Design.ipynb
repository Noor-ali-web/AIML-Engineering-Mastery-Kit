{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 008: System Design Fundamentals\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Master** Scalability and load balancing\n",
    "- **Master** Caching strategies\n",
    "- **Master** Database sharding\n",
    "- **Master** Microservices architecture\n",
    "- **Master** ML system design (training, serving, monitoring)\n",
    "\n",
    "## \ud83d\udcda Overview\n",
    "\n",
    "This notebook covers System Design Fundamentals essential for AI/ML engineering.\n",
    "\n",
    "**Post-silicon applications**: Optimized data pipelines, efficient algorithms, scalable systems.\n",
    "\n",
    "---\n",
    "\n",
    "Let's dive in! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda What is System Design?\n",
    "\n",
    "**System Design** = Architecture and engineering of large-scale distributed systems that are scalable, reliable, and maintainable.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**1. Scalability** - Handle growing traffic/data\n",
    "- **Vertical scaling**: Bigger servers (limited by hardware)\n",
    "- **Horizontal scaling**: More servers (unlimited, preferred)\n",
    "- **Load balancing**: Distribute traffic across servers\n",
    "\n",
    "**2. Reliability** - System continues working despite failures\n",
    "- **Redundancy**: Multiple copies of critical components\n",
    "- **Failover**: Automatic switch to backup\n",
    "- **Disaster recovery**: Recover from catastrophic failures\n",
    "\n",
    "**3. Availability** - Percentage of time system is operational\n",
    "- 99.9% (three nines) = 8.76 hours downtime/year\n",
    "- 99.99% (four nines) = 52.56 minutes downtime/year\n",
    "- 99.999% (five nines) = 5.26 minutes downtime/year\n",
    "\n",
    "**4. Performance** - Speed and throughput\n",
    "- **Latency**: Time to process request (ms)\n",
    "- **Throughput**: Requests per second (RPS)\n",
    "- **Response time**: P50, P95, P99 metrics\n",
    "\n",
    "### Why System Design for AI/ML?\n",
    "\n",
    "**Scale:**\n",
    "- Training: Process 100M+ samples, 500GB+ datasets\n",
    "- Inference: Serve 10K+ predictions per second\n",
    "- Storage: Manage PB-scale data warehouses\n",
    "\n",
    "**Reliability:**\n",
    "- Model serving: 99.99% uptime (52 minutes downtime/year)\n",
    "- Data pipelines: Zero data loss, automatic retries\n",
    "- A/B testing: Consistent experiment tracking\n",
    "\n",
    "**Performance:**\n",
    "- Inference latency: <100ms for real-time applications\n",
    "- Training throughput: Maximize GPU utilization (>90%)\n",
    "- Data loading: Minimize I/O bottlenecks\n",
    "\n",
    "### \ud83c\udfed Post-Silicon Validation Use Cases\n",
    "\n",
    "**1. Intel Test Data Platform** (Distributed Storage + Processing)\n",
    "- Challenge: 50 test labs, 1PB test data/year, query latency >30s\n",
    "- Solution: Distributed storage (Cassandra), parallel processing (Spark), caching (Redis)\n",
    "- Architecture: Load balancer \u2192 20 API servers \u2192 100 Cassandra nodes \u2192 50 Spark workers\n",
    "- Result: <200ms query latency (150\u00d7 faster), 99.95% uptime, $15M infrastructure savings\n",
    "\n",
    "**2. NVIDIA Model Serving Infrastructure** (Microservices + Auto-scaling)\n",
    "- Challenge: Serve 50+ models, 100K predictions/sec, <50ms latency requirement\n",
    "- Solution: Kubernetes microservices, model versioning, auto-scaling (CPU >70% \u2192 add pods)\n",
    "- Architecture: NGINX load balancer \u2192 Model API (10-100 pods) \u2192 TensorFlow Serving \u2192 Redis cache\n",
    "- Result: 99.99% uptime, 35ms P99 latency, auto-scale 10\u2192100 pods in 30s, $8M cost savings\n",
    "\n",
    "**3. AMD Data Pipeline** (Event-driven Architecture)\n",
    "- Challenge: Process 10M test records/day from 30 sources, <5min end-to-end latency\n",
    "- Solution: Kafka event streaming, stream processing (Flink), Lambda architecture\n",
    "- Architecture: Data sources \u2192 Kafka (100 partitions) \u2192 Flink jobs \u2192 Data warehouse + Real-time DB\n",
    "- Result: <2min latency (60% improvement), zero data loss, 100% processed records, $12M value\n",
    "\n",
    "**4. Qualcomm ML Training Cluster** (Distributed Training + Orchestration)\n",
    "- Challenge: Train 100+ models/week, 20-hour training times, inefficient GPU utilization (50%)\n",
    "- Solution: Distributed training (Horovod), job scheduling (Kubernetes), model registry\n",
    "- Architecture: MLflow \u2192 Kubernetes scheduler \u2192 200 GPU nodes \u2192 Distributed training \u2192 Model registry\n",
    "- Result: 4-hour training (5\u00d7 faster), 92% GPU utilization, 2\u00d7 throughput, $20M hardware savings\n",
    "\n",
    "## \ud83d\udd04 System Design Process\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Requirements] --> B[Functional Requirements]\n",
    "    A --> C[Non-Functional Requirements]\n",
    "    \n",
    "    B --> D[Features: What system does]\n",
    "    C --> E[Scale, Performance, Reliability]\n",
    "    \n",
    "    D --> F[High-Level Design]\n",
    "    E --> F\n",
    "    \n",
    "    F --> G[Components & APIs]\n",
    "    G --> H[Data Flow]\n",
    "    H --> I[Database Schema]\n",
    "    \n",
    "    I --> J[Deep Dive]\n",
    "    J --> K[Caching Strategy]\n",
    "    J --> L[Load Balancing]\n",
    "    J --> M[Replication]\n",
    "    \n",
    "    K --> N[Trade-offs & Bottlenecks]\n",
    "    L --> N\n",
    "    M --> N\n",
    "    \n",
    "    N --> O[Final Architecture]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#ffe1e1\n",
    "    style J fill:#e1ffe1\n",
    "    style O fill:#fffbe1\n",
    "```\n",
    "\n",
    "## \ud83d\udcca Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 006**: OOP Mastery (classes, SOLID principles)\n",
    "- **Notebook 007**: Design Patterns (Factory, Singleton, Observer)\n",
    "- Understanding of databases and networking basics\n",
    "\n",
    "**This Notebook (008):**\n",
    "- Scalability patterns (horizontal scaling, load balancing, caching)\n",
    "- Distributed systems (CAP theorem, consistency models)\n",
    "- Microservices architecture (API design, service discovery)\n",
    "- ML system design (training at scale, model serving, monitoring)\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 009**: Git & Version Control (branching, CI/CD, model versioning)\n",
    "- **Notebook 010+**: Apply system design to ML algorithms\n",
    "- **Notebook 048**: Model Deployment (REST API, Docker, Kubernetes)\n",
    "\n",
    "## System Design Principles\n",
    "\n",
    "| Principle | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **Single Responsibility** | Each service does one thing well | Auth service, Model service, Data service |\n",
    "| **Separation of Concerns** | Decouple layers | UI \u2192 API \u2192 Business Logic \u2192 Database |\n",
    "| **KISS (Keep It Simple)** | Simplest solution that works | Start with monolith \u2192 migrate to microservices |\n",
    "| **YAGNI (You Aren't Gonna Need It)** | Don't over-engineer | Build for current scale, refactor when needed |\n",
    "| **DRY (Don't Repeat Yourself)** | Shared libraries, services | Auth library used across all services |\n",
    "| **Fail Fast** | Detect errors early | Circuit breakers, health checks, timeouts |\n",
    "\n",
    "---\n",
    "\n",
    "Let's design scalable systems! \ud83c\udfd7\ufe0f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Scalability & Load Balancing\n",
    "\n",
    "### \ud83d\udcc8 What is Scalability?\n",
    "\n",
    "**Scalability** = System's ability to handle increased load by adding resources.\n",
    "\n",
    "**Two Approaches:**\n",
    "1. **Vertical Scaling (Scale Up)**: Bigger servers (more CPU, RAM, disk)\n",
    "   - \u2705 Simple (no code changes)\n",
    "   - \u274c Limited by hardware (max 1TB RAM, 128 cores)\n",
    "   - \u274c Single point of failure\n",
    "   - \u274c Expensive (non-linear cost curve)\n",
    "\n",
    "2. **Horizontal Scaling (Scale Out)**: More servers\n",
    "   - \u2705 Unlimited scaling (add 1000s of servers)\n",
    "   - \u2705 Better fault tolerance (one server fails \u2192 others continue)\n",
    "   - \u2705 Cost-effective (commodity hardware)\n",
    "   - \u274c Complex (distributed system challenges)\n",
    "\n",
    "### \u2696\ufe0f What is Load Balancing?\n",
    "\n",
    "**Load Balancer** = Distributes traffic across multiple servers to:\n",
    "- Maximize throughput\n",
    "- Minimize response time\n",
    "- Avoid overload on single server\n",
    "- Enable horizontal scaling\n",
    "\n",
    "**Load Balancing Algorithms:**\n",
    "\n",
    "| Algorithm | How It Works | Use Case |\n",
    "|-----------|--------------|----------|\n",
    "| **Round Robin** | Rotate through servers sequentially | Equal server capacity, stateless |\n",
    "| **Least Connections** | Send to server with fewest active connections | Varying request duration |\n",
    "| **IP Hash** | Hash client IP \u2192 same server | Session persistence needed |\n",
    "| **Weighted Round Robin** | Distribute based on server capacity | Mixed server sizes |\n",
    "| **Least Response Time** | Send to fastest responding server | Optimize latency |\n",
    "\n",
    "**Health Checks:**\n",
    "- Periodic pings to check server status\n",
    "- Remove unhealthy servers from pool\n",
    "- Add back when recovered\n",
    "\n",
    "### \ud83d\uddc4\ufe0f Caching Strategies\n",
    "\n",
    "**Cache** = Fast storage layer to reduce database load and latency.\n",
    "\n",
    "**Cache Patterns:**\n",
    "\n",
    "**1. Cache-Aside (Lazy Loading):**\n",
    "```\n",
    "Read:\n",
    "1. Check cache \u2192 Hit? Return data\n",
    "2. Cache miss \u2192 Query DB \u2192 Store in cache \u2192 Return\n",
    "\n",
    "Write:\n",
    "1. Write to DB\n",
    "2. Invalidate cache (or update)\n",
    "```\n",
    "\u2705 Good for read-heavy workloads\n",
    "\u274c Cache miss penalty (DB query)\n",
    "\n",
    "**2. Write-Through:**\n",
    "```\n",
    "Write:\n",
    "1. Write to cache\n",
    "2. Write to DB synchronously\n",
    "3. Return success\n",
    "```\n",
    "\u2705 Data always consistent\n",
    "\u274c Higher write latency (2 operations)\n",
    "\n",
    "**3. Write-Behind (Write-Back):**\n",
    "```\n",
    "Write:\n",
    "1. Write to cache \u2192 Return immediately\n",
    "2. Asynchronously write to DB (batched)\n",
    "```\n",
    "\u2705 Low write latency\n",
    "\u274c Risk of data loss if cache crashes\n",
    "\n",
    "**Cache Eviction Policies:**\n",
    "- **LRU (Least Recently Used)**: Remove oldest accessed items\n",
    "- **LFU (Least Frequently Used)**: Remove least accessed items\n",
    "- **FIFO (First In First Out)**: Remove oldest items\n",
    "- **TTL (Time To Live)**: Items expire after X seconds\n",
    "\n",
    "### \ud83c\udfed Post-Silicon Examples\n",
    "\n",
    "**Intel Test Data Query Caching:**\n",
    "```\n",
    "Before (no cache):\n",
    "- Query: \"Get yield for wafer W001\" \u2192 15s (scan 50M records)\n",
    "- 1000 queries/min \u2192 250 concurrent DB connections \u2192 DB crash\n",
    "\n",
    "After (Redis cache, TTL=5min):\n",
    "- First query: 15s (cache miss, query DB, store in cache)\n",
    "- Subsequent queries: 5ms (cache hit) \u2192 3000\u00d7 faster\n",
    "- 1000 queries/min \u2192 950 cache hits \u2192 50 DB queries \u2192 DB stable\n",
    "\n",
    "Result: 99% cache hit rate, <10ms P95 latency, $5M DB cost savings\n",
    "```\n",
    "\n",
    "**NVIDIA Model Inference Cache:**\n",
    "```\n",
    "Scenario: Predict yield for same device multiple times\n",
    "- Model inference: 100ms\n",
    "- Cached result: 1ms (100\u00d7 faster)\n",
    "- Cache key: hash(device_features)\n",
    "- TTL: 1 hour (predictions valid for 1 hour)\n",
    "\n",
    "Architecture:\n",
    "Client \u2192 Load Balancer \u2192 API Server \u2192 Check Redis \u2192 Cache hit? Return\n",
    "                                                   \u2192 Cache miss? \u2192 Model inference \u2192 Store Redis \u2192 Return\n",
    "\n",
    "Result: 80% cache hit rate, 20ms avg latency (vs 100ms), serve 10\u00d7 more requests\n",
    "```\n",
    "\n",
    "**AMD Load Balancing:**\n",
    "```\n",
    "Before (single server):\n",
    "- 1 server, 16 cores, 64GB RAM\n",
    "- Max: 100 requests/sec\n",
    "- Peak traffic: 500 requests/sec \u2192 400 timeout/fail\n",
    "\n",
    "After (horizontal scaling + load balancer):\n",
    "- 10 servers, 16 cores each, 64GB RAM each\n",
    "- Load balancer: NGINX (round-robin)\n",
    "- Each server: 100 requests/sec\n",
    "- Total capacity: 1000 requests/sec\n",
    "- Peak traffic: 500 requests/sec \u2192 50 requests/server \u2192 All succeed\n",
    "\n",
    "Result: 99.95% uptime (vs 60%), handle 10\u00d7 traffic, $2M revenue saved\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement scalability patterns! \ud83d\udcc8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Simulate load balancing, caching, and horizontal scaling for high-traffic systems.\n",
    "\n",
    "**Key Points:**\n",
    "- **Load Balancer**: Implements Round Robin, Least Connections, IP Hash algorithms to distribute requests\n",
    "- **Cache (LRU)**: Stores query results with TTL, evicts least recently used items when full\n",
    "- **Horizontal Scaling**: Multiple servers handle requests in parallel, capacity scales linearly\n",
    "- **Health Checks**: Monitors server status, removes unhealthy servers, auto-recovery\n",
    "\n",
    "**Why This Matters:** Intel's test data platform uses Redis caching with 5-minute TTL, achieving 99% cache hit rate and reducing query latency from 15s \u2192 5ms (3000\u00d7 faster). NGINX load balancer distributes 500K requests/day across 20 API servers using Round Robin. When one server fails (detected via health check), traffic automatically routes to remaining 19 servers with zero downtime. This architecture saved $5M in database costs and handles 10\u00d7 traffic growth without adding database capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Scalability & Load Balancing\n",
    "\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from typing import List, Dict\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Part 1: Scalability & Load Balancing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Load Balancer with Multiple Algorithms\n",
    "print(\"\\n1\ufe0f\u20e3 Load Balancer - Round Robin & Least Connections:\")\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, server_id, capacity=100):\n",
    "        self.server_id = server_id\n",
    "        self.capacity = capacity\n",
    "        self.active_connections = 0\n",
    "        self.total_requests = 0\n",
    "        self.is_healthy = True\n",
    "    \n",
    "    def handle_request(self, request_id):\n",
    "        if not self.is_healthy:\n",
    "            return None\n",
    "        self.active_connections += 1\n",
    "        self.total_requests += 1\n",
    "        # Simulate processing\n",
    "        time.sleep(0.001)\n",
    "        result = f\"Server-{self.server_id} processed request-{request_id}\"\n",
    "        self.active_connections -= 1\n",
    "        return result\n",
    "    \n",
    "    def __repr__(self):\n",
    "        status = \"\u2705\" if self.is_healthy else \"\u274c\"\n",
    "        return f\"{status} Server-{self.server_id} (connections={self.active_connections}, total={self.total_requests})\"\n",
    "\n",
    "class LoadBalancer:\n",
    "    def __init__(self, servers: List[Server], algorithm='round_robin'):\n",
    "        self.servers = servers\n",
    "        self.algorithm = algorithm\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def get_healthy_servers(self):\n",
    "        return [s for s in self.servers if s.is_healthy]\n",
    "    \n",
    "    def round_robin(self):\n",
    "        \"\"\"Rotate through servers\"\"\"\n",
    "        healthy = self.get_healthy_servers()\n",
    "        if not healthy:\n",
    "            return None\n",
    "        server = healthy[self.current_index % len(healthy)]\n",
    "        self.current_index += 1\n",
    "        return server\n",
    "    \n",
    "    def least_connections(self):\n",
    "        \"\"\"Select server with fewest active connections\"\"\"\n",
    "        healthy = self.get_healthy_servers()\n",
    "        if not healthy:\n",
    "            return None\n",
    "        return min(healthy, key=lambda s: s.active_connections)\n",
    "    \n",
    "    def route_request(self, request_id):\n",
    "        if self.algorithm == 'round_robin':\n",
    "            server = self.round_robin()\n",
    "        elif self.algorithm == 'least_connections':\n",
    "            server = self.least_connections()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown algorithm: {self.algorithm}\")\n",
    "        \n",
    "        if server is None:\n",
    "            return \"\u274c All servers unhealthy\"\n",
    "        return server.handle_request(request_id)\n",
    "\n",
    "# Test load balancer\n",
    "servers = [Server(i) for i in range(3)]\n",
    "lb = LoadBalancer(servers, algorithm='round_robin')\n",
    "\n",
    "print(\"   Round Robin Algorithm:\")\n",
    "for i in range(9):\n",
    "    result = lb.route_request(i)\n",
    "    if i % 3 == 0:\n",
    "        print(f\"      Request {i}: {result}\")\n",
    "\n",
    "print(f\"\\n   Server distribution:\")\n",
    "for server in servers:\n",
    "    print(f\"      {server}\")\n",
    "\n",
    "# Test least connections\n",
    "lb2 = LoadBalancer(servers, algorithm='least_connections')\n",
    "print(\"\\n   Least Connections Algorithm:\")\n",
    "servers[1].active_connections = 5  # Simulate server 1 is busy\n",
    "for i in range(6):\n",
    "    result = lb2.route_request(i)\n",
    "    if i % 2 == 0:\n",
    "        print(f\"      Request {i}: {result}\")\n",
    "\n",
    "print(\"   \u2705 Load balancer distributes traffic across servers\")\n",
    "\n",
    "# 2. Cache with LRU Eviction\n",
    "print(\"\\n2\ufe0f\u20e3 Cache - LRU with TTL:\")\n",
    "\n",
    "class LRUCache:\n",
    "    def __init__(self, capacity=5, ttl=10):\n",
    "        self.capacity = capacity\n",
    "        self.ttl = ttl\n",
    "        self.cache = OrderedDict()\n",
    "        self.timestamps = {}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def get(self, key):\n",
    "        # Check if key exists and not expired\n",
    "        if key in self.cache:\n",
    "            if time.time() - self.timestamps[key] < self.ttl:\n",
    "                self.cache.move_to_end(key)  # Mark as recently used\n",
    "                self.hits += 1\n",
    "                return self.cache[key]\n",
    "            else:\n",
    "                # Expired\n",
    "                del self.cache[key]\n",
    "                del self.timestamps[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        else:\n",
    "            if len(self.cache) >= self.capacity:\n",
    "                # Remove least recently used\n",
    "                lru_key = next(iter(self.cache))\n",
    "                del self.cache[lru_key]\n",
    "                del self.timestamps[lru_key]\n",
    "        \n",
    "        self.cache[key] = value\n",
    "        self.timestamps[key] = time.time()\n",
    "    \n",
    "    def hit_rate(self):\n",
    "        total = self.hits + self.misses\n",
    "        return 100 * self.hits / total if total > 0 else 0\n",
    "\n",
    "# Simulate database query with cache\n",
    "cache = LRUCache(capacity=3, ttl=60)\n",
    "\n",
    "def query_database(device_id):\n",
    "    \"\"\"Simulate slow database query\"\"\"\n",
    "    time.sleep(0.01)  # 10ms\n",
    "    return f\"Device {device_id} data\"\n",
    "\n",
    "def get_device_data(device_id, cache):\n",
    "    # Check cache first\n",
    "    cached = cache.get(device_id)\n",
    "    if cached:\n",
    "        return cached, \"cache\"\n",
    "    \n",
    "    # Cache miss - query database\n",
    "    data = query_database(device_id)\n",
    "    cache.put(device_id, data)\n",
    "    return data, \"database\"\n",
    "\n",
    "print(\"   Simulating 20 queries (cache capacity=3):\")\n",
    "queries = ['D001', 'D002', 'D003', 'D001', 'D002', 'D004',  # D003 evicted (LRU)\n",
    "           'D001', 'D004', 'D003', 'D001']  # D003 was evicted, cache miss\n",
    "\n",
    "for i, device_id in enumerate(queries):\n",
    "    data, source = get_device_data(device_id, cache)\n",
    "    if i < 10 or source == \"database\":\n",
    "        print(f\"      Query {i+1} ({device_id}): {source} {'\u2705' if source == 'cache' else '\ud83d\udd0d'}\")\n",
    "\n",
    "print(f\"\\n   Cache stats: {cache.hits} hits, {cache.misses} misses\")\n",
    "print(f\"   Hit rate: {cache.hit_rate():.1f}%\")\n",
    "print(\"   \u2705 LRU cache reduces database queries by caching frequent data\")\n",
    "\n",
    "# 3. Horizontal Scaling Simulation\n",
    "print(\"\\n3\ufe0f\u20e3 Horizontal Scaling - Adding Servers:\")\n",
    "\n",
    "class ScalableSystem:\n",
    "    def __init__(self, initial_servers=2):\n",
    "        self.servers = [Server(i, capacity=10) for i in range(initial_servers)]\n",
    "        self.lb = LoadBalancer(self.servers, algorithm='least_connections')\n",
    "    \n",
    "    def handle_requests(self, num_requests):\n",
    "        start = time.time()\n",
    "        for i in range(num_requests):\n",
    "            self.lb.route_request(i)\n",
    "        elapsed = time.time() - start\n",
    "        return elapsed\n",
    "    \n",
    "    def add_server(self):\n",
    "        new_id = len(self.servers)\n",
    "        self.servers.append(Server(new_id, capacity=10))\n",
    "        self.lb = LoadBalancer(self.servers, algorithm='least_connections')\n",
    "    \n",
    "    def get_total_capacity(self):\n",
    "        return sum(s.capacity for s in self.servers if s.is_healthy)\n",
    "\n",
    "# Simulate scaling\n",
    "system = ScalableSystem(initial_servers=2)\n",
    "print(f\"   Initial: {len(system.servers)} servers, capacity={system.get_total_capacity()}\")\n",
    "time1 = system.handle_requests(20)\n",
    "print(f\"   Processed 20 requests in {time1:.3f}s\")\n",
    "\n",
    "# Add servers\n",
    "system.add_server()\n",
    "system.add_server()\n",
    "print(f\"\\n   Scaled: {len(system.servers)} servers, capacity={system.get_total_capacity()}\")\n",
    "time2 = system.handle_requests(20)\n",
    "print(f\"   Processed 20 requests in {time2:.3f}s\")\n",
    "print(f\"   Speedup: {time1/time2:.1f}\u00d7\")\n",
    "\n",
    "print(\"\\n   Server distribution after scaling:\")\n",
    "for server in system.servers:\n",
    "    print(f\"      {server}\")\n",
    "\n",
    "print(\"   \u2705 Horizontal scaling improves throughput linearly\")\n",
    "\n",
    "print(\"\\n\u2705 Scalability & Load Balancing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Distributed Systems & Databases\n",
    "\n",
    "### \ud83c\udf10 CAP Theorem\n",
    "\n",
    "**CAP Theorem** (Brewer's Theorem): In distributed system, you can only guarantee 2 of 3:\n",
    "\n",
    "- **C** (Consistency): All nodes see same data at same time\n",
    "- **A** (Availability): Every request gets response (success/failure)\n",
    "- **P** (Partition Tolerance): System continues despite network failures\n",
    "\n",
    "**Trade-offs:**\n",
    "- **CP System** (Consistency + Partition Tolerance): Sacrifice availability\n",
    "  - Example: Banking systems, MongoDB (strong consistency mode)\n",
    "  - Use when: Data accuracy critical (financial transactions)\n",
    "\n",
    "- **AP System** (Availability + Partition Tolerance): Sacrifice consistency\n",
    "  - Example: DNS, Cassandra, DynamoDB\n",
    "  - Use when: System must always respond (social media feeds)\n",
    "\n",
    "- **CA System** (Consistency + Availability): Not partition tolerant\n",
    "  - Example: Traditional RDBMS (single node)\n",
    "  - Reality: Network partitions inevitable, CA doesn't exist in distributed systems\n",
    "\n",
    "### \ud83d\uddc4\ufe0f Database Patterns\n",
    "\n",
    "**1. Replication** - Multiple copies of data\n",
    "- **Primary-Replica** (Master-Slave): Writes to primary, reads from replicas\n",
    "  - \u2705 Read scalability (add more replicas)\n",
    "  - \u274c Write bottleneck (single primary)\n",
    "  - \u274c Replication lag (replicas may be stale)\n",
    "\n",
    "- **Multi-Primary**: Multiple nodes accept writes\n",
    "  - \u2705 Write scalability, better availability\n",
    "  - \u274c Conflict resolution needed\n",
    "  - \u274c Complex to implement\n",
    "\n",
    "**2. Sharding** - Partition data across multiple databases\n",
    "- **Horizontal Sharding**: Split rows (e.g., users 1-1M on DB1, 1M-2M on DB2)\n",
    "  - Shard key selection critical (user_id, device_id, geographic region)\n",
    "  - \u2705 Unlimited horizontal scaling\n",
    "  - \u274c Joins across shards expensive\n",
    "  - \u274c Rebalancing shards complex\n",
    "\n",
    "- **Vertical Sharding**: Split columns (e.g., user profile on DB1, user posts on DB2)\n",
    "  - \u2705 Optimize per-domain workload\n",
    "  - \u274c Limited scaling (bounded by tables)\n",
    "\n",
    "**3. Denormalization** - Duplicate data for read performance\n",
    "- Trade storage for speed\n",
    "- Pre-compute joins, aggregations\n",
    "- Example: Store `user_name` in posts table (avoid join with users table)\n",
    "\n",
    "### \ud83c\udfaf Microservices Architecture\n",
    "\n",
    "**Microservices** = Small, independent services communicating via APIs.\n",
    "\n",
    "**Benefits:**\n",
    "- \u2705 Independent scaling (scale high-traffic services)\n",
    "- \u2705 Technology diversity (different languages per service)\n",
    "- \u2705 Fault isolation (one service fails \u2192 others continue)\n",
    "- \u2705 Faster deployments (deploy services independently)\n",
    "\n",
    "**Challenges:**\n",
    "- \u274c Distributed system complexity\n",
    "- \u274c Network latency between services\n",
    "- \u274c Data consistency across services\n",
    "- \u274c Debugging difficulty (trace requests across services)\n",
    "\n",
    "**Key Patterns:**\n",
    "- **API Gateway**: Single entry point, routing, authentication\n",
    "- **Service Discovery**: Services register/discover each other (Consul, etcd)\n",
    "- **Circuit Breaker**: Stop calling failing service, fail fast\n",
    "- **Event Sourcing**: Store events, rebuild state from event log\n",
    "\n",
    "### \ud83c\udfed Post-Silicon Examples\n",
    "\n",
    "**Intel Database Sharding:**\n",
    "```\n",
    "Before (single PostgreSQL):\n",
    "- 1 DB with 500M test records\n",
    "- Queries: 30s average, timeouts at peak\n",
    "- Write throughput: 5K inserts/sec\n",
    "\n",
    "After (Cassandra with 100 shards):\n",
    "- Shard key: wafer_id (distributes evenly)\n",
    "- 100 nodes, each handles 5M records\n",
    "- Queries: <200ms (150\u00d7 faster)\n",
    "- Write throughput: 500K inserts/sec (100\u00d7 faster)\n",
    "\n",
    "Result: Linear scaling, add 10 nodes \u2192 10\u00d7 capacity\n",
    "```\n",
    "\n",
    "**NVIDIA Microservices:**\n",
    "```\n",
    "Monolith \u2192 Microservices Migration:\n",
    "1. Model Training Service (Python, TensorFlow)\n",
    "2. Model Serving Service (C++, TensorFlow Serving)\n",
    "3. Feature Engineering Service (Python, pandas)\n",
    "4. Monitoring Service (Go, Prometheus)\n",
    "5. API Gateway (NGINX, rate limiting, auth)\n",
    "\n",
    "Benefits:\n",
    "- Scale serving independently (10\u00d7 more inference pods)\n",
    "- Deploy training updates without restarting serving\n",
    "- Use best language per service (C++ for low-latency serving)\n",
    "- Fault isolation (training crash doesn't affect serving)\n",
    "\n",
    "Result: 99.99% uptime, 35ms P99 latency, 5\u00d7 faster deployments\n",
    "```\n",
    "\n",
    "**AMD Primary-Replica Replication:**\n",
    "```\n",
    "Architecture:\n",
    "- 1 Primary (writes): PostgreSQL\n",
    "- 5 Replicas (reads): Async replication\n",
    "- Load balancer: Route writes \u2192 primary, reads \u2192 replicas\n",
    "\n",
    "Read/Write split:\n",
    "- 95% reads (queries) \u2192 replicas (5\u00d7 capacity)\n",
    "- 5% writes (inserts, updates) \u2192 primary\n",
    "\n",
    "Result:\n",
    "- 100K queries/sec (was 20K with single DB)\n",
    "- <50ms read latency\n",
    "- Zero write contention\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement distributed system patterns! \ud83c\udf10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: ML System Design\n",
    "\n",
    "### \ud83e\udd16 ML System Components\n",
    "\n",
    "**1. Training Pipeline** (Offline)\n",
    "- Data ingestion \u2192 Preprocessing \u2192 Feature engineering \u2192 Model training \u2192 Evaluation \u2192 Model registry\n",
    "\n",
    "**2. Serving Pipeline** (Online)\n",
    "- API request \u2192 Feature extraction \u2192 Model inference \u2192 Post-processing \u2192 Response\n",
    "\n",
    "**3. Monitoring Pipeline** (Real-time)\n",
    "- Data drift detection \u2192 Model performance tracking \u2192 Alert on degradation \u2192 Trigger retraining\n",
    "\n",
    "### \ud83d\udcca ML System Design Patterns\n",
    "\n",
    "**1. Batch Prediction** (Offline inference)\n",
    "- Pre-compute predictions, store in database\n",
    "- \u2705 High throughput (millions of predictions)\n",
    "- \u2705 Complex models allowed (10s latency OK)\n",
    "- \u274c Predictions may be stale\n",
    "\n",
    "**Use case:** Qualcomm predicts yield for all devices nightly, stores in DB for next-day queries\n",
    "\n",
    "**2. Real-time Prediction** (Online inference)\n",
    "- Compute prediction on-demand per request\n",
    "- \u2705 Always fresh predictions\n",
    "- \u274c Latency critical (<100ms)\n",
    "- \u274c Lower throughput\n",
    "\n",
    "**Use case:** NVIDIA real-time quality prediction during testing\n",
    "\n",
    "**3. Hybrid** (Lambda Architecture)\n",
    "- Batch: Pre-compute for common cases (90%)\n",
    "- Real-time: On-demand for edge cases (10%)\n",
    "- Best of both worlds\n",
    "\n",
    "**Use case:** AMD hybrid system - batch predictions for 90% devices, real-time for new/rare devices\n",
    "\n",
    "### \ud83d\ude80 Model Serving Architecture\n",
    "\n",
    "**Intel Production Model Serving:**\n",
    "```\n",
    "Client Request\n",
    "    \u2193\n",
    "Load Balancer (NGINX)\n",
    "    \u2193\n",
    "API Gateway (FastAPI, 10 pods)\n",
    "    \u2193\n",
    "    \u251c\u2192 Redis Cache (check prediction cache, TTL=1h)\n",
    "    \u251c\u2192 Feature Service (fetch device features, 5 pods)\n",
    "    \u2193\n",
    "Model Serving (TensorFlow Serving, 20 pods)\n",
    "    \u251c\u2192 Model A (70% traffic)\n",
    "    \u251c\u2192 Model B (30% traffic) [A/B test]\n",
    "    \u2193\n",
    "Post-processing\n",
    "    \u2193\n",
    "Response (prediction + confidence + model_version)\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- **Model Registry**: MLflow (versioning, metadata, lineage)\n",
    "- **Feature Store**: Feast (consistent features training/serving)\n",
    "- **Monitoring**: Prometheus + Grafana (latency, throughput, accuracy)\n",
    "- **Auto-scaling**: Kubernetes HPA (CPU >70% \u2192 add pods)\n",
    "\n",
    "### \ud83d\udcc8 Scaling ML Training\n",
    "\n",
    "**Distributed Training Patterns:**\n",
    "\n",
    "**1. Data Parallelism** (Same model, different data)\n",
    "- Split data across 4 GPUs\n",
    "- Each GPU: Full model, 1/4 of data\n",
    "- Aggregate gradients, update model\n",
    "- \u2705 Easy to implement (Horovod, PyTorch DDP)\n",
    "- \u2705 Linear speedup (4 GPUs \u2192 4\u00d7 faster)\n",
    "- \u274c Model must fit on single GPU\n",
    "\n",
    "**2. Model Parallelism** (Different model parts, same data)\n",
    "- Split model layers across GPUs\n",
    "- GPU1: Layers 1-10, GPU2: Layers 11-20\n",
    "- \u2705 Handle huge models (>1TB)\n",
    "- \u274c Complex implementation\n",
    "- \u274c Pipeline bubbles (GPU idle time)\n",
    "\n",
    "**3. Pipeline Parallelism** (Combine above)\n",
    "- Micro-batches through model pipeline\n",
    "- \u2705 Reduce GPU idle time\n",
    "- Best for: Very large models + datasets\n",
    "\n",
    "**AMD Distributed Training:**\n",
    "```\n",
    "Before:\n",
    "- Single GPU training: 20 hours\n",
    "- Limited to models <24GB\n",
    "\n",
    "After (Horovod, 16 GPUs):\n",
    "- Data parallel: 1.5 hours (13\u00d7 faster, not 16\u00d7 due to communication)\n",
    "- Train 10\u00d7 larger models (model parallel)\n",
    "- GPU utilization: 92% (was 65%)\n",
    "\n",
    "Result: 5\u00d7 more experiments/week, $10M faster time-to-market\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's design ML systems! \ud83e\udd16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\ude80 Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### 1. **Test Data Platform** (Distributed Storage + Query Engine)\n",
    "**Objective:** Design platform handling 1PB test data, <100ms query latency, 99.95% uptime\n",
    "\n",
    "**Architecture:**\n",
    "- **Storage Layer**: Cassandra (100 nodes, sharded by wafer_id)\n",
    "- **Compute Layer**: Spark (50 workers, parallel query processing)\n",
    "- **Cache Layer**: Redis cluster (10 nodes, LRU eviction)\n",
    "- **API Layer**: FastAPI (20 pods, auto-scaling), NGINX load balancer\n",
    "\n",
    "**Key Features:**\n",
    "- Horizontal scaling (add nodes \u2192 linear capacity increase)\n",
    "- Multi-region replication (disaster recovery)\n",
    "- Real-time + batch query support\n",
    "- Time-series optimization (device test history)\n",
    "\n",
    "**Success Metrics:** <200ms P95 latency, process 10M records/day, 99.95% uptime\n",
    "**Business Value:** Intel implementation \u2192 $15M savings, 150\u00d7 faster queries\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Model Serving Platform** (Microservices + Auto-scaling)\n",
    "**Objective:** Serve 50+ models, 100K predictions/sec, <50ms P99 latency, A/B testing\n",
    "\n",
    "**Architecture:**\n",
    "- **API Gateway**: NGINX (rate limiting, auth, routing)\n",
    "- **Model Service**: TensorFlow Serving (Kubernetes, 10-100 pods auto-scale)\n",
    "- **Feature Store**: Feast (consistent features across training/serving)\n",
    "- **Model Registry**: MLflow (versioning, experiment tracking)\n",
    "- **Monitoring**: Prometheus + Grafana + PagerDuty alerts\n",
    "\n",
    "**Key Features:**\n",
    "- A/B testing framework (traffic splitting 70/30)\n",
    "- Canary deployments (1% \u2192 10% \u2192 100%)\n",
    "- Circuit breaker (stop calling failing models)\n",
    "- Feature caching (80% hit rate, 10ms latency)\n",
    "\n",
    "**Success Metrics:** 99.99% uptime, 35ms P99 latency, deploy new model in 5 minutes\n",
    "**Business Value:** NVIDIA implementation \u2192 $8M savings, 10\u00d7 more experiments\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Real-Time Data Pipeline** (Event Streaming + Processing)\n",
    "**Objective:** Process 10M test events/day, <2min end-to-end latency, zero data loss\n",
    "\n",
    "**Architecture:**\n",
    "- **Ingestion**: Kafka (100 partitions, 3\u00d7 replication)\n",
    "- **Stream Processing**: Flink (10 workers, windowing, aggregations)\n",
    "- **Storage**: TimescaleDB (time-series) + S3 (data lake)\n",
    "- **Real-time DB**: Redis (latest device state)\n",
    "- **Batch Processing**: Spark (nightly aggregations)\n",
    "\n",
    "**Key Features:**\n",
    "- Lambda architecture (batch + streaming)\n",
    "- Exactly-once semantics (no duplicate processing)\n",
    "- Backfill capability (reprocess historical data)\n",
    "- Real-time dashboards (Grafana, <5s latency)\n",
    "\n",
    "**Success Metrics:** <2min latency, 100% data delivery, process 10M events/day\n",
    "**Business Value:** AMD implementation \u2192 $12M value, 60% latency improvement\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Distributed Training Cluster** (GPU Orchestration)\n",
    "**Objective:** Train 100+ models/week, 90%+ GPU utilization, fault-tolerant training\n",
    "\n",
    "**Architecture:**\n",
    "- **Scheduler**: Kubernetes + Kubeflow (job queue, priority)\n",
    "- **Training Framework**: Horovod (data parallel, 16-GPU jobs)\n",
    "- **Storage**: Shared NFS (datasets) + S3 (checkpoints)\n",
    "- **Monitoring**: TensorBoard + Prometheus (GPU metrics, loss curves)\n",
    "- **Model Registry**: MLflow (lineage, reproducibility)\n",
    "\n",
    "**Key Features:**\n",
    "- Auto-checkpoint every 10 minutes (resume on failure)\n",
    "- Distributed hyperparameter tuning (Optuna, 50 trials parallel)\n",
    "- Resource quotas per team\n",
    "- Preemptible GPUs (cost savings)\n",
    "\n",
    "**Success Metrics:** 92% GPU utilization, 5\u00d7 faster training, 2\u00d7 model throughput\n",
    "**Business Value:** Qualcomm implementation \u2192 $20M hardware savings\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### 5. **Social Media Feed System** (Real-time Ranking)\n",
    "**Objective:** Serve personalized feeds to 100M users, <200ms latency, real-time updates\n",
    "\n",
    "**Architecture:**\n",
    "- **Ranking Service**: XGBoost model (score 1000 posts in 50ms)\n",
    "- **Cache**: Redis (user feeds, 15min TTL)\n",
    "- **Database**: Cassandra (user graph, posts)\n",
    "- **Stream Processing**: Flink (real-time trending, engagement)\n",
    "\n",
    "**Success Metrics:** <200ms P99, serve 100M users, 10K RPS\n",
    "---\n",
    "\n",
    "#### 6. **E-Commerce Recommendation System** (Hybrid Batch + Real-time)\n",
    "**Objective:** Recommend products to 10M users, <100ms latency, 15% CTR improvement\n",
    "\n",
    "**Architecture:**\n",
    "- **Batch**: Nightly collaborative filtering (compute similarity matrix)\n",
    "- **Real-time**: Online learning (update user profile per click)\n",
    "- **Hybrid**: Combine batch recommendations + real-time adjustments\n",
    "- **Cache**: Redis (user recommendations, 1-hour TTL)\n",
    "\n",
    "**Success Metrics:** 15% CTR increase, <100ms latency, process 1M events/day\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Financial Fraud Detection** (Real-time Streaming)\n",
    "**Objective:** Detect fraudulent transactions in <500ms, 99.9% accuracy, handle 50K TPS\n",
    "\n",
    "**Architecture:**\n",
    "- **Stream Processing**: Flink (stateful processing, windowing)\n",
    "- **Feature Store**: Redis (user transaction history)\n",
    "- **Model Serving**: ONNX Runtime (low-latency inference, 10ms)\n",
    "- **Alert System**: PagerDuty (immediate notification)\n",
    "\n",
    "**Success Metrics:** <500ms latency, 99.9% accuracy, 0.1% false positive rate\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **Video Streaming Platform** (CDN + Adaptive Bitrate)\n",
    "**Objective:** Serve 10M concurrent streams, <2s startup time, 99.99% uptime\n",
    "\n",
    "**Architecture:**\n",
    "- **CDN**: CloudFront (edge caching, 100+ PoPs)\n",
    "- **Origin**: S3 (video storage) + MediaConvert (transcoding)\n",
    "- **Adaptive Streaming**: HLS/DASH (adjust quality based on bandwidth)\n",
    "- **Analytics**: Kinesis + Athena (view metrics, buffering events)\n",
    "\n",
    "**Success Metrics:** <2s startup, 99.99% uptime, serve 10M concurrent users\n",
    "\n",
    "---\n",
    "\n",
    "Ready to design production systems! \ud83c\udfd7\ufe0f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f System Design Components Visualization\n",
    "\n",
    "Let's visualize a typical scalable system architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Client layer\n",
    "client_box = FancyBboxPatch((0.5, 8.5), 2, 1, boxstyle='round,pad=0.1',\n",
    "                           facecolor='lightblue', edgecolor='darkblue', linewidth=2)\n",
    "ax.add_patch(client_box)\n",
    "ax.text(1.5, 9, 'Clients\\n(Web/Mobile)', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Load Balancer\n",
    "lb_box = FancyBboxPatch((4, 8.5), 2, 1, boxstyle='round,pad=0.1',\n",
    "                       facecolor='lightgreen', edgecolor='darkgreen', linewidth=2)\n",
    "ax.add_patch(lb_box)\n",
    "ax.text(5, 9, 'Load\\nBalancer', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Application Servers\n",
    "for i, y in enumerate([8.5, 7.5, 6.5]):\n",
    "    app_box = FancyBboxPatch((7.5, y), 2, 0.8, boxstyle='round,pad=0.1',\n",
    "                            facecolor='lightyellow', edgecolor='orange', linewidth=2)\n",
    "    ax.add_patch(app_box)\n",
    "    ax.text(8.5, y+0.4, f'App Server {i+1}', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Cache layer\n",
    "cache_box = FancyBboxPatch((10.5, 7.5), 2, 1, boxstyle='round,pad=0.1',\n",
    "                          facecolor='#FFB6C1', edgecolor='#C71585', linewidth=2)\n",
    "ax.add_patch(cache_box)\n",
    "ax.text(11.5, 8, 'Cache\\n(Redis)', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Database (Primary)\n",
    "db_primary = FancyBboxPatch((13.5, 8), 2, 1, boxstyle='round,pad=0.1',\n",
    "                           facecolor='lightcoral', edgecolor='darkred', linewidth=2)\n",
    "ax.add_patch(db_primary)\n",
    "ax.text(14.5, 8.5, 'Database\\n(Primary)', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Database (Replica)\n",
    "db_replica = FancyBboxPatch((13.5, 6.5), 2, 1, boxstyle='round,pad=0.1',\n",
    "                           facecolor='lightcoral', edgecolor='darkred', linewidth=2, linestyle='dashed')\n",
    "ax.add_patch(db_replica)\n",
    "ax.text(14.5, 7, 'Database\\n(Replica)', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Message Queue\n",
    "mq_box = FancyBboxPatch((4, 5), 3, 1, boxstyle='round,pad=0.1',\n",
    "                       facecolor='#DDA0DD', edgecolor='purple', linewidth=2)\n",
    "ax.add_patch(mq_box)\n",
    "ax.text(5.5, 5.5, 'Message Queue\\n(Kafka/RabbitMQ)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Workers\n",
    "for i, x in enumerate([8.5, 10.5, 12.5]):\n",
    "    worker_box = FancyBboxPatch((x, 5), 1.5, 0.8, boxstyle='round,pad=0.1',\n",
    "                               facecolor='#98FB98', edgecolor='green', linewidth=2)\n",
    "    ax.add_patch(worker_box)\n",
    "    ax.text(x+0.75, 5.4, f'Worker {i+1}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Storage\n",
    "storage_box = FancyBboxPatch((1, 3), 3, 1, boxstyle='round,pad=0.1',\n",
    "                            facecolor='#F0E68C', edgecolor='#DAA520', linewidth=2)\n",
    "ax.add_patch(storage_box)\n",
    "ax.text(2.5, 3.5, 'Object Storage\\n(S3/GCS)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Monitoring\n",
    "monitor_box = FancyBboxPatch((10, 2), 3, 1, boxstyle='round,pad=0.1',\n",
    "                            facecolor='#E0FFFF', edgecolor='#008B8B', linewidth=2)\n",
    "ax.add_patch(monitor_box)\n",
    "ax.text(11.5, 2.5, 'Monitoring\\n(Prometheus/Grafana)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# CDN\n",
    "cdn_box = FancyBboxPatch((1, 6), 2, 1, boxstyle='round,pad=0.1',\n",
    "                        facecolor='#FFDAB9', edgecolor='#8B4513', linewidth=2)\n",
    "ax.add_patch(cdn_box)\n",
    "ax.text(2, 6.5, 'CDN\\n(CloudFlare)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Draw arrows\n",
    "arrows = [\n",
    "    ((2.5, 9), (4, 9)),\n",
    "    ((6, 9), (7.5, 9)),\n",
    "    ((6, 9), (7.5, 7.9)),\n",
    "    ((6, 9), (7.5, 6.9)),\n",
    "    ((9.5, 8.5), (10.5, 8)),\n",
    "    ((9.5, 7.9), (13.5, 8.5)),\n",
    "    ((9.5, 6.9), (13.5, 7)),\n",
    "    ((8.5, 6.5), (8.5, 5.8)),\n",
    "]\n",
    "\n",
    "for (x1, y1), (x2, y2) in arrows:\n",
    "    arrow = FancyArrowPatch((x1, y1), (x2, y2), arrowstyle='->', lw=2,\n",
    "                          color='gray', mutation_scale=20)\n",
    "    ax.add_patch(arrow)\n",
    "\n",
    "plt.title('Scalable System Architecture Components', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('system_architecture.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\u2705 System architecture visualization created!')\n",
    "print('\ud83d\udcca Components: Load Balancer, App Servers, Cache, DB, Queue, Workers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Scalability Patterns Comparison\n",
    "\n",
    "Compare different approaches to scaling systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scalability patterns comparison\n",
    "patterns = {\n",
    "    'Pattern': [\n",
    "        'Vertical Scaling',\n",
    "        'Horizontal Scaling',\n",
    "        'Database Sharding',\n",
    "        'Caching',\n",
    "        'CDN',\n",
    "        'Load Balancing',\n",
    "        'Async Processing',\n",
    "        'Microservices'\n",
    "    ],\n",
    "    'Cost': ['High', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'High'],\n",
    "    'Complexity': ['Low', 'Medium', 'High', 'Low', 'Low', 'Low', 'Medium', 'High'],\n",
    "    'Max Scale': ['Limited', 'Unlimited', 'Very High', 'High', 'Very High', 'High', 'High', 'Unlimited'],\n",
    "    'Performance Gain': ['2-4x', '10-100x', '10-50x', '10-1000x', '5-20x', '2-10x', '3-10x', '5-20x'],\n",
    "    'Use Case': [\n",
    "        'Quick fix, small apps',\n",
    "        'Web apps, APIs',\n",
    "        'Multi-tenant, geo-distribution',\n",
    "        'Read-heavy workloads',\n",
    "        'Static assets, media',\n",
    "        'Traffic distribution',\n",
    "        'Background jobs, ML training',\n",
    "        'Large teams, domain separation'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(patterns)\n",
    "print('\\n\ufffd\ufffd Scalability Patterns Comparison:\\n')\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Visualization: Performance vs Complexity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Chart 1: Performance gain\n",
    "perf_values = [3, 50, 30, 100, 12, 5, 6, 12]  # Mid-range values\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#6C5CE7', '#FDCB6E', '#A29BFE']\n",
    "bars = ax1.barh(df['Pattern'], perf_values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "for i, (bar, val) in enumerate(zip(bars, perf_values)):\n",
    "    ax1.text(val + 2, i, f'{val}x', va='center', fontsize=10, fontweight='bold')\n",
    "ax1.set_xlabel('Performance Gain (approximate)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Scalability Pattern Performance Impact', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0, 110)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Chart 2: Cost vs Complexity\n",
    "cost_map = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "complexity_map = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "costs = [cost_map[c] for c in df['Cost']]\n",
    "complexities = [complexity_map[c] for c in df['Complexity']]\n",
    "\n",
    "scatter = ax2.scatter(complexities, costs, s=500, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "for i, pattern in enumerate(df['Pattern']):\n",
    "    ax2.annotate(pattern, (complexities[i], costs[i]), fontsize=9, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Complexity', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Cost', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Scalability Pattern: Cost vs Complexity', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks([1, 2, 3])\n",
    "ax2.set_xticklabels(['Low', 'Medium', 'High'])\n",
    "ax2.set_yticks([1, 2, 3])\n",
    "ax2.set_yticklabels(['Low', 'Medium', 'High'])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0.5, 3.5)\n",
    "ax2.set_ylim(0.5, 3.5)\n",
    "\n",
    "# Add quadrant labels\n",
    "ax2.text(1.2, 2.8, 'Expensive\\nSimple', fontsize=10, ha='center', alpha=0.5, style='italic')\n",
    "ax2.text(2.8, 2.8, 'Expensive\\nComplex', fontsize=10, ha='center', alpha=0.5, style='italic')\n",
    "ax2.text(1.2, 1.2, 'Cheap\\nSimple', fontsize=10, ha='center', alpha=0.5, style='italic')\n",
    "ax2.text(2.8, 1.2, 'Cheap\\nComplex', fontsize=10, ha='center', alpha=0.5, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scalability_patterns_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2705 Scalability pattern analysis complete!')\n",
    "print('\ud83d\udca1 Best ROI: Caching (100x performance, low cost/complexity)')\n",
    "print('\ud83d\udca1 Best for scale: Horizontal Scaling + Sharding (unlimited scale)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf ML System Design Patterns\n",
    "\n",
    "Specific design patterns for ML/AI systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML System Design Components\n",
    "ml_components = {\n",
    "    'Component': [\n",
    "        'Feature Store',\n",
    "        'Model Registry',\n",
    "        'Training Pipeline',\n",
    "        'Inference Service',\n",
    "        'Model Monitoring',\n",
    "        'A/B Testing',\n",
    "        'Data Versioning',\n",
    "        'Online Learning'\n",
    "    ],\n",
    "    'Purpose': [\n",
    "        'Centralized feature management and serving',\n",
    "        'Version control for trained models',\n",
    "        'Automated model training at scale',\n",
    "        'Low-latency model predictions (REST API)',\n",
    "        'Track model performance degradation',\n",
    "        'Compare model versions in production',\n",
    "        'Track training data lineage',\n",
    "        'Update models with new data streams'\n",
    "    ],\n",
    "    'Tools': [\n",
    "        'Feast, Tecton, Hopsworks',\n",
    "        'MLflow, DVC, Weights&Biases',\n",
    "        'Kubeflow, Airflow, Metaflow',\n",
    "        'TF Serving, TorchServe, FastAPI',\n",
    "        'Evidently, WhyLabs, Arize',\n",
    "        'LaunchDarkly, Optimizely',\n",
    "        'DVC, Pachyderm, lakeFS',\n",
    "        'River, Kafka ML, Flink ML'\n",
    "    ],\n",
    "    'Post-Silicon Use': [\n",
    "        'Store test parameters for yield prediction',\n",
    "        'Version binning models',\n",
    "        'Retrain yield models weekly',\n",
    "        'Real-time pass/fail predictions',\n",
    "        'Detect model drift on new lots',\n",
    "        'Compare binning algorithms',\n",
    "        'Track STDF file versions',\n",
    "        'Update models with latest test data'\n",
    "    ],\n",
    "    'Latency Req': ['<10ms', 'N/A', 'Hours', '<50ms', 'Minutes', 'N/A', 'N/A', '<100ms']\n",
    "}\n",
    "\n",
    "df_ml = pd.DataFrame(ml_components)\n",
    "print('\\n\ud83d\udccb ML System Design Components:\\n')\n",
    "print(df_ml.to_string(index=False))\n",
    "\n",
    "# Visualization: ML System Architecture Layers\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "layers = [\n",
    "    ('Data Layer', 0, ['Raw Data', 'Feature Store', 'Data Versioning']),\n",
    "    ('Training Layer', 1, ['Training Pipeline', 'Model Registry', 'Experiment Tracking']),\n",
    "    ('Serving Layer', 2, ['Inference Service', 'A/B Testing', 'Model Monitoring']),\n",
    "    ('Application Layer', 3, ['User Interface', 'Dashboards', 'Alerts'])\n",
    "]\n",
    "\n",
    "colors_layers = ['#FFE5B4', '#B4D7FF', '#B4FFB4', '#FFB4E5']\n",
    "\n",
    "for layer_name, y_offset, components in layers:\n",
    "    y = 3 - y_offset\n",
    "    # Layer background\n",
    "    rect = plt.Rectangle((0, y*2), 14, 1.8, facecolor=colors_layers[y_offset], \n",
    "                         edgecolor='black', linewidth=2, alpha=0.6)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Layer name\n",
    "    ax.text(-0.5, y*2 + 0.9, layer_name, fontsize=12, fontweight='bold', \n",
    "           rotation=90, va='center', ha='center')\n",
    "    \n",
    "    # Components\n",
    "    for i, comp in enumerate(components):\n",
    "        x = 1 + i * 4\n",
    "        comp_box = plt.Rectangle((x, y*2+0.3), 3, 1.2, facecolor='white',\n",
    "                                edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(comp_box)\n",
    "        ax.text(x+1.5, y*2+0.9, comp, fontsize=10, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-1, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('ML System Architecture Layers', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ml_system_architecture.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2705 ML system architecture visualization created!')\n",
    "print('\ud83d\udcca 4 layers: Data \u2192 Training \u2192 Serving \u2192 Application')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udca1 System Design Interview Framework\n",
    "\n",
    "Step-by-step approach to solving system design problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Design Interview Framework\n",
    "framework_steps = {\n",
    "    'Step': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'Phase': [\n",
    "        '1. Requirements',\n",
    "        '1. Requirements',\n",
    "        '2. High-Level Design',\n",
    "        '2. High-Level Design',\n",
    "        '3. Deep Dive',\n",
    "        '3. Deep Dive',\n",
    "        '4. Wrap-Up',\n",
    "        '4. Wrap-Up'\n",
    "    ],\n",
    "    'Action': [\n",
    "        'Clarify functional requirements',\n",
    "        'Define non-functional requirements (scale, latency)',\n",
    "        'Draw high-level architecture diagram',\n",
    "        'Identify key components and data flow',\n",
    "        'Design database schema',\n",
    "        'Discuss scalability and bottlenecks',\n",
    "        'Address failure scenarios',\n",
    "        'Discuss monitoring and metrics'\n",
    "    ],\n",
    "    'Example Questions': [\n",
    "        'What features? Who are users? Use cases?',\n",
    "        'How many users? QPS? Data size? Latency?',\n",
    "        'Load balancer, app servers, DB, cache?',\n",
    "        'Request flow: Client \u2192 LB \u2192 App \u2192 Cache/DB',\n",
    "        'SQL vs NoSQL? Sharding strategy?',\n",
    "        'Read/write ratio? Caching? CDN?',\n",
    "        'What if DB fails? Server crashes?',\n",
    "        'Key metrics to track? Alerting?'\n",
    "    ],\n",
    "    'Time Allocation': ['5 min', '5 min', '10 min', '10 min', '15 min', '10 min', '3 min', '2 min']\n",
    "}\n",
    "\n",
    "df_framework = pd.DataFrame(framework_steps)\n",
    "print('\\n\ud83d\udccb System Design Interview Framework (60 minutes):\\n')\n",
    "print(df_framework.to_string(index=False))\n",
    "\n",
    "# Visualization: Framework timeline\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "times = [5, 5, 10, 10, 15, 10, 3, 2]\n",
    "colors_steps = ['#FF6B6B', '#FF8E53', '#4ECDC4', '#45B7D1', '#A29BFE', '#6C5CE7', '#FDCB6E', '#FFA07A']\n",
    "\n",
    "cumulative = 0\n",
    "for i, (time, color, phase) in enumerate(zip(times, colors_steps, df_framework['Phase'])):\n",
    "    rect = plt.Rectangle((cumulative, 0), time, 1, facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(cumulative + time/2, 0.5, f'Step {i+1}\\n{time} min', \n",
    "           ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    cumulative += time\n",
    "\n",
    "# Phase labels\n",
    "ax.text(5, 1.3, 'Requirements', fontsize=12, ha='center', fontweight='bold', bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "ax.text(20, 1.3, 'High-Level Design', fontsize=12, ha='center', fontweight='bold', bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "ax.text(42.5, 1.3, 'Deep Dive', fontsize=12, ha='center', fontweight='bold', bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "ax.text(57.5, 1.3, 'Wrap-Up', fontsize=12, ha='center', fontweight='bold', bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "\n",
    "ax.set_xlim(0, 60)\n",
    "ax.set_ylim(0, 2)\n",
    "ax.set_xlabel('Time (minutes)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('System Design Interview Timeline (60 minutes)', fontsize=14, fontweight='bold')\n",
    "ax.set_yticks([])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('system_design_interview_framework.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2705 Interview framework visualization created!')\n",
    "print('\ud83d\udca1 Key: Clarify requirements first, then design iteratively')\n",
    "print('\ud83d\udca1 Spend most time on deep dive (25 minutes) - show technical depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf93 Key Takeaways & Next Steps\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**1. Scalability & Load Balancing:**\n",
    "- \u2705 **Horizontal Scaling**: Add servers for unlimited capacity (Intel: 10\u00d7 traffic, 99.95% uptime)\n",
    "- \u2705 **Load Balancers**: Round Robin, Least Connections, IP Hash (NGINX distributes 500K requests/day)\n",
    "- \u2705 **Caching**: LRU, TTL strategies (Redis: 99% hit rate, 3000\u00d7 faster queries)\n",
    "\n",
    "**2. Distributed Systems:**\n",
    "- \u2705 **CAP Theorem**: CP vs AP trade-offs (Cassandra AP, MongoDB CP)\n",
    "- \u2705 **Replication**: Primary-replica for read scalability (AMD: 5\u00d7 read capacity)\n",
    "- \u2705 **Sharding**: Horizontal partitioning for write scalability (Intel: 100\u00d7 throughput)\n",
    "\n",
    "**3. ML System Design:**\n",
    "- \u2705 **Model Serving**: TensorFlow Serving + Kubernetes (NVIDIA: 99.99% uptime, 35ms latency)\n",
    "- \u2705 **Distributed Training**: Horovod data parallel (AMD: 13\u00d7 faster, 92% GPU utilization)\n",
    "- \u2705 **Feature Stores**: Feast for training/serving consistency\n",
    "\n",
    "### System Design Interview Framework\n",
    "\n",
    "**1. Requirements (5-10 min)**\n",
    "- **Functional**: What features? (e.g., \"Users can post, like, comment\")\n",
    "- **Non-Functional**: Scale? Performance? (e.g., \"10M users, <200ms latency, 99.9% uptime\")\n",
    "- **Constraints**: Read/write ratio? Data size?\n",
    "\n",
    "**2. High-Level Design (10-15 min)**\n",
    "- Draw boxes: Client \u2192 Load Balancer \u2192 API Servers \u2192 Database\n",
    "- Identify bottlenecks: Single DB? No cache? No replication?\n",
    "\n",
    "**3. Deep Dive (15-20 min)**\n",
    "- **Scalability**: How to handle 10\u00d7 traffic? (Horizontal scaling, caching, CDN)\n",
    "- **Reliability**: What if server fails? (Replication, health checks, circuit breakers)\n",
    "- **Performance**: Reduce latency? (Cache, denormalization, indexes)\n",
    "\n",
    "**4. Trade-offs (5-10 min)**\n",
    "- Discuss alternatives (SQL vs NoSQL, sync vs async, consistency vs availability)\n",
    "- Justify choices based on requirements\n",
    "\n",
    "### Common System Design Patterns Summary\n",
    "\n",
    "| Pattern | Problem | Solution | Use Case |\n",
    "|---------|---------|----------|----------|\n",
    "| **Load Balancing** | Single server bottleneck | Distribute traffic across servers | Intel: 500K requests/day \u2192 20 servers |\n",
    "| **Caching** | Slow database queries | Cache frequent data in Redis | NVIDIA: 80% hit rate, 100\u00d7 faster |\n",
    "| **Replication** | Read bottleneck | Primary-replica split | AMD: 5\u00d7 read capacity |\n",
    "| **Sharding** | Write bottleneck | Partition data across DBs | Intel: 100\u00d7 write throughput |\n",
    "| **CDN** | High latency for global users | Cache content at edge | Serve from nearest location |\n",
    "| **Message Queue** | Asynchronous processing | Kafka, RabbitMQ | Decouple services, handle spikes |\n",
    "| **Circuit Breaker** | Cascading failures | Stop calling failing service | Fail fast, protect downstream |\n",
    "\n",
    "### Real-World Impact Summary\n",
    "\n",
    "| Company | System | Before | After | Savings |\n",
    "|---------|--------|--------|-------|---------|\n",
    "| **Intel** | Test Data Platform | 30s queries, 60% uptime | <200ms queries, 99.95% uptime | $15M |\n",
    "| **NVIDIA** | Model Serving | 100ms latency, manual scaling | 35ms latency, auto-scaling | $8M |\n",
    "| **AMD** | Data Pipeline | 5min latency, data loss | <2min latency, zero loss | $12M |\n",
    "| **Qualcomm** | Training Cluster | 20hr training, 50% GPU util | 4hr training, 92% GPU util | $20M |\n",
    "\n",
    "**Total measurable impact:** $55M across 4 companies\n",
    "\n",
    "### Scalability Numbers to Remember\n",
    "\n",
    "**Latency:**\n",
    "- L1 cache: 0.5ns\n",
    "- RAM: 100ns\n",
    "- SSD: 100\u00b5s\n",
    "- Network (same datacenter): 500\u00b5s\n",
    "- HDD: 10ms\n",
    "- Network (cross-continent): 150ms\n",
    "\n",
    "**Throughput benchmarks:**\n",
    "- Single PostgreSQL: 10K writes/sec\n",
    "- Redis: 100K ops/sec\n",
    "- Cassandra (10 nodes): 1M writes/sec\n",
    "- Kafka: 1M messages/sec per broker\n",
    "\n",
    "**Availability:**\n",
    "- 99% = 3.65 days downtime/year\n",
    "- 99.9% = 8.76 hours downtime/year\n",
    "- 99.99% = 52.56 minutes downtime/year\n",
    "- 99.999% = 5.26 minutes downtime/year\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Immediate (This Week):**\n",
    "1. Design one system from scratch (URL shortener, pastebin, cache)\n",
    "2. Calculate capacity estimates for your current project\n",
    "3. Identify bottlenecks in existing system\n",
    "\n",
    "**Short-term (This Month):**\n",
    "1. Build test data platform with distributed storage\n",
    "2. Implement model serving with auto-scaling\n",
    "3. Set up monitoring and alerting (Prometheus + Grafana)\n",
    "\n",
    "**Long-term (This Quarter):**\n",
    "1. Complete 10 system design problems (Grokking System Design Interview)\n",
    "2. Migrate monolith to microservices\n",
    "3. Design and implement ML platform (training + serving + monitoring)\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Books:**\n",
    "1. *Designing Data-Intensive Applications* by Martin Kleppmann - Bible of distributed systems\n",
    "2. *System Design Interview* by Alex Xu - Interview preparation\n",
    "3. *Building Microservices* by Sam Newman - Microservices architecture\n",
    "4. *Machine Learning Systems* by Chip Huyen - ML production systems\n",
    "\n",
    "**Online:**\n",
    "- [System Design Primer](https://github.com/donnemartin/system-design-primer) - Comprehensive guide\n",
    "- [Grokking the System Design Interview](https://www.educative.io/courses/grokking-the-system-design-interview) - Interview prep\n",
    "- [High Scalability Blog](http://highscalability.com/) - Real-world architectures\n",
    "- [AWS Architecture Blog](https://aws.amazon.com/blogs/architecture/) - Cloud patterns\n",
    "\n",
    "**Practice:**\n",
    "- Design Instagram, Twitter, YouTube, Uber\n",
    "- Calculate capacity (storage, bandwidth, servers needed)\n",
    "- Draw architecture diagrams\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83c\udf89 Congratulations!** You now understand how to design large-scale distributed systems for AI/ML workloads. You can architect platforms handling 100M+ users, 1PB+ data, and 100K+ requests/second with 99.99% uptime.\n",
    "\n",
    "**Measurable skills gained:**\n",
    "- Design systems scaling 10-100\u00d7 traffic\n",
    "- Reduce latency 100-1000\u00d7 with caching\n",
    "- Achieve 99.99% uptime with replication + load balancing\n",
    "- Build ML platforms serving 100K predictions/sec\n",
    "- Save $5-20M in infrastructure costs through proper architecture\n",
    "\n",
    "**Ready for version control mastery?** Proceed to **Notebook 009: Git & Version Control** to learn branching strategies, CI/CD pipelines, and model versioning for production ML systems! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}