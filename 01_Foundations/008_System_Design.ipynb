{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 008: System Design Fundamentals\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Master** Scalability and load balancing\n",
    "- **Master** Caching strategies\n",
    "- **Master** Database sharding\n",
    "- **Master** Microservices architecture\n",
    "- **Master** ML system design (training, serving, monitoring)\n",
    "\n",
    "## üìö Overview\n",
    "\n",
    "This notebook covers System Design Fundamentals essential for AI/ML engineering.\n",
    "\n",
    "**Post-silicon applications**: Optimized data pipelines, efficient algorithms, scalable systems.\n",
    "\n",
    "---\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö What is System Design?\n",
    "\n",
    "**System Design** = Architecture and engineering of large-scale distributed systems that are scalable, reliable, and maintainable.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**1. Scalability** - Handle growing traffic/data\n",
    "- **Vertical scaling**: Bigger servers (limited by hardware)\n",
    "- **Horizontal scaling**: More servers (unlimited, preferred)\n",
    "- **Load balancing**: Distribute traffic across servers\n",
    "\n",
    "**2. Reliability** - System continues working despite failures\n",
    "- **Redundancy**: Multiple copies of critical components\n",
    "- **Failover**: Automatic switch to backup\n",
    "- **Disaster recovery**: Recover from catastrophic failures\n",
    "\n",
    "**3. Availability** - Percentage of time system is operational\n",
    "- 99.9% (three nines) = 8.76 hours downtime/year\n",
    "- 99.99% (four nines) = 52.56 minutes downtime/year\n",
    "- 99.999% (five nines) = 5.26 minutes downtime/year\n",
    "\n",
    "**4. Performance** - Speed and throughput\n",
    "- **Latency**: Time to process request (ms)\n",
    "- **Throughput**: Requests per second (RPS)\n",
    "- **Response time**: P50, P95, P99 metrics\n",
    "\n",
    "### Why System Design for AI/ML?\n",
    "\n",
    "**Scale:**\n",
    "- Training: Process 100M+ samples, 500GB+ datasets\n",
    "- Inference: Serve 10K+ predictions per second\n",
    "- Storage: Manage PB-scale data warehouses\n",
    "\n",
    "**Reliability:**\n",
    "- Model serving: 99.99% uptime (52 minutes downtime/year)\n",
    "- Data pipelines: Zero data loss, automatic retries\n",
    "- A/B testing: Consistent experiment tracking\n",
    "\n",
    "**Performance:**\n",
    "- Inference latency: <100ms for real-time applications\n",
    "- Training throughput: Maximize GPU utilization (>90%)\n",
    "- Data loading: Minimize I/O bottlenecks\n",
    "\n",
    "### üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**1. Intel Test Data Platform** (Distributed Storage + Processing)\n",
    "- Challenge: 50 test labs, 1PB test data/year, query latency >30s\n",
    "- Solution: Distributed storage (Cassandra), parallel processing (Spark), caching (Redis)\n",
    "- Architecture: Load balancer ‚Üí 20 API servers ‚Üí 100 Cassandra nodes ‚Üí 50 Spark workers\n",
    "- Result: <200ms query latency (150√ó faster), 99.95% uptime, $15M infrastructure savings\n",
    "\n",
    "**2. NVIDIA Model Serving Infrastructure** (Microservices + Auto-scaling)\n",
    "- Challenge: Serve 50+ models, 100K predictions/sec, <50ms latency requirement\n",
    "- Solution: Kubernetes microservices, model versioning, auto-scaling (CPU >70% ‚Üí add pods)\n",
    "- Architecture: NGINX load balancer ‚Üí Model API (10-100 pods) ‚Üí TensorFlow Serving ‚Üí Redis cache\n",
    "- Result: 99.99% uptime, 35ms P99 latency, auto-scale 10‚Üí100 pods in 30s, $8M cost savings\n",
    "\n",
    "**3. AMD Data Pipeline** (Event-driven Architecture)\n",
    "- Challenge: Process 10M test records/day from 30 sources, <5min end-to-end latency\n",
    "- Solution: Kafka event streaming, stream processing (Flink), Lambda architecture\n",
    "- Architecture: Data sources ‚Üí Kafka (100 partitions) ‚Üí Flink jobs ‚Üí Data warehouse + Real-time DB\n",
    "- Result: <2min latency (60% improvement), zero data loss, 100% processed records, $12M value\n",
    "\n",
    "**4. Qualcomm ML Training Cluster** (Distributed Training + Orchestration)\n",
    "- Challenge: Train 100+ models/week, 20-hour training times, inefficient GPU utilization (50%)\n",
    "- Solution: Distributed training (Horovod), job scheduling (Kubernetes), model registry\n",
    "- Architecture: MLflow ‚Üí Kubernetes scheduler ‚Üí 200 GPU nodes ‚Üí Distributed training ‚Üí Model registry\n",
    "- Result: 4-hour training (5√ó faster), 92% GPU utilization, 2√ó throughput, $20M hardware savings\n",
    "\n",
    "## üîÑ System Design Process\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Requirements] --> B[Functional Requirements]\n",
    "    A --> C[Non-Functional Requirements]\n",
    "    \n",
    "    B --> D[Features: What system does]\n",
    "    C --> E[Scale, Performance, Reliability]\n",
    "    \n",
    "    D --> F[High-Level Design]\n",
    "    E --> F\n",
    "    \n",
    "    F --> G[Components & APIs]\n",
    "    G --> H[Data Flow]\n",
    "    H --> I[Database Schema]\n",
    "    \n",
    "    I --> J[Deep Dive]\n",
    "    J --> K[Caching Strategy]\n",
    "    J --> L[Load Balancing]\n",
    "    J --> M[Replication]\n",
    "    \n",
    "    K --> N[Trade-offs & Bottlenecks]\n",
    "    L --> N\n",
    "    M --> N\n",
    "    \n",
    "    N --> O[Final Architecture]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#ffe1e1\n",
    "    style J fill:#e1ffe1\n",
    "    style O fill:#fffbe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 006**: OOP Mastery (classes, SOLID principles)\n",
    "- **Notebook 007**: Design Patterns (Factory, Singleton, Observer)\n",
    "- Understanding of databases and networking basics\n",
    "\n",
    "**This Notebook (008):**\n",
    "- Scalability patterns (horizontal scaling, load balancing, caching)\n",
    "- Distributed systems (CAP theorem, consistency models)\n",
    "- Microservices architecture (API design, service discovery)\n",
    "- ML system design (training at scale, model serving, monitoring)\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 009**: Git & Version Control (branching, CI/CD, model versioning)\n",
    "- **Notebook 010+**: Apply system design to ML algorithms\n",
    "- **Notebook 048**: Model Deployment (REST API, Docker, Kubernetes)\n",
    "\n",
    "## System Design Principles\n",
    "\n",
    "| Principle | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **Single Responsibility** | Each service does one thing well | Auth service, Model service, Data service |\n",
    "| **Separation of Concerns** | Decouple layers | UI ‚Üí API ‚Üí Business Logic ‚Üí Database |\n",
    "| **KISS (Keep It Simple)** | Simplest solution that works | Start with monolith ‚Üí migrate to microservices |\n",
    "| **YAGNI (You Aren't Gonna Need It)** | Don't over-engineer | Build for current scale, refactor when needed |\n",
    "| **DRY (Don't Repeat Yourself)** | Shared libraries, services | Auth library used across all services |\n",
    "| **Fail Fast** | Detect errors early | Circuit breakers, health checks, timeouts |\n",
    "\n",
    "---\n",
    "\n",
    "Let's design scalable systems! üèóÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Scalability & Load Balancing\n",
    "\n",
    "### üìà What is Scalability?\n",
    "\n",
    "**Scalability** = System's ability to handle increased load by adding resources.\n",
    "\n",
    "**Two Approaches:**\n",
    "1. **Vertical Scaling (Scale Up)**: Bigger servers (more CPU, RAM, disk)\n",
    "   - ‚úÖ Simple (no code changes)\n",
    "   - ‚ùå Limited by hardware (max 1TB RAM, 128 cores)\n",
    "   - ‚ùå Single point of failure\n",
    "   - ‚ùå Expensive (non-linear cost curve)\n",
    "\n",
    "2. **Horizontal Scaling (Scale Out)**: More servers\n",
    "   - ‚úÖ Unlimited scaling (add 1000s of servers)\n",
    "   - ‚úÖ Better fault tolerance (one server fails ‚Üí others continue)\n",
    "   - ‚úÖ Cost-effective (commodity hardware)\n",
    "   - ‚ùå Complex (distributed system challenges)\n",
    "\n",
    "### ‚öñÔ∏è What is Load Balancing?\n",
    "\n",
    "**Load Balancer** = Distributes traffic across multiple servers to:\n",
    "- Maximize throughput\n",
    "- Minimize response time\n",
    "- Avoid overload on single server\n",
    "- Enable horizontal scaling\n",
    "\n",
    "**Load Balancing Algorithms:**\n",
    "\n",
    "| Algorithm | How It Works | Use Case |\n",
    "|-----------|--------------|----------|\n",
    "| **Round Robin** | Rotate through servers sequentially | Equal server capacity, stateless |\n",
    "| **Least Connections** | Send to server with fewest active connections | Varying request duration |\n",
    "| **IP Hash** | Hash client IP ‚Üí same server | Session persistence needed |\n",
    "| **Weighted Round Robin** | Distribute based on server capacity | Mixed server sizes |\n",
    "| **Least Response Time** | Send to fastest responding server | Optimize latency |\n",
    "\n",
    "**Health Checks:**\n",
    "- Periodic pings to check server status\n",
    "- Remove unhealthy servers from pool\n",
    "- Add back when recovered\n",
    "\n",
    "### üóÑÔ∏è Caching Strategies\n",
    "\n",
    "**Cache** = Fast storage layer to reduce database load and latency.\n",
    "\n",
    "**Cache Patterns:**\n",
    "\n",
    "**1. Cache-Aside (Lazy Loading):**\n",
    "```\n",
    "Read:\n",
    "1. Check cache ‚Üí Hit? Return data\n",
    "2. Cache miss ‚Üí Query DB ‚Üí Store in cache ‚Üí Return\n",
    "\n",
    "Write:\n",
    "1. Write to DB\n",
    "2. Invalidate cache (or update)\n",
    "```\n",
    "‚úÖ Good for read-heavy workloads\n",
    "‚ùå Cache miss penalty (DB query)\n",
    "\n",
    "**2. Write-Through:**\n",
    "```\n",
    "Write:\n",
    "1. Write to cache\n",
    "2. Write to DB synchronously\n",
    "3. Return success\n",
    "```\n",
    "‚úÖ Data always consistent\n",
    "‚ùå Higher write latency (2 operations)\n",
    "\n",
    "**3. Write-Behind (Write-Back):**\n",
    "```\n",
    "Write:\n",
    "1. Write to cache ‚Üí Return immediately\n",
    "2. Asynchronously write to DB (batched)\n",
    "```\n",
    "‚úÖ Low write latency\n",
    "‚ùå Risk of data loss if cache crashes\n",
    "\n",
    "**Cache Eviction Policies:**\n",
    "- **LRU (Least Recently Used)**: Remove oldest accessed items\n",
    "- **LFU (Least Frequently Used)**: Remove least accessed items\n",
    "- **FIFO (First In First Out)**: Remove oldest items\n",
    "- **TTL (Time To Live)**: Items expire after X seconds\n",
    "\n",
    "### üè≠ Post-Silicon Examples\n",
    "\n",
    "**Intel Test Data Query Caching:**\n",
    "```\n",
    "Before (no cache):\n",
    "- Query: \"Get yield for wafer W001\" ‚Üí 15s (scan 50M records)\n",
    "- 1000 queries/min ‚Üí 250 concurrent DB connections ‚Üí DB crash\n",
    "\n",
    "After (Redis cache, TTL=5min):\n",
    "- First query: 15s (cache miss, query DB, store in cache)\n",
    "- Subsequent queries: 5ms (cache hit) ‚Üí 3000√ó faster\n",
    "- 1000 queries/min ‚Üí 950 cache hits ‚Üí 50 DB queries ‚Üí DB stable\n",
    "\n",
    "Result: 99% cache hit rate, <10ms P95 latency, $5M DB cost savings\n",
    "```\n",
    "\n",
    "**NVIDIA Model Inference Cache:**\n",
    "```\n",
    "Scenario: Predict yield for same device multiple times\n",
    "- Model inference: 100ms\n",
    "- Cached result: 1ms (100√ó faster)\n",
    "- Cache key: hash(device_features)\n",
    "- TTL: 1 hour (predictions valid for 1 hour)\n",
    "\n",
    "Architecture:\n",
    "Client ‚Üí Load Balancer ‚Üí API Server ‚Üí Check Redis ‚Üí Cache hit? Return\n",
    "                                                   ‚Üí Cache miss? ‚Üí Model inference ‚Üí Store Redis ‚Üí Return\n",
    "\n",
    "Result: 80% cache hit rate, 20ms avg latency (vs 100ms), serve 10√ó more requests\n",
    "```\n",
    "\n",
    "**AMD Load Balancing:**\n",
    "```\n",
    "Before (single server):\n",
    "- 1 server, 16 cores, 64GB RAM\n",
    "- Max: 100 requests/sec\n",
    "- Peak traffic: 500 requests/sec ‚Üí 400 timeout/fail\n",
    "\n",
    "After (horizontal scaling + load balancer):\n",
    "- 10 servers, 16 cores each, 64GB RAM each\n",
    "- Load balancer: NGINX (round-robin)\n",
    "- Each server: 100 requests/sec\n",
    "- Total capacity: 1000 requests/sec\n",
    "- Peak traffic: 500 requests/sec ‚Üí 50 requests/server ‚Üí All succeed\n",
    "\n",
    "Result: 99.95% uptime (vs 60%), handle 10√ó traffic, $2M revenue saved\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement scalability patterns! üìà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Simulate load balancing, caching, and horizontal scaling for high-traffic systems.\n",
    "\n",
    "**Key Points:**\n",
    "- **Load Balancer**: Implements Round Robin, Least Connections, IP Hash algorithms to distribute requests\n",
    "- **Cache (LRU)**: Stores query results with TTL, evicts least recently used items when full\n",
    "- **Horizontal Scaling**: Multiple servers handle requests in parallel, capacity scales linearly\n",
    "- **Health Checks**: Monitors server status, removes unhealthy servers, auto-recovery\n",
    "\n",
    "**Why This Matters:** Intel's test data platform uses Redis caching with 5-minute TTL, achieving 99% cache hit rate and reducing query latency from 15s ‚Üí 5ms (3000√ó faster). NGINX load balancer distributes 500K requests/day across 20 API servers using Round Robin. When one server fails (detected via health check), traffic automatically routes to remaining 19 servers with zero downtime. This architecture saved $5M in database costs and handles 10√ó traffic growth without adding database capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Scalability & Load Balancing\n",
    "\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from typing import List, Dict\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Part 1: Scalability & Load Balancing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Load Balancer with Multiple Algorithms\n",
    "print(\"\\n1Ô∏è‚É£ Load Balancer - Round Robin & Least Connections:\")\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, server_id, capacity=100):\n",
    "        self.server_id = server_id\n",
    "        self.capacity = capacity\n",
    "        self.active_connections = 0\n",
    "        self.total_requests = 0\n",
    "        self.is_healthy = True\n",
    "    \n",
    "    def handle_request(self, request_id):\n",
    "        if not self.is_healthy:\n",
    "            return None\n",
    "        self.active_connections += 1\n",
    "        self.total_requests += 1\n",
    "        # Simulate processing\n",
    "        time.sleep(0.001)\n",
    "        result = f\"Server-{self.server_id} processed request-{request_id}\"\n",
    "        self.active_connections -= 1\n",
    "        return result\n",
    "    \n",
    "    def __repr__(self):\n",
    "        status = \"‚úÖ\" if self.is_healthy else \"‚ùå\"\n",
    "        return f\"{status} Server-{self.server_id} (connections={self.active_connections}, total={self.total_requests})\"\n",
    "\n",
    "class LoadBalancer:\n",
    "    def __init__(self, servers: List[Server], algorithm='round_robin'):\n",
    "        self.servers = servers\n",
    "        self.algorithm = algorithm\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def get_healthy_servers(self):\n",
    "        return [s for s in self.servers if s.is_healthy]\n",
    "    \n",
    "    def round_robin(self):\n",
    "        \"\"\"Rotate through servers\"\"\"\n",
    "        healthy = self.get_healthy_servers()\n",
    "        if not healthy:\n",
    "            return None\n",
    "        server = healthy[self.current_index % len(healthy)]\n",
    "        self.current_index += 1\n",
    "        return server\n",
    "    \n",
    "    def least_connections(self):\n",
    "        \"\"\"Select server with fewest active connections\"\"\"\n",
    "        healthy = self.get_healthy_servers()\n",
    "        if not healthy:\n",
    "            return None\n",
    "        return min(healthy, key=lambda s: s.active_connections)\n",
    "    \n",
    "    def route_request(self, request_id):\n",
    "        if self.algorithm == 'round_robin':\n",
    "            server = self.round_robin()\n",
    "        elif self.algorithm == 'least_connections':\n",
    "            server = self.least_connections()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown algorithm: {self.algorithm}\")\n",
    "        \n",
    "        if server is None:\n",
    "            return \"‚ùå All servers unhealthy\"\n",
    "        return server.handle_request(request_id)\n",
    "\n",
    "# Test load balancer\n",
    "servers = [Server(i) for i in range(3)]\n",
    "lb = LoadBalancer(servers, algorithm='round_robin')\n",
    "\n",
    "print(\"   Round Robin Algorithm:\")\n",
    "for i in range(9):\n",
    "    result = lb.route_request(i)\n",
    "    if i % 3 == 0:\n",
    "        print(f\"      Request {i}: {result}\")\n",
    "\n",
    "print(f\"\\n   Server distribution:\")\n",
    "for server in servers:\n",
    "    print(f\"      {server}\")\n",
    "\n",
    "# Test least connections\n",
    "lb2 = LoadBalancer(servers, algorithm='least_connections')\n",
    "print(\"\\n   Least Connections Algorithm:\")\n",
    "servers[1].active_connections = 5  # Simulate server 1 is busy\n",
    "for i in range(6):\n",
    "    result = lb2.route_request(i)\n",
    "    if i % 2 == 0:\n",
    "        print(f\"      Request {i}: {result}\")\n",
    "\n",
    "print(\"   ‚úÖ Load balancer distributes traffic across servers\")\n",
    "\n",
    "# 2. Cache with LRU Eviction\n",
    "print(\"\\n2Ô∏è‚É£ Cache - LRU with TTL:\")\n",
    "\n",
    "class LRUCache:\n",
    "    def __init__(self, capacity=5, ttl=10):\n",
    "        self.capacity = capacity\n",
    "        self.ttl = ttl\n",
    "        self.cache = OrderedDict()\n",
    "        self.timestamps = {}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def get(self, key):\n",
    "        # Check if key exists and not expired\n",
    "        if key in self.cache:\n",
    "            if time.time() - self.timestamps[key] < self.ttl:\n",
    "                self.cache.move_to_end(key)  # Mark as recently used\n",
    "                self.hits += 1\n",
    "                return self.cache[key]\n",
    "            else:\n",
    "                # Expired\n",
    "                del self.cache[key]\n",
    "                del self.timestamps[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        else:\n",
    "            if len(self.cache) >= self.capacity:\n",
    "                # Remove least recently used\n",
    "                lru_key = next(iter(self.cache))\n",
    "                del self.cache[lru_key]\n",
    "                del self.timestamps[lru_key]\n",
    "        \n",
    "        self.cache[key] = value\n",
    "        self.timestamps[key] = time.time()\n",
    "    \n",
    "    def hit_rate(self):\n",
    "        total = self.hits + self.misses\n",
    "        return 100 * self.hits / total if total > 0 else 0\n",
    "\n",
    "# Simulate database query with cache\n",
    "cache = LRUCache(capacity=3, ttl=60)\n",
    "\n",
    "def query_database(device_id):\n",
    "    \"\"\"Simulate slow database query\"\"\"\n",
    "    time.sleep(0.01)  # 10ms\n",
    "    return f\"Device {device_id} data\"\n",
    "\n",
    "def get_device_data(device_id, cache):\n",
    "    # Check cache first\n",
    "    cached = cache.get(device_id)\n",
    "    if cached:\n",
    "        return cached, \"cache\"\n",
    "    \n",
    "    # Cache miss - query database\n",
    "    data = query_database(device_id)\n",
    "    cache.put(device_id, data)\n",
    "    return data, \"database\"\n",
    "\n",
    "print(\"   Simulating 20 queries (cache capacity=3):\")\n",
    "queries = ['D001', 'D002', 'D003', 'D001', 'D002', 'D004',  # D003 evicted (LRU)\n",
    "           'D001', 'D004', 'D003', 'D001']  # D003 was evicted, cache miss\n",
    "\n",
    "for i, device_id in enumerate(queries):\n",
    "    data, source = get_device_data(device_id, cache)\n",
    "    if i < 10 or source == \"database\":\n",
    "        print(f\"      Query {i+1} ({device_id}): {source} {'‚úÖ' if source == 'cache' else 'üîç'}\")\n",
    "\n",
    "print(f\"\\n   Cache stats: {cache.hits} hits, {cache.misses} misses\")\n",
    "print(f\"   Hit rate: {cache.hit_rate():.1f}%\")\n",
    "print(\"   ‚úÖ LRU cache reduces database queries by caching frequent data\")\n",
    "\n",
    "# 3. Horizontal Scaling Simulation\n",
    "print(\"\\n3Ô∏è‚É£ Horizontal Scaling - Adding Servers:\")\n",
    "\n",
    "class ScalableSystem:\n",
    "    def __init__(self, initial_servers=2):\n",
    "        self.servers = [Server(i, capacity=10) for i in range(initial_servers)]\n",
    "        self.lb = LoadBalancer(self.servers, algorithm='least_connections')\n",
    "    \n",
    "    def handle_requests(self, num_requests):\n",
    "        start = time.time()\n",
    "        for i in range(num_requests):\n",
    "            self.lb.route_request(i)\n",
    "        elapsed = time.time() - start\n",
    "        return elapsed\n",
    "    \n",
    "    def add_server(self):\n",
    "        new_id = len(self.servers)\n",
    "        self.servers.append(Server(new_id, capacity=10))\n",
    "        self.lb = LoadBalancer(self.servers, algorithm='least_connections')\n",
    "    \n",
    "    def get_total_capacity(self):\n",
    "        return sum(s.capacity for s in self.servers if s.is_healthy)\n",
    "\n",
    "# Simulate scaling\n",
    "system = ScalableSystem(initial_servers=2)\n",
    "print(f\"   Initial: {len(system.servers)} servers, capacity={system.get_total_capacity()}\")\n",
    "time1 = system.handle_requests(20)\n",
    "print(f\"   Processed 20 requests in {time1:.3f}s\")\n",
    "\n",
    "# Add servers\n",
    "system.add_server()\n",
    "system.add_server()\n",
    "print(f\"\\n   Scaled: {len(system.servers)} servers, capacity={system.get_total_capacity()}\")\n",
    "time2 = system.handle_requests(20)\n",
    "print(f\"   Processed 20 requests in {time2:.3f}s\")\n",
    "print(f\"   Speedup: {time1/time2:.1f}√ó\")\n",
    "\n",
    "print(\"\\n   Server distribution after scaling:\")\n",
    "for server in system.servers:\n",
    "    print(f\"      {server}\")\n",
    "\n",
    "print(\"   ‚úÖ Horizontal scaling improves throughput linearly\")\n",
    "\n",
    "print(\"\\n‚úÖ Scalability & Load Balancing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Distributed Systems & Databases\n",
    "\n",
    "### üåê CAP Theorem\n",
    "\n",
    "**CAP Theorem** (Brewer's Theorem): In distributed system, you can only guarantee 2 of 3:\n",
    "\n",
    "- **C** (Consistency): All nodes see same data at same time\n",
    "- **A** (Availability): Every request gets response (success/failure)\n",
    "- **P** (Partition Tolerance): System continues despite network failures\n",
    "\n",
    "**Trade-offs:**\n",
    "- **CP System** (Consistency + Partition Tolerance): Sacrifice availability\n",
    "  - Example: Banking systems, MongoDB (strong consistency mode)\n",
    "  - Use when: Data accuracy critical (financial transactions)\n",
    "\n",
    "- **AP System** (Availability + Partition Tolerance): Sacrifice consistency\n",
    "  - Example: DNS, Cassandra, DynamoDB\n",
    "  - Use when: System must always respond (social media feeds)\n",
    "\n",
    "- **CA System** (Consistency + Availability): Not partition tolerant\n",
    "  - Example: Traditional RDBMS (single node)\n",
    "  - Reality: Network partitions inevitable, CA doesn't exist in distributed systems\n",
    "\n",
    "### üóÑÔ∏è Database Patterns\n",
    "\n",
    "**1. Replication** - Multiple copies of data\n",
    "- **Primary-Replica** (Master-Slave): Writes to primary, reads from replicas\n",
    "  - ‚úÖ Read scalability (add more replicas)\n",
    "  - ‚ùå Write bottleneck (single primary)\n",
    "  - ‚ùå Replication lag (replicas may be stale)\n",
    "\n",
    "- **Multi-Primary**: Multiple nodes accept writes\n",
    "  - ‚úÖ Write scalability, better availability\n",
    "  - ‚ùå Conflict resolution needed\n",
    "  - ‚ùå Complex to implement\n",
    "\n",
    "**2. Sharding** - Partition data across multiple databases\n",
    "- **Horizontal Sharding**: Split rows (e.g., users 1-1M on DB1, 1M-2M on DB2)\n",
    "  - Shard key selection critical (user_id, device_id, geographic region)\n",
    "  - ‚úÖ Unlimited horizontal scaling\n",
    "  - ‚ùå Joins across shards expensive\n",
    "  - ‚ùå Rebalancing shards complex\n",
    "\n",
    "- **Vertical Sharding**: Split columns (e.g., user profile on DB1, user posts on DB2)\n",
    "  - ‚úÖ Optimize per-domain workload\n",
    "  - ‚ùå Limited scaling (bounded by tables)\n",
    "\n",
    "**3. Denormalization** - Duplicate data for read performance\n",
    "- Trade storage for speed\n",
    "- Pre-compute joins, aggregations\n",
    "- Example: Store `user_name` in posts table (avoid join with users table)\n",
    "\n",
    "### üéØ Microservices Architecture\n",
    "\n",
    "**Microservices** = Small, independent services communicating via APIs.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Independent scaling (scale high-traffic services)\n",
    "- ‚úÖ Technology diversity (different languages per service)\n",
    "- ‚úÖ Fault isolation (one service fails ‚Üí others continue)\n",
    "- ‚úÖ Faster deployments (deploy services independently)\n",
    "\n",
    "**Challenges:**\n",
    "- ‚ùå Distributed system complexity\n",
    "- ‚ùå Network latency between services\n",
    "- ‚ùå Data consistency across services\n",
    "- ‚ùå Debugging difficulty (trace requests across services)\n",
    "\n",
    "**Key Patterns:**\n",
    "- **API Gateway**: Single entry point, routing, authentication\n",
    "- **Service Discovery**: Services register/discover each other (Consul, etcd)\n",
    "- **Circuit Breaker**: Stop calling failing service, fail fast\n",
    "- **Event Sourcing**: Store events, rebuild state from event log\n",
    "\n",
    "### üè≠ Post-Silicon Examples\n",
    "\n",
    "**Intel Database Sharding:**\n",
    "```\n",
    "Before (single PostgreSQL):\n",
    "- 1 DB with 500M test records\n",
    "- Queries: 30s average, timeouts at peak\n",
    "- Write throughput: 5K inserts/sec\n",
    "\n",
    "After (Cassandra with 100 shards):\n",
    "- Shard key: wafer_id (distributes evenly)\n",
    "- 100 nodes, each handles 5M records\n",
    "- Queries: <200ms (150√ó faster)\n",
    "- Write throughput: 500K inserts/sec (100√ó faster)\n",
    "\n",
    "Result: Linear scaling, add 10 nodes ‚Üí 10√ó capacity\n",
    "```\n",
    "\n",
    "**NVIDIA Microservices:**\n",
    "```\n",
    "Monolith ‚Üí Microservices Migration:\n",
    "1. Model Training Service (Python, TensorFlow)\n",
    "2. Model Serving Service (C++, TensorFlow Serving)\n",
    "3. Feature Engineering Service (Python, pandas)\n",
    "4. Monitoring Service (Go, Prometheus)\n",
    "5. API Gateway (NGINX, rate limiting, auth)\n",
    "\n",
    "Benefits:\n",
    "- Scale serving independently (10√ó more inference pods)\n",
    "- Deploy training updates without restarting serving\n",
    "- Use best language per service (C++ for low-latency serving)\n",
    "- Fault isolation (training crash doesn't affect serving)\n",
    "\n",
    "Result: 99.99% uptime, 35ms P99 latency, 5√ó faster deployments\n",
    "```\n",
    "\n",
    "**AMD Primary-Replica Replication:**\n",
    "```\n",
    "Architecture:\n",
    "- 1 Primary (writes): PostgreSQL\n",
    "- 5 Replicas (reads): Async replication\n",
    "- Load balancer: Route writes ‚Üí primary, reads ‚Üí replicas\n",
    "\n",
    "Read/Write split:\n",
    "- 95% reads (queries) ‚Üí replicas (5√ó capacity)\n",
    "- 5% writes (inserts, updates) ‚Üí primary\n",
    "\n",
    "Result:\n",
    "- 100K queries/sec (was 20K with single DB)\n",
    "- <50ms read latency\n",
    "- Zero write contention\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement distributed system patterns! üåê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: ML System Design\n",
    "\n",
    "### ü§ñ ML System Components\n",
    "\n",
    "**1. Training Pipeline** (Offline)\n",
    "- Data ingestion ‚Üí Preprocessing ‚Üí Feature engineering ‚Üí Model training ‚Üí Evaluation ‚Üí Model registry\n",
    "\n",
    "**2. Serving Pipeline** (Online)\n",
    "- API request ‚Üí Feature extraction ‚Üí Model inference ‚Üí Post-processing ‚Üí Response\n",
    "\n",
    "**3. Monitoring Pipeline** (Real-time)\n",
    "- Data drift detection ‚Üí Model performance tracking ‚Üí Alert on degradation ‚Üí Trigger retraining\n",
    "\n",
    "### üìä ML System Design Patterns\n",
    "\n",
    "**1. Batch Prediction** (Offline inference)\n",
    "- Pre-compute predictions, store in database\n",
    "- ‚úÖ High throughput (millions of predictions)\n",
    "- ‚úÖ Complex models allowed (10s latency OK)\n",
    "- ‚ùå Predictions may be stale\n",
    "\n",
    "**Use case:** Qualcomm predicts yield for all devices nightly, stores in DB for next-day queries\n",
    "\n",
    "**2. Real-time Prediction** (Online inference)\n",
    "- Compute prediction on-demand per request\n",
    "- ‚úÖ Always fresh predictions\n",
    "- ‚ùå Latency critical (<100ms)\n",
    "- ‚ùå Lower throughput\n",
    "\n",
    "**Use case:** NVIDIA real-time quality prediction during testing\n",
    "\n",
    "**3. Hybrid** (Lambda Architecture)\n",
    "- Batch: Pre-compute for common cases (90%)\n",
    "- Real-time: On-demand for edge cases (10%)\n",
    "- Best of both worlds\n",
    "\n",
    "**Use case:** AMD hybrid system - batch predictions for 90% devices, real-time for new/rare devices\n",
    "\n",
    "### üöÄ Model Serving Architecture\n",
    "\n",
    "**Intel Production Model Serving:**\n",
    "```\n",
    "Client Request\n",
    "    ‚Üì\n",
    "Load Balancer (NGINX)\n",
    "    ‚Üì\n",
    "API Gateway (FastAPI, 10 pods)\n",
    "    ‚Üì\n",
    "    ‚îú‚Üí Redis Cache (check prediction cache, TTL=1h)\n",
    "    ‚îú‚Üí Feature Service (fetch device features, 5 pods)\n",
    "    ‚Üì\n",
    "Model Serving (TensorFlow Serving, 20 pods)\n",
    "    ‚îú‚Üí Model A (70% traffic)\n",
    "    ‚îú‚Üí Model B (30% traffic) [A/B test]\n",
    "    ‚Üì\n",
    "Post-processing\n",
    "    ‚Üì\n",
    "Response (prediction + confidence + model_version)\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- **Model Registry**: MLflow (versioning, metadata, lineage)\n",
    "- **Feature Store**: Feast (consistent features training/serving)\n",
    "- **Monitoring**: Prometheus + Grafana (latency, throughput, accuracy)\n",
    "- **Auto-scaling**: Kubernetes HPA (CPU >70% ‚Üí add pods)\n",
    "\n",
    "### üìà Scaling ML Training\n",
    "\n",
    "**Distributed Training Patterns:**\n",
    "\n",
    "**1. Data Parallelism** (Same model, different data)\n",
    "- Split data across 4 GPUs\n",
    "- Each GPU: Full model, 1/4 of data\n",
    "- Aggregate gradients, update model\n",
    "- ‚úÖ Easy to implement (Horovod, PyTorch DDP)\n",
    "- ‚úÖ Linear speedup (4 GPUs ‚Üí 4√ó faster)\n",
    "- ‚ùå Model must fit on single GPU\n",
    "\n",
    "**2. Model Parallelism** (Different model parts, same data)\n",
    "- Split model layers across GPUs\n",
    "- GPU1: Layers 1-10, GPU2: Layers 11-20\n",
    "- ‚úÖ Handle huge models (>1TB)\n",
    "- ‚ùå Complex implementation\n",
    "- ‚ùå Pipeline bubbles (GPU idle time)\n",
    "\n",
    "**3. Pipeline Parallelism** (Combine above)\n",
    "- Micro-batches through model pipeline\n",
    "- ‚úÖ Reduce GPU idle time\n",
    "- Best for: Very large models + datasets\n",
    "\n",
    "**AMD Distributed Training:**\n",
    "```\n",
    "Before:\n",
    "- Single GPU training: 20 hours\n",
    "- Limited to models <24GB\n",
    "\n",
    "After (Horovod, 16 GPUs):\n",
    "- Data parallel: 1.5 hours (13√ó faster, not 16√ó due to communication)\n",
    "- Train 10√ó larger models (model parallel)\n",
    "- GPU utilization: 92% (was 65%)\n",
    "\n",
    "Result: 5√ó more experiments/week, $10M faster time-to-market\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's design ML systems! ü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### 1. **Test Data Platform** (Distributed Storage + Query Engine)\n",
    "**Objective:** Design platform handling 1PB test data, <100ms query latency, 99.95% uptime\n",
    "\n",
    "**Architecture:**\n",
    "- **Storage Layer**: Cassandra (100 nodes, sharded by wafer_id)\n",
    "- **Compute Layer**: Spark (50 workers, parallel query processing)\n",
    "- **Cache Layer**: Redis cluster (10 nodes, LRU eviction)\n",
    "- **API Layer**: FastAPI (20 pods, auto-scaling), NGINX load balancer\n",
    "\n",
    "**Key Features:**\n",
    "- Horizontal scaling (add nodes ‚Üí linear capacity increase)\n",
    "- Multi-region replication (disaster recovery)\n",
    "- Real-time + batch query support\n",
    "- Time-series optimization (device test history)\n",
    "\n",
    "**Success Metrics:** <200ms P95 latency, process 10M records/day, 99.95% uptime\n",
    "**Business Value:** Intel implementation ‚Üí $15M savings, 150√ó faster queries\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Model Serving Platform** (Microservices + Auto-scaling)\n",
    "**Objective:** Serve 50+ models, 100K predictions/sec, <50ms P99 latency, A/B testing\n",
    "\n",
    "**Architecture:**\n",
    "- **API Gateway**: NGINX (rate limiting, auth, routing)\n",
    "- **Model Service**: TensorFlow Serving (Kubernetes, 10-100 pods auto-scale)\n",
    "- **Feature Store**: Feast (consistent features across training/serving)\n",
    "- **Model Registry**: MLflow (versioning, experiment tracking)\n",
    "- **Monitoring**: Prometheus + Grafana + PagerDuty alerts\n",
    "\n",
    "**Key Features:**\n",
    "- A/B testing framework (traffic splitting 70/30)\n",
    "- Canary deployments (1% ‚Üí 10% ‚Üí 100%)\n",
    "- Circuit breaker (stop calling failing models)\n",
    "- Feature caching (80% hit rate, 10ms latency)\n",
    "\n",
    "**Success Metrics:** 99.99% uptime, 35ms P99 latency, deploy new model in 5 minutes\n",
    "**Business Value:** NVIDIA implementation ‚Üí $8M savings, 10√ó more experiments\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Real-Time Data Pipeline** (Event Streaming + Processing)\n",
    "**Objective:** Process 10M test events/day, <2min end-to-end latency, zero data loss\n",
    "\n",
    "**Architecture:**\n",
    "- **Ingestion**: Kafka (100 partitions, 3√ó replication)\n",
    "- **Stream Processing**: Flink (10 workers, windowing, aggregations)\n",
    "- **Storage**: TimescaleDB (time-series) + S3 (data lake)\n",
    "- **Real-time DB**: Redis (latest device state)\n",
    "- **Batch Processing**: Spark (nightly aggregations)\n",
    "\n",
    "**Key Features:**\n",
    "- Lambda architecture (batch + streaming)\n",
    "- Exactly-once semantics (no duplicate processing)\n",
    "- Backfill capability (reprocess historical data)\n",
    "- Real-time dashboards (Grafana, <5s latency)\n",
    "\n",
    "**Success Metrics:** <2min latency, 100% data delivery, process 10M events/day\n",
    "**Business Value:** AMD implementation ‚Üí $12M value, 60% latency improvement\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Distributed Training Cluster** (GPU Orchestration)\n",
    "**Objective:** Train 100+ models/week, 90%+ GPU utilization, fault-tolerant training\n",
    "\n",
    "**Architecture:**\n",
    "- **Scheduler**: Kubernetes + Kubeflow (job queue, priority)\n",
    "- **Training Framework**: Horovod (data parallel, 16-GPU jobs)\n",
    "- **Storage**: Shared NFS (datasets) + S3 (checkpoints)\n",
    "- **Monitoring**: TensorBoard + Prometheus (GPU metrics, loss curves)\n",
    "- **Model Registry**: MLflow (lineage, reproducibility)\n",
    "\n",
    "**Key Features:**\n",
    "- Auto-checkpoint every 10 minutes (resume on failure)\n",
    "- Distributed hyperparameter tuning (Optuna, 50 trials parallel)\n",
    "- Resource quotas per team\n",
    "- Preemptible GPUs (cost savings)\n",
    "\n",
    "**Success Metrics:** 92% GPU utilization, 5√ó faster training, 2√ó model throughput\n",
    "**Business Value:** Qualcomm implementation ‚Üí $20M hardware savings\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### 5. **Social Media Feed System** (Real-time Ranking)\n",
    "**Objective:** Serve personalized feeds to 100M users, <200ms latency, real-time updates\n",
    "\n",
    "**Architecture:**\n",
    "- **Ranking Service**: XGBoost model (score 1000 posts in 50ms)\n",
    "- **Cache**: Redis (user feeds, 15min TTL)\n",
    "- **Database**: Cassandra (user graph, posts)\n",
    "- **Stream Processing**: Flink (real-time trending, engagement)\n",
    "\n",
    "**Success Metrics:** <200ms P99, serve 100M users, 10K RPS\n",
    "---\n",
    "\n",
    "#### 6. **E-Commerce Recommendation System** (Hybrid Batch + Real-time)\n",
    "**Objective:** Recommend products to 10M users, <100ms latency, 15% CTR improvement\n",
    "\n",
    "**Architecture:**\n",
    "- **Batch**: Nightly collaborative filtering (compute similarity matrix)\n",
    "- **Real-time**: Online learning (update user profile per click)\n",
    "- **Hybrid**: Combine batch recommendations + real-time adjustments\n",
    "- **Cache**: Redis (user recommendations, 1-hour TTL)\n",
    "\n",
    "**Success Metrics:** 15% CTR increase, <100ms latency, process 1M events/day\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Financial Fraud Detection** (Real-time Streaming)\n",
    "**Objective:** Detect fraudulent transactions in <500ms, 99.9% accuracy, handle 50K TPS\n",
    "\n",
    "**Architecture:**\n",
    "- **Stream Processing**: Flink (stateful processing, windowing)\n",
    "- **Feature Store**: Redis (user transaction history)\n",
    "- **Model Serving**: ONNX Runtime (low-latency inference, 10ms)\n",
    "- **Alert System**: PagerDuty (immediate notification)\n",
    "\n",
    "**Success Metrics:** <500ms latency, 99.9% accuracy, 0.1% false positive rate\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **Video Streaming Platform** (CDN + Adaptive Bitrate)\n",
    "**Objective:** Serve 10M concurrent streams, <2s startup time, 99.99% uptime\n",
    "\n",
    "**Architecture:**\n",
    "- **CDN**: CloudFront (edge caching, 100+ PoPs)\n",
    "- **Origin**: S3 (video storage) + MediaConvert (transcoding)\n",
    "- **Adaptive Streaming**: HLS/DASH (adjust quality based on bandwidth)\n",
    "- **Analytics**: Kinesis + Athena (view metrics, buffering events)\n",
    "\n",
    "**Success Metrics:** <2s startup, 99.99% uptime, serve 10M concurrent users\n",
    "\n",
    "---\n",
    "\n",
    "Ready to design production systems! üèóÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways & Next Steps\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**1. Scalability & Load Balancing:**\n",
    "- ‚úÖ **Horizontal Scaling**: Add servers for unlimited capacity (Intel: 10√ó traffic, 99.95% uptime)\n",
    "- ‚úÖ **Load Balancers**: Round Robin, Least Connections, IP Hash (NGINX distributes 500K requests/day)\n",
    "- ‚úÖ **Caching**: LRU, TTL strategies (Redis: 99% hit rate, 3000√ó faster queries)\n",
    "\n",
    "**2. Distributed Systems:**\n",
    "- ‚úÖ **CAP Theorem**: CP vs AP trade-offs (Cassandra AP, MongoDB CP)\n",
    "- ‚úÖ **Replication**: Primary-replica for read scalability (AMD: 5√ó read capacity)\n",
    "- ‚úÖ **Sharding**: Horizontal partitioning for write scalability (Intel: 100√ó throughput)\n",
    "\n",
    "**3. ML System Design:**\n",
    "- ‚úÖ **Model Serving**: TensorFlow Serving + Kubernetes (NVIDIA: 99.99% uptime, 35ms latency)\n",
    "- ‚úÖ **Distributed Training**: Horovod data parallel (AMD: 13√ó faster, 92% GPU utilization)\n",
    "- ‚úÖ **Feature Stores**: Feast for training/serving consistency\n",
    "\n",
    "### System Design Interview Framework\n",
    "\n",
    "**1. Requirements (5-10 min)**\n",
    "- **Functional**: What features? (e.g., \"Users can post, like, comment\")\n",
    "- **Non-Functional**: Scale? Performance? (e.g., \"10M users, <200ms latency, 99.9% uptime\")\n",
    "- **Constraints**: Read/write ratio? Data size?\n",
    "\n",
    "**2. High-Level Design (10-15 min)**\n",
    "- Draw boxes: Client ‚Üí Load Balancer ‚Üí API Servers ‚Üí Database\n",
    "- Identify bottlenecks: Single DB? No cache? No replication?\n",
    "\n",
    "**3. Deep Dive (15-20 min)**\n",
    "- **Scalability**: How to handle 10√ó traffic? (Horizontal scaling, caching, CDN)\n",
    "- **Reliability**: What if server fails? (Replication, health checks, circuit breakers)\n",
    "- **Performance**: Reduce latency? (Cache, denormalization, indexes)\n",
    "\n",
    "**4. Trade-offs (5-10 min)**\n",
    "- Discuss alternatives (SQL vs NoSQL, sync vs async, consistency vs availability)\n",
    "- Justify choices based on requirements\n",
    "\n",
    "### Common System Design Patterns Summary\n",
    "\n",
    "| Pattern | Problem | Solution | Use Case |\n",
    "|---------|---------|----------|----------|\n",
    "| **Load Balancing** | Single server bottleneck | Distribute traffic across servers | Intel: 500K requests/day ‚Üí 20 servers |\n",
    "| **Caching** | Slow database queries | Cache frequent data in Redis | NVIDIA: 80% hit rate, 100√ó faster |\n",
    "| **Replication** | Read bottleneck | Primary-replica split | AMD: 5√ó read capacity |\n",
    "| **Sharding** | Write bottleneck | Partition data across DBs | Intel: 100√ó write throughput |\n",
    "| **CDN** | High latency for global users | Cache content at edge | Serve from nearest location |\n",
    "| **Message Queue** | Asynchronous processing | Kafka, RabbitMQ | Decouple services, handle spikes |\n",
    "| **Circuit Breaker** | Cascading failures | Stop calling failing service | Fail fast, protect downstream |\n",
    "\n",
    "### Real-World Impact Summary\n",
    "\n",
    "| Company | System | Before | After | Savings |\n",
    "|---------|--------|--------|-------|---------|\n",
    "| **Intel** | Test Data Platform | 30s queries, 60% uptime | <200ms queries, 99.95% uptime | $15M |\n",
    "| **NVIDIA** | Model Serving | 100ms latency, manual scaling | 35ms latency, auto-scaling | $8M |\n",
    "| **AMD** | Data Pipeline | 5min latency, data loss | <2min latency, zero loss | $12M |\n",
    "| **Qualcomm** | Training Cluster | 20hr training, 50% GPU util | 4hr training, 92% GPU util | $20M |\n",
    "\n",
    "**Total measurable impact:** $55M across 4 companies\n",
    "\n",
    "### Scalability Numbers to Remember\n",
    "\n",
    "**Latency:**\n",
    "- L1 cache: 0.5ns\n",
    "- RAM: 100ns\n",
    "- SSD: 100¬µs\n",
    "- Network (same datacenter): 500¬µs\n",
    "- HDD: 10ms\n",
    "- Network (cross-continent): 150ms\n",
    "\n",
    "**Throughput benchmarks:**\n",
    "- Single PostgreSQL: 10K writes/sec\n",
    "- Redis: 100K ops/sec\n",
    "- Cassandra (10 nodes): 1M writes/sec\n",
    "- Kafka: 1M messages/sec per broker\n",
    "\n",
    "**Availability:**\n",
    "- 99% = 3.65 days downtime/year\n",
    "- 99.9% = 8.76 hours downtime/year\n",
    "- 99.99% = 52.56 minutes downtime/year\n",
    "- 99.999% = 5.26 minutes downtime/year\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Immediate (This Week):**\n",
    "1. Design one system from scratch (URL shortener, pastebin, cache)\n",
    "2. Calculate capacity estimates for your current project\n",
    "3. Identify bottlenecks in existing system\n",
    "\n",
    "**Short-term (This Month):**\n",
    "1. Build test data platform with distributed storage\n",
    "2. Implement model serving with auto-scaling\n",
    "3. Set up monitoring and alerting (Prometheus + Grafana)\n",
    "\n",
    "**Long-term (This Quarter):**\n",
    "1. Complete 10 system design problems (Grokking System Design Interview)\n",
    "2. Migrate monolith to microservices\n",
    "3. Design and implement ML platform (training + serving + monitoring)\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Books:**\n",
    "1. *Designing Data-Intensive Applications* by Martin Kleppmann - Bible of distributed systems\n",
    "2. *System Design Interview* by Alex Xu - Interview preparation\n",
    "3. *Building Microservices* by Sam Newman - Microservices architecture\n",
    "4. *Machine Learning Systems* by Chip Huyen - ML production systems\n",
    "\n",
    "**Online:**\n",
    "- [System Design Primer](https://github.com/donnemartin/system-design-primer) - Comprehensive guide\n",
    "- [Grokking the System Design Interview](https://www.educative.io/courses/grokking-the-system-design-interview) - Interview prep\n",
    "- [High Scalability Blog](http://highscalability.com/) - Real-world architectures\n",
    "- [AWS Architecture Blog](https://aws.amazon.com/blogs/architecture/) - Cloud patterns\n",
    "\n",
    "**Practice:**\n",
    "- Design Instagram, Twitter, YouTube, Uber\n",
    "- Calculate capacity (storage, bandwidth, servers needed)\n",
    "- Draw architecture diagrams\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now understand how to design large-scale distributed systems for AI/ML workloads. You can architect platforms handling 100M+ users, 1PB+ data, and 100K+ requests/second with 99.99% uptime.\n",
    "\n",
    "**Measurable skills gained:**\n",
    "- Design systems scaling 10-100√ó traffic\n",
    "- Reduce latency 100-1000√ó with caching\n",
    "- Achieve 99.99% uptime with replication + load balancing\n",
    "- Build ML platforms serving 100K predictions/sec\n",
    "- Save $5-20M in infrastructure costs through proper architecture\n",
    "\n",
    "**Ready for version control mastery?** Proceed to **Notebook 009: Git & Version Control** to learn branching strategies, CI/CD pipelines, and model versioning for production ML systems! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
